{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "pnWO37v6oLFg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from bert_score import score as bert_score_fn\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import re\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "import gdown\n",
        "from functools import partial\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import spacy\n",
        "import ast\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mps\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    DEVICE = 'mps'  # Apple GPU 사용\n",
        "elif torch.cuda.is_available():\n",
        "    DEVICE = 'cuda'  # NVIDIA GPU 사용\n",
        "else:\n",
        "    DEVICE = 'cpu'   # CPU fallback\n",
        "\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "iuol8CxrHQZC"
      },
      "outputs": [],
      "source": [
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzpEEDthSnhi",
        "outputId": "3677fb2b-d313-4960-fa2a-f5932daa0af8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data size: 162899\n",
            "Dev data size: 1065\n",
            "Test data size: 1081\n",
            "\n",
            "Train data sample:\n",
            "                      Title  \\\n",
            "0       No-Bake Nut Cookies   \n",
            "1               Creamy Corn   \n",
            "2      Reeses Cups(Candy)     \n",
            "3  Cheeseburger Potato Soup   \n",
            "4       Rhubarb Coffee Cake   \n",
            "\n",
            "                                         Ingredients  \\\n",
            "0  [\"1 c. firmly packed brown sugar\", \"1/2 c. eva...   \n",
            "1  [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...   \n",
            "2  [\"1 c. peanut butter\", \"3/4 c. graham cracker ...   \n",
            "3  [\"6 baking potatoes\", \"1 lb. of extra lean gro...   \n",
            "4  [\"1 1/2 c. sugar\", \"1/2 c. butter\", \"1 egg\", \"...   \n",
            "\n",
            "                                              Recipe  \n",
            "0  [\"In a heavy 2-quart saucepan, mix brown sugar...  \n",
            "1  [\"In a slow cooker, combine all ingredients. C...  \n",
            "2  [\"Combine first four ingredients and press in ...  \n",
            "3  [\"Wash potatoes; prick several times with a fo...  \n",
            "4  [\"Cream sugar and butter.\", \"Add egg and beat ...  \n"
          ]
        }
      ],
      "source": [
        "# data 다운\n",
        "train_path = 'Cooking_Dataset/train.csv'\n",
        "dev_path = 'Cooking_Dataset/dev.csv'\n",
        "test_path = 'Cooking_Dataset/test.csv'\n",
        "\n",
        "\n",
        "if not os.path.exists('Cooking_Dataset'):\n",
        "    os.makedirs('Cooking_Dataset')\n",
        "    print(\"Downloading Dataset\")\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1uZdYjvllt0dSdKKtrCgKHUk-APKdmeNU\", train_path, quiet=False)\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1SAMbkdtjGBYgojqobiwe7ZmnEq7SiGsF\", dev_path, quiet=False)\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1v6Rr2et_4WA5mRwwlRxtLhn38pbmr9Yr\", test_path, quiet=False)\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "dev_df = pd.read_csv(dev_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"Train data size: {len(train_df)}\")\n",
        "print(f\"Dev data size: {len(dev_df)}\")\n",
        "print(f\"Test data size: {len(test_df)}\")\n",
        "print(\"\\nTrain data sample:\")\n",
        "print(train_df.head())\n",
        "#No-Bake Nut Cookies\n",
        "# [\"1 c. firmly packed brown sugar\", \"1/2 c. evaporated milk\", \"1/2 tsp. vanilla\", \"1/2 c. broken nuts (pecans)\", \"2 Tbsp. butter or margarine\", \"3 1/2 c. bite size shredded rice biscuits\"]\n",
        "# [\"In a heavy 2-quart saucepan, mix brown sugar, nuts, evaporated milk and butter or margarine.\", \"Stir over medium heat until mixture bubbles all over top.\", \"Boil and stir 5 minutes more. Take off heat.\", \"Stir in vanilla and cereal; mix well.\", \"Using 2 teaspoons, drop and shape into 30 clusters on wax paper.\", \"Let stand until firm, about 30 minutes.\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtQwUvtETjCF",
        "outputId": "86b57a8d-0dd5-4fa7-abdf-0d96cb367749"
      },
      "outputs": [],
      "source": [
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenizer_ingredient_baseline(text):\n",
        "    \"\"\"\n",
        "    Baseline 전처리: 소문자화 + lemmatization + 간단한 필터링\n",
        "    - stopword 제거 있음\n",
        "    - 숫자, 구두점 제거 있음\n",
        "    - 불필요한 복잡 전처리 없음\n",
        "    \"\"\"\n",
        "    import ast\n",
        "    text_list = ast.literal_eval(text)\n",
        "    tokens = []\n",
        "\n",
        "    with spacy_en.select_pipes(disable=[\"parser\", \"ner\", \"tagger\"]):\n",
        "        for item in text_list:\n",
        "            doc = spacy_en(item.lower())\n",
        "            for token in doc:\n",
        "                if token.is_punct or token.like_num or token.is_stop:\n",
        "                    continue\n",
        "                tokens.append(token.lemma_.strip())  # 기본 lemmatization만 유지\n",
        "    return tokens\n",
        "\n",
        "\n",
        "\n",
        "def tokenizer_recipe_baseline(text):\n",
        "    \"\"\"\n",
        "    Recipe 전처리: 소문자화 + lemmatization만 수행 (구두점, stopword, 숫자는 유지)\n",
        "    - 조리 순서, 동사 등 자연스러운 문장 흐름을 보존해야 하므로 간단한 전처리만 적용\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    with spacy_en.select_pipes(disable=[\"parser\", \"ner\", \"tagger\"]):\n",
        "        doc = spacy_en(text.lower())\n",
        "        for token in doc:\n",
        "            # 너무 공격적인 필터링은 하지 않음\n",
        "            if token.is_space:\n",
        "                continue\n",
        "            tokens.append(token.lemma_.strip())\n",
        "    return tokens\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def tokenizer_ingredient_extension(text, remove_stopwords=True, lemmatize=True):\n",
        "    text_list = ast.literal_eval(text)  # 문자열 → 리스트로 변환\n",
        "    unit_keywords = {\n",
        "        'c', 'cup', 'cups', 'tbsp', 'tablespoon', 'tsp', 'teaspoon',\n",
        "        'oz', 'ounce', 'lb', 'pound', 'g', 'kg', 'mg',\n",
        "        'pt', 'qt', 'gal', 'ml', 'l',\n",
        "        'package', 'pkg', 'envelope', 'box', 'bag', 'jar', 'can', 'cans', 'bottle',\n",
        "        'dash', 'pinch', 'slice', 'slices', 'head', 'inch', 'inches',\n",
        "        'stick', 'sticks', 'small', 'medium', 'large'\n",
        "    }\n",
        "\n",
        "    tokens = []\n",
        "\n",
        "    with spacy_en.select_pipes(disable=[\"parser\", \"ner\"]):\n",
        "        for item in text_list:\n",
        "            doc = spacy_en(item.lower())\n",
        "            for token in doc:\n",
        "                if token.is_punct or token.is_space:\n",
        "                    continue\n",
        "                if token.like_num:\n",
        "                    continue\n",
        "                if token.text.strip(\".\") in unit_keywords:\n",
        "                    continue\n",
        "                if remove_stopwords and token.is_stop:\n",
        "                    continue\n",
        "                if token.pos_ != \"NOUN\":  # 명사만\n",
        "                    continue\n",
        "\n",
        "                tokens.append(token.lemma_.strip() if lemmatize else token.text.strip())\n",
        "    return tokens\n",
        "\n",
        "def tokenizer_recipe_extension(text):  # 디폴트: lemmatize 안 함\n",
        "    text_list = ast.literal_eval(text)\n",
        "    tokens = []\n",
        "\n",
        "    # tagger는 유지해서 lemma 써도 warning 없음\n",
        "    with spacy_en.select_pipes(disable=[\"parser\", \"ner\", \"tagger\"]):\n",
        "        for item in text_list:\n",
        "            doc = spacy_en(item.lower())  # 소문자화\n",
        "            for token in doc:\n",
        "                if token.is_punct or token.is_space:\n",
        "                    continue  # 마침표, 쉼표 제거\n",
        "                if token.like_num:  # 숫자 유지\n",
        "                    tokens.append(token.text.strip())\n",
        "                    continue\n",
        "                \n",
        "                tokens.append(token.text.strip())\n",
        "                \n",
        "    return tokens\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"In a slow cooker, combine all ingredients. Cover and cook on low for 4 hours or until heated through and cheese is melted. Stir well before serving. Yields 6 servings.\"]\n",
            "['in', 'a', 'slow', 'cooker', 'combine', 'all', 'ingredients', 'cover', 'and', 'cook', 'on', 'low', 'for', '4', 'hours', 'or', 'until', 'heated', 'through', 'and', 'cheese', 'is', 'melted', 'stir', 'well', 'before', 'serving', 'yields', '6', 'servings']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jeeeunkim/Desktop/FIT5217_NLP/NLP/.venv/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ],
      "source": [
        "ing = train_df.iloc[1,2]\n",
        "print(ing)\n",
        "print(tokenizer_recipe_extension(ing))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "O5IrjwMXuv3h"
      },
      "outputs": [],
      "source": [
        "def build_vocab(token_lists, min_freq=2):\n",
        "    # vocab 생성: 자주 등장하는 단어만 포함 + 특수 토큰 정의\n",
        "    vocab = build_vocab_from_iterator(\n",
        "        token_lists,  # 토큰 리스트들을 직접 반복\n",
        "        min_freq=min_freq,  # 최소 등장 빈도\n",
        "        specials=['<pad>', '<sos>', '<eos>', '<unk>']  # 특수 토큰 추가\n",
        "    )\n",
        "    vocab.set_default_index(vocab['<unk>'])  # 없는 단어는 <unk>로 처리\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_or_tokenize_data(model_type, train_df):\n",
        "    if \"extension\" in model_type:\n",
        "        tokenizer_ingredient = tokenizer_ingredient_extension\n",
        "        tokenizer_recipe = tokenizer_recipe_extension\n",
        "        ingredient_cache_path = \"tokens/ingredient_tokens_ext.pkl\"\n",
        "        recipe_cache_path = \"tokens/recipe_tokens_ext.pkl\"\n",
        "        ingredient_cache_download = \"https://drive.google.com/uc?id=1uXHVNptWeQgkv0uSRLaKYF1SFpcng5cF\"\n",
        "        recipe_cache_dowload = \"https://drive.google.com/uc?id=1qXrpx_nxt8p-ErDPq5w4AkrqNahHki_o\"\n",
        "    else:\n",
        "        tokenizer_ingredient = tokenizer_ingredient_baseline\n",
        "        tokenizer_recipe = tokenizer_recipe_baseline\n",
        "        ingredient_cache_path = \"tokens/ingredient_tokens.pkl\"\n",
        "        recipe_cache_path = \"tokens/recipe_tokens.pkl\"\n",
        "        ingredient_cache_download = \"https://drive.google.com/uc?id=...\"\n",
        "        recipe_cache_dowload = \"https://drive.google.com/uc?id=...\"\n",
        "\n",
        "    if not os.path.exists('tokens'):\n",
        "        os.makedirs('tokens')\n",
        "        gdown.download(ingredient_cache_download, ingredient_cache_path, quiet=False)\n",
        "        gdown.download(recipe_cache_dowload, recipe_cache_path, quiet=False)\n",
        "\n",
        "    try:\n",
        "        with open(ingredient_cache_path, \"rb\") as f:\n",
        "            ingredient_token_lists = pickle.load(f)\n",
        "        with open(recipe_cache_path, \"rb\") as f:\n",
        "            recipe_token_lists = pickle.load(f)\n",
        "        print(\"✅ Loaded token cache.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"⚠️ No cache → Tokenizing...\")\n",
        "        ingredient_token_lists = [tokenizer_ingredient(text) for text in tqdm(train_df['Ingredients'], desc=\"Tokenizing ingredients\")]\n",
        "        recipe_token_lists = [tokenizer_recipe(text) for text in tqdm(train_df['Recipe'], desc=\"Tokenizing recipes\")]\n",
        "        with open(ingredient_cache_path, \"wb\") as f:\n",
        "            pickle.dump(ingredient_token_lists, f)\n",
        "        with open(recipe_cache_path, \"wb\") as f:\n",
        "            pickle.dump(recipe_token_lists, f)\n",
        "\n",
        "    ingredient_vocab = build_vocab(ingredient_token_lists, min_freq=1)\n",
        "    recipe_vocab = build_vocab(recipe_token_lists)\n",
        "    return ingredient_token_lists, recipe_token_lists, ingredient_vocab, recipe_vocab, tokenizer_ingredient, tokenizer_recipe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "u3aRuH1BcoQQ"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, df, ingredient_vocab, recipe_vocab,\n",
        "                 tokenizer_ingredient, tokenizer_recipe):\n",
        "        self.df = df\n",
        "        self.ingredient_vocab = ingredient_vocab\n",
        "        self.recipe_vocab = recipe_vocab\n",
        "        self.tokenizer_ingredient = tokenizer_ingredient\n",
        "        self.tokenizer_recipe = tokenizer_recipe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ingredient_tokens = self.tokenizer_ingredient(self.df.iloc[idx][\"Ingredients\"])\n",
        "        recipe_tokens = self.tokenizer_recipe(self.df.iloc[idx][\"Recipe\"])\n",
        "\n",
        "        ingredient_ids = [self.ingredient_vocab[token] for token in ingredient_tokens]\n",
        "        recipe_ids = [self.recipe_vocab['<sos>']] + \\\n",
        "                     [self.recipe_vocab[token] for token in recipe_tokens] + \\\n",
        "                     [self.recipe_vocab['<eos>']]\n",
        "\n",
        "        return torch.tensor(ingredient_ids), torch.tensor(recipe_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "zvtpEXhJyzRN"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch, ingredient_vocab, recipe_vocab, device):\n",
        "    ingredients, recipes = zip(*batch)\n",
        "    ingredients_padded = pad_sequence(ingredients, batch_first=True, padding_value=ingredient_vocab['<pad>'])\n",
        "    recipes_padded = pad_sequence(recipes, batch_first=True, padding_value=recipe_vocab['<pad>'])\n",
        "    return ingredients_padded.to(device), recipes_padded.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "ZHuvOqpGoe5N"
      },
      "outputs": [],
      "source": [
        "class Encoder_GRU(nn.Module):\n",
        "    def __init__(self, ingredient_vocab_size, embedding_dim, hidden_dim, n_layers, dropout_ratio,\n",
        "                 embedding_weights=None, freeze=False):\n",
        "        super().__init__()\n",
        "        # 임베딩\n",
        "        if embedding_weights is not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=freeze)\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(ingredient_vocab_size, embedding_dim)\n",
        "        \n",
        "\n",
        "        # GRU 레이어\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.gru = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          n_layers,\n",
        "                          dropout=dropout_ratio if n_layers>1 else 0,\n",
        "                          batch_first=True)\n",
        "\n",
        "        # 드롭아웃\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src : [batch_size, src_len]\n",
        "        src = src.long() \n",
        "        # 임베딩\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded : [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        # gru 통과\n",
        "        outputs, hidden = self.gru(embedded) # h0를 따로 주지 않으면, 디폴트로 h0가 0로 초기화되서 들어감\n",
        "        # outputs: [batch_size, src_len, hidden_size]\n",
        "        # hidden: [n_layers, batch_size, hidden_size]\n",
        "\n",
        "        return outputs,hidden\n",
        "\n",
        "class Decoder_GRU(nn.Module):\n",
        "    def __init__(self, recipe_vocab_size, embedding_dim, hidden_dim, n_layers, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        self.recipe_vocab_size = recipe_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "\n",
        "        # 임베딩\n",
        "        self.embedding = nn.Embedding(recipe_vocab_size, embedding_dim)\n",
        "\n",
        "        # GRU 레이어\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.gru = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          n_layers,\n",
        "                          dropout=dropout_ratio if n_layers>1 else 0,\n",
        "                          batch_first=True)\n",
        "\n",
        "        # fc 레이어\n",
        "        self.fc_out = nn.Linear(hidden_dim, recipe_vocab_size)\n",
        "\n",
        "        # 드롭아웃\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input = input.long()\n",
        "        # input : [batch_size]\n",
        "        input = input.unsqueeze(1)\n",
        "        # input : [batch_size, 단어의 개수=1]\n",
        "\n",
        "        # 임베딩\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded : [batch_size, 단어의 개수=1, hidden_dim]\n",
        "\n",
        "        # GRU 통과\n",
        "        outputs, hidden = self.gru(embedded,hidden)\n",
        "        # outputs: [batch_size, 단어의 개수=1, hidden_size]\n",
        "        # hidden: [n_layers, batch_size, hidden_size]\n",
        "\n",
        "        # fc 통과\n",
        "        prediction = self.fc_out(outputs.squeeze(1))\n",
        "        # prediction: [batch_size, vocab_size]\n",
        "\n",
        "        return prediction, hidden\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device, use_attention):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.use_attention = use_attention\n",
        "\n",
        "    def forward(self, src, target=None, teacher_forcing_ratio=0.5, max_len=50):\n",
        "        # 인코더\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        batch_size = src.size(0)\n",
        "        vocab_size = self.decoder.recipe_vocab_size\n",
        "\n",
        "        # 추론 모드\n",
        "        if target is None:\n",
        "            outputs = []\n",
        "            input_token = torch.tensor([recipe_vocab['<sos>']] * batch_size).to(self.device).long()\n",
        "\n",
        "            for _ in range(max_len):\n",
        "                if self.use_attention:\n",
        "                    output, hidden, _ = self.decoder(input_token, hidden, encoder_outputs)\n",
        "                else:\n",
        "                    output, hidden = self.decoder(input_token, hidden)\n",
        "                top1 = output.argmax(1)\n",
        "                outputs.append(top1.unsqueeze(1))\n",
        "                input_token = top1\n",
        "\n",
        "            return torch.cat(outputs, dim=1)  # [batch_size, max_len]\n",
        "\n",
        "        # 학습 모드\n",
        "        target = target.long()\n",
        "        target_len = target.shape[1]\n",
        "        outputs = torch.zeros(batch_size, target_len, vocab_size).to(self.device)\n",
        "        input_token = target[:, 0].long()  # <sos>\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            if self.use_attention:\n",
        "                output, hidden, _ = self.decoder(input_token, hidden, encoder_outputs)\n",
        "            else:\n",
        "                output, hidden = self.decoder(input_token, hidden)\n",
        "            outputs[:, t, :] = output\n",
        "            top1 = output.argmax(1)\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            input_token = target[:, t].long() if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "import torch.nn.functional as F\n",
        "    \n",
        "\n",
        "# Define a decoder with attention mechanism using PyTorch's nn.Module\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, recipe_vocab_size,embedding_dim, hidden_size, n_layers, dropout_ratio):\n",
        "        # Initialize the base nn.Module class\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "\n",
        "        # Save parameters\n",
        "        self.recipe_vocab_size = recipe_vocab_size              # Size of the output vocabulary\n",
        "        self.embedding_dim = embedding_dim                  # Dropout probability\n",
        "        self.hidden_size = hidden_size              # Size of the hidden state in GRU\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "                  \n",
        "\n",
        "        # Define layers\n",
        "        self.embedding = nn.Embedding(self.recipe_vocab_size, self.hidden_size)  # Converts word indices to dense vectors\n",
        "        self.dropout = nn.Dropout(self.dropout_ratio)                          # Applies dropout for regularization\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, self.n_layers, dropout=self.dropout_ratio if self.n_layers>1 else 0, batch_first=True)\n",
        "             # GRU to process the embedded inputs\n",
        "        self.out = nn.Linear(self.hidden_size * 2, self.recipe_vocab_size)       # Linear layer for generating final output\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        input = input.long() \n",
        "        # input : [batch_size]\n",
        "        input = input.unsqueeze(1)\n",
        "        # input : [batch_size, 단어의 개수=1]\n",
        "\n",
        "        # 임베딩\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded : [batch_size, 단어의 개수=1, hidden_dim]\n",
        "\n",
        "        # Pass through GRU\n",
        "        output, hidden = self.gru(embedded, hidden)  # output: [batch, 1, hidden_size]\n",
        "\n",
        "        # Compute attention weights using dot-product attention:\n",
        "        # hidden[-1]: [batch, hidden_size]\n",
        "        # encoder_outputs: [batch, src_len, hidden_size]\n",
        "    \n",
        "        attn_weights = F.softmax(\n",
        "            torch.bmm(output, encoder_outputs.transpose(1, 2)), # [batch, 1, hidden_size] x [batch, hidden_size, src_len]\n",
        "            dim=-1\n",
        "        )  # [batch, 1, src_len]\n",
        "\n",
        "        # Apply attention weights to encoder outputs to get context vector\n",
        "        # attn_weights: (1, 1, max_length)\n",
        "        # encoder_outputs.unsqueeze(0): (1, max_length, hidden_size)\n",
        "        attn_output = torch.bmm(attn_weights, encoder_outputs)  # [batch, 1, hidden_size]\n",
        "\n",
        "        # Concatenate attention output and decoder hidden state\n",
        "        concat_output = torch.cat((output, attn_output), dim=2)  # [batch, 1, hidden*2]\n",
        "\n",
        "        # Pass through linear layer and softmax to get output word probabilities\n",
        "        output = F.log_softmax(self.out(concat_output).squeeze(1), dim=1)  # [batch, vocab_size]\n",
        "\n",
        "        # Return output word distribution, updated hidden state, and attention weights\n",
        "        return output, hidden, attn_weights.squeeze(1)\n",
        "\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_glove_embedding_matrix(glove_path, vocab, glove_dim=100):\n",
        "    print(\"🔎 Loading GloVe vectors...\")\n",
        "    glove_embeddings = {}\n",
        "\n",
        "    with open(glove_path, 'r', encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            vector = torch.tensor(list(map(float, values[1:])))\n",
        "            glove_embeddings[word] = vector\n",
        "\n",
        "    # Initialize matrix with random vectors\n",
        "    embedding_matrix = torch.randn(len(vocab), glove_dim)\n",
        "\n",
        "    for word, idx in vocab.get_stoi().items():\n",
        "        if word in glove_embeddings:\n",
        "            embedding_matrix[idx] = glove_embeddings[word]\n",
        "\n",
        "    print(\"✅ GloVe embedding matrix created.\")\n",
        "    return embedding_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_epoch(model, dataloader, criterion, optimizer=None, teacher_forcing_ratio=0.5, max_len=50):\n",
        "    rloss = 0\n",
        "    batch_losses = []\n",
        "\n",
        "    for i, (src_batch, trg_batch) in enumerate(tqdm(dataloader, desc=\"Training batches\", leave=False)):\n",
        "        src_batch = src_batch.to(DEVICE)\n",
        "        trg_batch = trg_batch.to(DEVICE)\n",
        "\n",
        "        output = model(src_batch, trg_batch, teacher_forcing_ratio=teacher_forcing_ratio, max_len=max_len)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:, :].reshape(-1, output_dim)\n",
        "        trg = trg_batch[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "            optimizer.step()\n",
        "\n",
        "        batch_loss = loss.item() * src_batch.size(0)\n",
        "        batch_losses.append(batch_loss)\n",
        "        rloss += batch_loss\n",
        "\n",
        "        tqdm.write(f\"[Batch {i+1}/{len(dataloader)}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = rloss / len(dataloader.dataset)\n",
        "    return avg_loss, batch_losses\n",
        "\n",
        "\n",
        "def Train(model, train_loader, val_loader, criterion, optimizer,\n",
        "          EPOCHS, BATCH_SIZE, TRAIN_RATIO,\n",
        "          save_model_path, save_history_path, TEACHER_FORCING_RATIO, MAX_LEN, **kwargs\n",
        "          ):\n",
        "    \"\"\"\n",
        "    이어서 학습할 수 있게 start_epoch와 best_val_loss를 인자로 받음\n",
        "    \"\"\"\n",
        "    if \"LR_STEP\" in kwargs:\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=kwargs[\"LR_STEP\"], gamma=kwargs[\"LR_GAMMA\"])\n",
        "    else:\n",
        "        scheduler = None\n",
        "    loss_history = {\"train_epoch\": [], \"train_iter\": [], \"val_epoch\": []}\n",
        "    best_val_loss = float('inf')\n",
        "    train_start_time = time.time()\n",
        "    for ep in tqdm(range(EPOCHS), desc=\"Epochs\"):\n",
        "        print(f\"Epoch {ep+1}/{EPOCHS}\")\n",
        "\n",
        "        ep_start_time = time.time()\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_epoch_loss, train_batch_loss = loss_epoch(\n",
        "            model, train_loader, criterion, optimizer, \n",
        "            teacher_forcing_ratio=TEACHER_FORCING_RATIO,\n",
        "            max_len=MAX_LEN\n",
        "        )\n",
        "        loss_history[\"train_epoch\"].append(train_epoch_loss)\n",
        "        loss_history[\"train_iter\"].append(train_batch_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss, _ = loss_epoch(\n",
        "                model, val_loader, criterion, optimizer=None,\n",
        "                teacher_forcing_ratio=0.0,\n",
        "                max_len=MAX_LEN\n",
        "            )\n",
        "            loss_history[\"val_epoch\"].append(val_loss)\n",
        "\n",
        "        ep_elapsed_time = time.time() - ep_start_time\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            # 디렉토리 없으면 생성\n",
        "            os.makedirs(\"results\", exist_ok=True)\n",
        "            torch.save({\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"epoch\": ep,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"train_loss\": train_epoch_loss,  # train_loss 저장\n",
        "            }, save_model_path)\n",
        "            print(\"Best model saved!\")\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        print(f\"Train Loss: {train_epoch_loss:.4f} | Val Loss: {val_loss:.4f} | Time: {ep_elapsed_time:.2f}s\")\n",
        "    train_elapsed_time = time.time() - train_start_time\n",
        "    # Save training history\n",
        "    \n",
        "    torch.save({\n",
        "        \"loss_history\": loss_history,\n",
        "        \"EPOCHS\": EPOCHS,\n",
        "        \"BATCH_SIZE\": BATCH_SIZE,   \n",
        "        \"TRAIN_RATIO\": TRAIN_RATIO,\n",
        "        \"train_elapsed_time\": train_elapsed_time,\n",
        "        \"LR_STEP\": kwargs[\"LR_STEP\"] if \"LR_STEP\" in kwargs else None,\n",
        "        \"LR_GAMMA\": kwargs[\"LR_GAMMA\"] if \"LR_GAMMA\" in kwargs else None\n",
        "    }, save_history_path)\n",
        "    total_iterations = sum(len(batch_list) for batch_list in loss_history[\"train_iter\"])\n",
        "    print(f\"Total number of iteration : {total_iterations}\")\n",
        "    print(f\"Training Completed! Total Time: {train_elapsed_time:.2f}s\")\n",
        "    return loss_history\n",
        "\n",
        "\n",
        "\n",
        "def Test(model, test_loader, criterion, recipe_vocab, MAX_LEN):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loss, _ = loss_epoch(model, test_loader, criterion, optimizer=None, teacher_forcing_ratio=0.0, max_len=MAX_LEN)\n",
        "\n",
        "    # 평가 지표 계산\n",
        "    bleu_score, meteor_avg, bertscore_f1 = compute_metrics(model, test_loader, recipe_vocab)\n",
        "\n",
        "    print(f\"Test Loss      : {test_loss:.4f}\")\n",
        "    print(f\"BLEU-4 Score   : {bleu_score:.4f}\")\n",
        "    print(f\"METEOR Score   : {meteor_avg:.4f}\")\n",
        "    print(f\"BERTScore (F1) : {bertscore_f1:.4f}\")\n",
        "\n",
        "    return test_loss, bleu_score, meteor_avg, bertscore_f1\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(model, dataloader, recipe_vocab, max_len=50):\n",
        "    \"\"\"\n",
        "    테스트셋 전체에서 BLEU, METEOR, BERTScore 계산\n",
        "\n",
        "    Returns:\n",
        "        bleu_score (float), meteor_avg (float), bertscore_f1 (float)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    smoothie = SmoothingFunction().method4\n",
        "\n",
        "    ref_list = []\n",
        "    hyp_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src_batch, trg_batch in tqdm(dataloader, desc=\"Evaluating Metrics\"):\n",
        "            src_batch = src_batch.to(DEVICE)\n",
        "            trg_batch = trg_batch.to(DEVICE)\n",
        "\n",
        "            generated = model(src_batch, target=None, teacher_forcing_ratio=0.0, max_len=max_len)\n",
        "\n",
        "            for i in range(src_batch.size(0)):\n",
        "                pred_tokens = generated[i].tolist()\n",
        "                trg_tokens = trg_batch[i].tolist()\n",
        "\n",
        "                # <eos> 기준으로 자르기\n",
        "                if recipe_vocab['<eos>'] in pred_tokens:\n",
        "                    pred_tokens = pred_tokens[:pred_tokens.index(recipe_vocab['<eos>'])]\n",
        "                if recipe_vocab['<eos>'] in trg_tokens:\n",
        "                    trg_tokens = trg_tokens[:trg_tokens.index(recipe_vocab['<eos>'])]\n",
        "\n",
        "                pred_words = [recipe_vocab.get_itos()[idx] for idx in pred_tokens]\n",
        "                trg_words = [recipe_vocab.get_itos()[idx] for idx in trg_tokens]\n",
        "\n",
        "                ref_list.append(trg_words)\n",
        "                hyp_list.append(pred_words)\n",
        "\n",
        "    # BLEU-4\n",
        "    bleu_score = corpus_bleu([[ref] for ref in ref_list], hyp_list, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie) \n",
        "\n",
        "    # METEOR\n",
        "    meteor_scores = [meteor_score([ref], hyp) for ref, hyp in zip(ref_list, hyp_list)]\n",
        "    meteor_avg = sum(meteor_scores) / len(meteor_scores)\n",
        "\n",
        "    # BERTScore\n",
        "    refs = [\" \".join(ref) for ref in ref_list]\n",
        "    hyps = [\" \".join(hyp) for hyp in hyp_list]\n",
        "    _, _, f1 = bert_score_fn(hyps, refs, lang='en', verbose=False)\n",
        "    bertscore_f1 = f1.mean().item()\n",
        "\n",
        "    return bleu_score, meteor_avg, bertscore_f1\n",
        "\n",
        "\n",
        "def plot_loss_epoch(name, loss_history):\n",
        "    plt.figure(figsize=(6, 3))\n",
        "\n",
        "    train_loss = loss_history[\"train_epoch\"]\n",
        "    val_loss = loss_history[\"val_epoch\"]\n",
        "\n",
        "    plt.plot(range(1, len(train_loss) + 1), train_loss, label=\"Train Loss\", color=\"blue\")\n",
        "    plt.plot(range(1, len(val_loss) + 1), val_loss, label=\"Validation Loss\", color=\"red\")\n",
        "\n",
        "    plt.xlabel(\"Epoch\", fontsize=10)\n",
        "    plt.ylabel(\"Loss\", fontsize=10)\n",
        "    plt.title(f\"Loss per Epoch: {name}\", fontsize=12)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_loss_iter(**models_histories):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for model_name, history in models_histories.items():\n",
        "        train_iter_losses = history.get(\"train_iter\", [])\n",
        "        if train_iter_losses:\n",
        "            plt.plot(train_iter_losses, label=model_name)\n",
        "        else:\n",
        "            print(f\"[경고] {model_name}에 train_iter 데이터가 없습니다.\")\n",
        "\n",
        "    plt.title(\"Training Iteration Loss\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main(model_type: str, config: dict,\n",
        "         train_df, dev_df, test_df,\n",
        "         glove_path: str = \"./glove.6B.100d.txt\"):\n",
        "\n",
        "    # 1. Tokenize & Vocab\n",
        "    ingredient_token_lists, recipe_token_lists, ingredient_vocab, recipe_vocab, tokenizer_ingredient, tokenizer_recipe = load_or_tokenize_data(model_type, train_df)\n",
        "\n",
        "    # 2. GloVe embedding 적용 (config에 \"embedding_weights\" 키가 있는 경우)\n",
        "    embedding_weights = None\n",
        "    freeze_embedding = False\n",
        "    if config.get(\"use_glove\", False):\n",
        "        ingredient_embedding_matrix = build_glove_embedding_matrix(glove_path, ingredient_vocab, glove_dim=config[\"EMBED_DIM\"])\n",
        "        embedding_weights = ingredient_embedding_matrix\n",
        "    else:\n",
        "        embedding_weights = None\n",
        "\n",
        "    # 3. Dataset & DataLoader\n",
        "    BATCH_SIZE = config.get(\"BATCH_SIZE\", 64)\n",
        "    train_dataset = CustomDataset(train_df, ingredient_vocab, recipe_vocab, tokenizer_ingredient, tokenizer_recipe)\n",
        "    dev_dataset = CustomDataset(dev_df, ingredient_vocab, recipe_vocab, tokenizer_ingredient, tokenizer_recipe)\n",
        "    test_dataset = CustomDataset(test_df, ingredient_vocab, recipe_vocab, tokenizer_ingredient, tokenizer_recipe)\n",
        "\n",
        "    collate = partial(collate_fn, ingredient_vocab=ingredient_vocab, recipe_vocab=recipe_vocab, device=DEVICE)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate)\n",
        "    dev_loader = DataLoader(dev_dataset, batch_size=64, shuffle=False, collate_fn=collate)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate)\n",
        "\n",
        "    # 4. Model 구성\n",
        "    INPUT_DIM = len(ingredient_vocab)\n",
        "    OUTPUT_DIM = len(recipe_vocab)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=recipe_vocab['<pad>'])\n",
        "\n",
        "    encoder = Encoder_GRU(\n",
        "        ingredient_vocab_size=INPUT_DIM,\n",
        "        embedding_dim=config[\"EMBED_DIM\"],\n",
        "        hidden_dim=config[\"HIDDEN_DIM\"],\n",
        "        n_layers=config[\"N_LAYERS\"],\n",
        "        dropout_ratio=config[\"DROPOUT\"],\n",
        "        embedding_weights=embedding_weights,\n",
        "        freeze=freeze_embedding\n",
        "    )\n",
        "\n",
        "    if config[\"USE_ATTENTION\"]:\n",
        "        decoder = AttnDecoderRNN(\n",
        "            recipe_vocab_size=OUTPUT_DIM,\n",
        "            embedding_dim=config[\"EMBED_DIM\"],\n",
        "            hidden_size=config[\"HIDDEN_DIM\"],\n",
        "            n_layers=config[\"N_LAYERS\"],\n",
        "            dropout_ratio=config[\"DROPOUT\"]\n",
        "        )\n",
        "    else:\n",
        "        decoder = Decoder_GRU(\n",
        "            recipe_vocab_size=OUTPUT_DIM,\n",
        "            embedding_dim=config[\"EMBED_DIM\"],\n",
        "            hidden_dim=config[\"HIDDEN_DIM\"],\n",
        "            n_layers=config[\"N_LAYERS\"],\n",
        "            dropout_ratio=config[\"DROPOUT\"]\n",
        "        )\n",
        "\n",
        "    model = Seq2Seq(encoder, decoder, DEVICE, use_attention=config[\"USE_ATTENTION\"]).to(DEVICE)\n",
        "\n",
        "    # 5. 학습 또는 로드\n",
        "    save_model_path = f\"results/{model_type}.pt\"\n",
        "    save_history_path = f\"results/{model_type}_history.pt\"\n",
        "\n",
        "    if config[\"new_model_train\"]:\n",
        "        print(f\"Training model: {model_type}\")\n",
        "        optimizer = optim.Adam(model.parameters(), lr=config[\"LR\"])\n",
        "        loss_history = Train(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=dev_loader,\n",
        "            criterion=criterion,\n",
        "            optimizer=optimizer,\n",
        "            EPOCHS=config[\"EPOCHS\"],\n",
        "            BATCH_SIZE=BATCH_SIZE,\n",
        "            TRAIN_RATIO=1.0,\n",
        "            save_model_path=save_model_path,\n",
        "            save_history_path=save_history_path,\n",
        "            TEACHER_FORCING_RATIO=config[\"TEACHER_FORCING_RATIO\"],\n",
        "            MAX_LEN=config[\"MAX_LEN\"],\n",
        "            LR_STEP=config.get(\"LR_STEP\"),\n",
        "            LR_GAMMA=config.get(\"LR_GAMMA\")\n",
        "        )\n",
        "        return model, encoder, decoder, loss_history, save_model_path, save_history_path, test_loader, recipe_vocab\n",
        "    else:\n",
        "        return model, encoder, decoder, None, save_model_path, save_history_path, test_loader, recipe_vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 하이퍼파라미터 저장\n",
        "experiment_configs = {\n",
        "    \"baseline1\": {\n",
        "        \"new_model_train\" : True,\n",
        "        \"EMBED_DIM\": 128,\n",
        "        \"HIDDEN_DIM\": 256,\n",
        "        \"DROPOUT\": 0.5,\n",
        "        \"N_LAYERS\": 1,\n",
        "        \"LR\": 0.001,\n",
        "        \"EPOCHS\": 5,\n",
        "        \"USE_ATTENTION\": False,\n",
        "        \"TEACHER_FORCING_RATIO\": 0.7,\n",
        "        \"MAX_LEN\": 50,\n",
        "        \"BATCH_SIZE\": 64\n",
        "    },\n",
        "    \"baseline2\": {\n",
        "        \"new_model_train\" : True,\n",
        "        \"EMBED_DIM\": 128,\n",
        "        \"HIDDEN_DIM\": 256,\n",
        "        \"DROPOUT\": 0.5,\n",
        "        \"N_LAYERS\": 1,\n",
        "        \"LR\": 0.001,\n",
        "        \"EPOCHS\": 5,\n",
        "        \"USE_ATTENTION\": True,\n",
        "        \"TEACHER_FORCING_RATIO\": 0.7,\n",
        "        \"MAX_LEN\": 50,\n",
        "        \"BATCH_SIZE\": 64\n",
        "    },\n",
        "    \"mild_extension1\": {\n",
        "        \"new_model_train\" : False,\n",
        "        \"EMBED_DIM\": 256,\n",
        "        \"HIDDEN_DIM\": 512,\n",
        "        \"DROPOUT\": 0.5,\n",
        "        \"N_LAYERS\": 3,\n",
        "        \"LR\": 0.001,\n",
        "        \"EPOCHS\": 5,\n",
        "        \"USE_ATTENTION\": True,\n",
        "        \"TEACHER_FORCING_RATIO\": 0.7,\n",
        "        \"MAX_LEN\": 100,\n",
        "        \"LR_STEP\": 1,         \n",
        "        \"LR_GAMMA\": 0.7,\n",
        "        \"BATCH_SIZE\": 64\n",
        "    },\n",
        "    \"mild_extension2\": {\n",
        "        \"new_model_train\" : False,\n",
        "        \"EMBED_DIM\": 100,  # GloVe 6B 100D와 일치\n",
        "        \"HIDDEN_DIM\": 256,\n",
        "        \"DROPOUT\": 0.5,\n",
        "        \"N_LAYERS\": 3,\n",
        "        \"LR\": 0.001,\n",
        "        \"EPOCHS\": 5,\n",
        "        \"USE_ATTENTION\": True,\n",
        "        \"TEACHER_FORCING_RATIO\": 0.7,\n",
        "        \"MAX_LEN\": 100,\n",
        "        \"LR_STEP\": 1,         \n",
        "        \"LR_GAMMA\": 0.7,\n",
        "        \"use_glove\": True,   \n",
        "        \"freeze_embedding\": False,\n",
        "        \"BATCH_SIZE\": 64\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def evaluate_and_report(model_type, config, encoder, decoder, save_model_path, save_history_path,\n",
        "                        test_loader, criterion, recipe_vocab):\n",
        "    if not os.path.exists(save_model_path) or not os.path.exists(save_history_path):\n",
        "        print(f\"⚠️ Checkpoint for {model_type} not found. Skipping evaluation.\\n\")\n",
        "        return\n",
        "\n",
        "    # 1. 모델 불러오기\n",
        "    model = Seq2Seq(encoder, decoder, DEVICE, use_attention=config[\"USE_ATTENTION\"]).to(DEVICE)\n",
        "    checkpoint = torch.load(save_model_path, map_location=DEVICE)\n",
        "    checkpoint2 = torch.load(save_history_path, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    # 2. 학습 정보 출력\n",
        "    best_epoch = checkpoint[\"epoch\"] + 1\n",
        "    best_train_loss = checkpoint[\"train_loss\"]\n",
        "    best_val_loss = checkpoint[\"val_loss\"]\n",
        "    train_elapsed_time = checkpoint2[\"train_elapsed_time\"]\n",
        "    print(f\"📌 {model_type} | Epoch {best_epoch} | Train Loss: {best_train_loss:.4f} | Val Loss: {best_val_loss:.4f}\")\n",
        "    print(f\"⏱️ Training Time: {train_elapsed_time:.2f}s\\n\")\n",
        "\n",
        "    # 3. 그래프 그리기\n",
        "    loss_history = checkpoint2[\"loss_history\"]\n",
        "    plot_loss_epoch(model_type, loss_history)\n",
        "\n",
        "    # 4. 테스트\n",
        "    Test(model, test_loader, criterion, recipe_vocab, MAX_LEN=config[\"MAX_LEN\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Evaluating baseline1 ===\n",
            "✅ Loaded token cache.\n",
            "Training model: baseline1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c88918dd96cb4bc8bca352765d8e3923",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18534cf56c0247c49ddf649e540939fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training batches:   0%|          | 0/2546 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Batch 1/2546] Loss: 9.1376\n",
            "[Batch 2/2546] Loss: 9.0519\n",
            "[Batch 3/2546] Loss: 8.9756\n",
            "[Batch 4/2546] Loss: 8.8499\n",
            "[Batch 5/2546] Loss: 8.7159\n",
            "[Batch 6/2546] Loss: 8.5131\n",
            "[Batch 7/2546] Loss: 8.2925\n",
            "[Batch 8/2546] Loss: 7.9638\n",
            "[Batch 9/2546] Loss: 7.5588\n",
            "[Batch 10/2546] Loss: 7.0794\n",
            "[Batch 11/2546] Loss: 6.5674\n",
            "[Batch 12/2546] Loss: 6.1141\n",
            "[Batch 13/2546] Loss: 5.8771\n",
            "[Batch 14/2546] Loss: 5.4591\n",
            "[Batch 15/2546] Loss: 5.4253\n",
            "[Batch 16/2546] Loss: 5.3304\n",
            "[Batch 17/2546] Loss: 5.2328\n",
            "[Batch 18/2546] Loss: 5.0859\n",
            "[Batch 19/2546] Loss: 5.0267\n",
            "[Batch 20/2546] Loss: 4.9686\n",
            "[Batch 21/2546] Loss: 4.9558\n",
            "[Batch 22/2546] Loss: 4.9603\n",
            "[Batch 23/2546] Loss: 4.8565\n",
            "[Batch 24/2546] Loss: 4.8112\n",
            "[Batch 25/2546] Loss: 4.7983\n",
            "[Batch 26/2546] Loss: 4.7920\n",
            "[Batch 27/2546] Loss: 4.7369\n",
            "[Batch 28/2546] Loss: 4.7008\n",
            "[Batch 29/2546] Loss: 4.8602\n",
            "[Batch 30/2546] Loss: 4.8175\n",
            "[Batch 31/2546] Loss: 4.9419\n",
            "[Batch 32/2546] Loss: 4.6641\n",
            "[Batch 33/2546] Loss: 4.7550\n",
            "[Batch 34/2546] Loss: 4.8779\n",
            "[Batch 35/2546] Loss: 4.7166\n",
            "[Batch 36/2546] Loss: 4.6487\n",
            "[Batch 37/2546] Loss: 4.7085\n",
            "[Batch 38/2546] Loss: 4.7231\n",
            "[Batch 39/2546] Loss: 4.6405\n",
            "[Batch 40/2546] Loss: 4.7006\n",
            "[Batch 41/2546] Loss: 4.7425\n",
            "[Batch 42/2546] Loss: 4.6624\n",
            "[Batch 43/2546] Loss: 4.7611\n",
            "[Batch 44/2546] Loss: 4.7122\n",
            "[Batch 45/2546] Loss: 4.7294\n",
            "[Batch 46/2546] Loss: 4.6722\n",
            "[Batch 47/2546] Loss: 4.8042\n",
            "[Batch 48/2546] Loss: 4.6729\n",
            "[Batch 49/2546] Loss: 4.6903\n",
            "[Batch 50/2546] Loss: 4.5789\n",
            "[Batch 51/2546] Loss: 4.6556\n",
            "[Batch 52/2546] Loss: 4.6775\n",
            "[Batch 53/2546] Loss: 4.5492\n",
            "[Batch 54/2546] Loss: 4.5889\n",
            "[Batch 55/2546] Loss: 4.4455\n",
            "[Batch 56/2546] Loss: 4.4841\n",
            "[Batch 57/2546] Loss: 4.7025\n",
            "[Batch 58/2546] Loss: 4.4940\n",
            "[Batch 59/2546] Loss: 4.5160\n",
            "[Batch 60/2546] Loss: 4.5671\n",
            "[Batch 61/2546] Loss: 4.4889\n",
            "[Batch 62/2546] Loss: 4.6370\n",
            "[Batch 63/2546] Loss: 4.5798\n",
            "[Batch 64/2546] Loss: 4.5631\n",
            "[Batch 65/2546] Loss: 4.4686\n",
            "[Batch 66/2546] Loss: 4.3740\n",
            "[Batch 67/2546] Loss: 4.4575\n",
            "[Batch 68/2546] Loss: 4.5177\n",
            "[Batch 69/2546] Loss: 4.5066\n",
            "[Batch 70/2546] Loss: 4.4582\n",
            "[Batch 71/2546] Loss: 4.6092\n",
            "[Batch 72/2546] Loss: 4.5385\n",
            "[Batch 73/2546] Loss: 4.4001\n",
            "[Batch 74/2546] Loss: 4.6831\n",
            "[Batch 75/2546] Loss: 4.4169\n",
            "[Batch 76/2546] Loss: 4.4399\n",
            "[Batch 77/2546] Loss: 4.3901\n",
            "[Batch 78/2546] Loss: 4.4312\n",
            "[Batch 79/2546] Loss: 4.4239\n",
            "[Batch 80/2546] Loss: 4.4374\n",
            "[Batch 81/2546] Loss: 4.5229\n",
            "[Batch 82/2546] Loss: 4.3425\n",
            "[Batch 83/2546] Loss: 4.3686\n",
            "[Batch 84/2546] Loss: 4.4307\n",
            "[Batch 85/2546] Loss: 4.3851\n",
            "[Batch 86/2546] Loss: 4.3916\n",
            "[Batch 87/2546] Loss: 4.4342\n",
            "[Batch 88/2546] Loss: 4.5158\n",
            "[Batch 89/2546] Loss: 4.5080\n",
            "[Batch 90/2546] Loss: 4.4680\n",
            "[Batch 91/2546] Loss: 4.5435\n",
            "[Batch 92/2546] Loss: 4.4155\n",
            "[Batch 93/2546] Loss: 4.3884\n",
            "[Batch 94/2546] Loss: 4.2045\n",
            "[Batch 95/2546] Loss: 4.3667\n",
            "[Batch 96/2546] Loss: 4.4072\n",
            "[Batch 97/2546] Loss: 4.3411\n",
            "[Batch 98/2546] Loss: 4.4043\n",
            "[Batch 99/2546] Loss: 4.3406\n",
            "[Batch 100/2546] Loss: 4.3647\n",
            "[Batch 101/2546] Loss: 4.5214\n",
            "[Batch 102/2546] Loss: 4.3726\n",
            "[Batch 103/2546] Loss: 4.4635\n",
            "[Batch 104/2546] Loss: 4.2931\n",
            "[Batch 105/2546] Loss: 4.3432\n",
            "[Batch 106/2546] Loss: 4.2862\n",
            "[Batch 107/2546] Loss: 4.3322\n",
            "[Batch 108/2546] Loss: 4.2469\n",
            "[Batch 109/2546] Loss: 4.2751\n",
            "[Batch 110/2546] Loss: 4.4689\n",
            "[Batch 111/2546] Loss: 4.4132\n",
            "[Batch 112/2546] Loss: 4.2165\n",
            "[Batch 113/2546] Loss: 4.3228\n",
            "[Batch 114/2546] Loss: 4.1476\n",
            "[Batch 115/2546] Loss: 4.1979\n",
            "[Batch 116/2546] Loss: 4.1011\n",
            "[Batch 117/2546] Loss: 4.1208\n",
            "[Batch 118/2546] Loss: 4.3075\n",
            "[Batch 119/2546] Loss: 4.2931\n",
            "[Batch 120/2546] Loss: 4.4531\n",
            "[Batch 121/2546] Loss: 4.1930\n",
            "[Batch 122/2546] Loss: 4.1447\n",
            "[Batch 123/2546] Loss: 4.0630\n",
            "[Batch 124/2546] Loss: 4.3607\n",
            "[Batch 125/2546] Loss: 4.3871\n",
            "[Batch 126/2546] Loss: 4.3042\n",
            "[Batch 127/2546] Loss: 4.1706\n",
            "[Batch 128/2546] Loss: 4.0821\n",
            "[Batch 129/2546] Loss: 4.1032\n",
            "[Batch 130/2546] Loss: 4.0671\n",
            "[Batch 131/2546] Loss: 4.2402\n",
            "[Batch 132/2546] Loss: 4.1305\n",
            "[Batch 133/2546] Loss: 4.2337\n",
            "[Batch 134/2546] Loss: 4.1512\n",
            "[Batch 135/2546] Loss: 4.2983\n",
            "[Batch 136/2546] Loss: 4.2263\n",
            "[Batch 137/2546] Loss: 4.0941\n",
            "[Batch 138/2546] Loss: 4.1698\n",
            "[Batch 139/2546] Loss: 4.1700\n",
            "[Batch 140/2546] Loss: 4.3274\n",
            "[Batch 141/2546] Loss: 4.2620\n",
            "[Batch 142/2546] Loss: 4.1606\n",
            "[Batch 143/2546] Loss: 4.3360\n",
            "[Batch 144/2546] Loss: 4.0419\n",
            "[Batch 145/2546] Loss: 4.2440\n",
            "[Batch 146/2546] Loss: 4.2837\n",
            "[Batch 147/2546] Loss: 4.1128\n",
            "[Batch 148/2546] Loss: 4.0700\n",
            "[Batch 149/2546] Loss: 3.9569\n",
            "[Batch 150/2546] Loss: 4.2062\n",
            "[Batch 151/2546] Loss: 4.1405\n",
            "[Batch 152/2546] Loss: 4.0913\n",
            "[Batch 153/2546] Loss: 4.1454\n",
            "[Batch 154/2546] Loss: 4.0485\n",
            "[Batch 155/2546] Loss: 4.1924\n",
            "[Batch 156/2546] Loss: 4.0630\n",
            "[Batch 157/2546] Loss: 4.0640\n",
            "[Batch 158/2546] Loss: 4.1491\n",
            "[Batch 159/2546] Loss: 4.1209\n",
            "[Batch 160/2546] Loss: 4.1314\n",
            "[Batch 161/2546] Loss: 4.2380\n",
            "[Batch 162/2546] Loss: 4.2248\n",
            "[Batch 163/2546] Loss: 4.1763\n",
            "[Batch 164/2546] Loss: 3.9671\n",
            "[Batch 165/2546] Loss: 4.0817\n",
            "[Batch 166/2546] Loss: 4.3905\n",
            "[Batch 167/2546] Loss: 4.0852\n",
            "[Batch 168/2546] Loss: 4.0548\n",
            "[Batch 169/2546] Loss: 4.2264\n",
            "[Batch 170/2546] Loss: 4.2088\n",
            "[Batch 171/2546] Loss: 3.8873\n",
            "[Batch 172/2546] Loss: 4.2070\n",
            "[Batch 173/2546] Loss: 3.9635\n",
            "[Batch 174/2546] Loss: 4.0535\n",
            "[Batch 175/2546] Loss: 3.9477\n",
            "[Batch 176/2546] Loss: 4.0185\n",
            "[Batch 177/2546] Loss: 4.0581\n",
            "[Batch 178/2546] Loss: 4.2627\n",
            "[Batch 179/2546] Loss: 3.9264\n",
            "[Batch 180/2546] Loss: 4.0451\n",
            "[Batch 181/2546] Loss: 4.1503\n",
            "[Batch 182/2546] Loss: 4.0267\n",
            "[Batch 183/2546] Loss: 3.9761\n",
            "[Batch 184/2546] Loss: 4.2056\n",
            "[Batch 185/2546] Loss: 4.1035\n",
            "[Batch 186/2546] Loss: 4.0849\n",
            "[Batch 187/2546] Loss: 3.9973\n",
            "[Batch 188/2546] Loss: 3.9362\n",
            "[Batch 189/2546] Loss: 4.0811\n",
            "[Batch 190/2546] Loss: 3.9478\n",
            "[Batch 191/2546] Loss: 4.1589\n",
            "[Batch 192/2546] Loss: 4.0380\n",
            "[Batch 193/2546] Loss: 4.1368\n",
            "[Batch 194/2546] Loss: 3.9400\n",
            "[Batch 195/2546] Loss: 4.0556\n",
            "[Batch 196/2546] Loss: 4.0421\n",
            "[Batch 197/2546] Loss: 3.9886\n",
            "[Batch 198/2546] Loss: 3.9900\n",
            "[Batch 199/2546] Loss: 3.9444\n",
            "[Batch 200/2546] Loss: 3.8711\n",
            "[Batch 201/2546] Loss: 3.8716\n",
            "[Batch 202/2546] Loss: 4.0248\n",
            "[Batch 203/2546] Loss: 4.0228\n",
            "[Batch 204/2546] Loss: 3.9388\n",
            "[Batch 205/2546] Loss: 4.0908\n",
            "[Batch 206/2546] Loss: 3.9296\n",
            "[Batch 207/2546] Loss: 3.8389\n",
            "[Batch 208/2546] Loss: 4.1232\n",
            "[Batch 209/2546] Loss: 3.8629\n",
            "[Batch 210/2546] Loss: 3.9550\n",
            "[Batch 211/2546] Loss: 3.8764\n",
            "[Batch 212/2546] Loss: 3.8604\n",
            "[Batch 213/2546] Loss: 3.9175\n",
            "[Batch 214/2546] Loss: 3.8287\n",
            "[Batch 215/2546] Loss: 4.0106\n",
            "[Batch 216/2546] Loss: 3.8903\n",
            "[Batch 217/2546] Loss: 3.7783\n",
            "[Batch 218/2546] Loss: 3.9077\n",
            "[Batch 219/2546] Loss: 4.0271\n",
            "[Batch 220/2546] Loss: 3.9327\n",
            "[Batch 221/2546] Loss: 3.7215\n",
            "[Batch 222/2546] Loss: 3.9820\n",
            "[Batch 223/2546] Loss: 3.9872\n",
            "[Batch 224/2546] Loss: 3.9345\n",
            "[Batch 225/2546] Loss: 3.8007\n",
            "[Batch 226/2546] Loss: 4.0596\n",
            "[Batch 227/2546] Loss: 3.8407\n",
            "[Batch 228/2546] Loss: 4.0460\n",
            "[Batch 229/2546] Loss: 3.9665\n",
            "[Batch 230/2546] Loss: 4.0141\n",
            "[Batch 231/2546] Loss: 3.9340\n",
            "[Batch 232/2546] Loss: 3.9176\n",
            "[Batch 233/2546] Loss: 3.7190\n",
            "[Batch 234/2546] Loss: 3.7908\n",
            "[Batch 235/2546] Loss: 3.8593\n",
            "[Batch 236/2546] Loss: 4.1281\n",
            "[Batch 237/2546] Loss: 3.6679\n",
            "[Batch 238/2546] Loss: 3.7489\n",
            "[Batch 239/2546] Loss: 4.0393\n",
            "[Batch 240/2546] Loss: 3.7892\n",
            "[Batch 241/2546] Loss: 3.7600\n",
            "[Batch 242/2546] Loss: 3.6758\n",
            "[Batch 243/2546] Loss: 3.8634\n",
            "[Batch 244/2546] Loss: 4.0368\n",
            "[Batch 245/2546] Loss: 3.7939\n",
            "[Batch 246/2546] Loss: 3.9946\n",
            "[Batch 247/2546] Loss: 4.1039\n",
            "[Batch 248/2546] Loss: 3.8897\n",
            "[Batch 249/2546] Loss: 3.9888\n",
            "[Batch 250/2546] Loss: 3.7696\n",
            "[Batch 251/2546] Loss: 3.7753\n",
            "[Batch 252/2546] Loss: 3.9964\n",
            "[Batch 253/2546] Loss: 3.8204\n",
            "[Batch 254/2546] Loss: 4.0710\n",
            "[Batch 255/2546] Loss: 3.8869\n",
            "[Batch 256/2546] Loss: 3.7674\n",
            "[Batch 257/2546] Loss: 4.0579\n",
            "[Batch 258/2546] Loss: 3.7413\n",
            "[Batch 259/2546] Loss: 3.9078\n",
            "[Batch 260/2546] Loss: 3.8768\n",
            "[Batch 261/2546] Loss: 3.8671\n",
            "[Batch 262/2546] Loss: 3.8819\n",
            "[Batch 263/2546] Loss: 3.9320\n",
            "[Batch 264/2546] Loss: 4.0604\n",
            "[Batch 265/2546] Loss: 3.9306\n",
            "[Batch 266/2546] Loss: 4.0580\n",
            "[Batch 267/2546] Loss: 3.9314\n",
            "[Batch 268/2546] Loss: 3.6768\n",
            "[Batch 269/2546] Loss: 3.9221\n",
            "[Batch 270/2546] Loss: 3.9298\n",
            "[Batch 271/2546] Loss: 3.6697\n",
            "[Batch 272/2546] Loss: 3.9415\n",
            "[Batch 273/2546] Loss: 3.7722\n",
            "[Batch 274/2546] Loss: 3.9921\n",
            "[Batch 275/2546] Loss: 3.8332\n",
            "[Batch 276/2546] Loss: 3.9427\n",
            "[Batch 277/2546] Loss: 4.0638\n",
            "[Batch 278/2546] Loss: 3.9010\n",
            "[Batch 279/2546] Loss: 3.5520\n",
            "[Batch 280/2546] Loss: 3.7790\n",
            "[Batch 281/2546] Loss: 3.8708\n",
            "[Batch 282/2546] Loss: 3.7677\n",
            "[Batch 283/2546] Loss: 3.9125\n",
            "[Batch 284/2546] Loss: 3.7238\n",
            "[Batch 285/2546] Loss: 3.8156\n",
            "[Batch 286/2546] Loss: 3.7023\n",
            "[Batch 287/2546] Loss: 3.7818\n",
            "[Batch 288/2546] Loss: 3.6673\n",
            "[Batch 289/2546] Loss: 4.0585\n",
            "[Batch 290/2546] Loss: 3.8696\n",
            "[Batch 291/2546] Loss: 3.9301\n",
            "[Batch 292/2546] Loss: 3.6680\n",
            "[Batch 293/2546] Loss: 3.7580\n",
            "[Batch 294/2546] Loss: 3.8841\n",
            "[Batch 295/2546] Loss: 3.8206\n",
            "[Batch 296/2546] Loss: 3.9475\n",
            "[Batch 297/2546] Loss: 3.9606\n",
            "[Batch 298/2546] Loss: 3.9350\n",
            "[Batch 299/2546] Loss: 3.7343\n",
            "[Batch 300/2546] Loss: 3.7170\n",
            "[Batch 301/2546] Loss: 3.7849\n",
            "[Batch 302/2546] Loss: 3.9058\n",
            "[Batch 303/2546] Loss: 3.9200\n",
            "[Batch 304/2546] Loss: 4.0212\n",
            "[Batch 305/2546] Loss: 3.8346\n",
            "[Batch 306/2546] Loss: 3.9864\n",
            "[Batch 307/2546] Loss: 3.7802\n",
            "[Batch 308/2546] Loss: 3.6680\n",
            "[Batch 309/2546] Loss: 3.7690\n",
            "[Batch 310/2546] Loss: 3.7896\n",
            "[Batch 311/2546] Loss: 3.7669\n",
            "[Batch 312/2546] Loss: 3.9037\n",
            "[Batch 313/2546] Loss: 3.8504\n",
            "[Batch 314/2546] Loss: 3.7712\n",
            "[Batch 315/2546] Loss: 3.7878\n",
            "[Batch 316/2546] Loss: 3.8199\n",
            "[Batch 317/2546] Loss: 3.7372\n",
            "[Batch 318/2546] Loss: 3.7683\n",
            "[Batch 319/2546] Loss: 3.6761\n",
            "[Batch 320/2546] Loss: 4.0798\n",
            "[Batch 321/2546] Loss: 3.7570\n",
            "[Batch 322/2546] Loss: 3.8272\n",
            "[Batch 323/2546] Loss: 3.9799\n",
            "[Batch 324/2546] Loss: 3.7686\n",
            "[Batch 325/2546] Loss: 3.8535\n",
            "[Batch 326/2546] Loss: 3.7257\n",
            "[Batch 327/2546] Loss: 3.7615\n",
            "[Batch 328/2546] Loss: 3.7887\n",
            "[Batch 329/2546] Loss: 3.6724\n",
            "[Batch 330/2546] Loss: 3.7099\n",
            "[Batch 331/2546] Loss: 3.6175\n",
            "[Batch 332/2546] Loss: 3.8020\n",
            "[Batch 333/2546] Loss: 3.8458\n",
            "[Batch 334/2546] Loss: 3.7264\n",
            "[Batch 335/2546] Loss: 4.0535\n",
            "[Batch 336/2546] Loss: 3.6981\n",
            "[Batch 337/2546] Loss: 3.8191\n",
            "[Batch 338/2546] Loss: 3.7527\n",
            "[Batch 339/2546] Loss: 3.7088\n",
            "[Batch 340/2546] Loss: 3.7484\n",
            "[Batch 341/2546] Loss: 3.6753\n",
            "[Batch 342/2546] Loss: 3.7973\n",
            "[Batch 343/2546] Loss: 3.7433\n",
            "[Batch 344/2546] Loss: 3.6804\n",
            "[Batch 345/2546] Loss: 3.7438\n",
            "[Batch 346/2546] Loss: 3.6579\n",
            "[Batch 347/2546] Loss: 3.7302\n",
            "[Batch 348/2546] Loss: 3.9228\n",
            "[Batch 349/2546] Loss: 3.7808\n",
            "[Batch 350/2546] Loss: 3.7318\n",
            "[Batch 351/2546] Loss: 3.8085\n",
            "[Batch 352/2546] Loss: 3.7437\n",
            "[Batch 353/2546] Loss: 3.6387\n",
            "[Batch 354/2546] Loss: 3.5117\n",
            "[Batch 355/2546] Loss: 3.6965\n",
            "[Batch 356/2546] Loss: 3.7256\n",
            "[Batch 357/2546] Loss: 3.8472\n",
            "[Batch 358/2546] Loss: 3.8834\n",
            "[Batch 359/2546] Loss: 3.6767\n",
            "[Batch 360/2546] Loss: 3.8353\n",
            "[Batch 361/2546] Loss: 3.6027\n",
            "[Batch 362/2546] Loss: 3.9634\n",
            "[Batch 363/2546] Loss: 3.5265\n",
            "[Batch 364/2546] Loss: 3.6204\n",
            "[Batch 365/2546] Loss: 3.8072\n",
            "[Batch 366/2546] Loss: 3.6259\n",
            "[Batch 367/2546] Loss: 3.6285\n",
            "[Batch 368/2546] Loss: 3.7857\n",
            "[Batch 369/2546] Loss: 3.7143\n",
            "[Batch 370/2546] Loss: 3.8071\n",
            "[Batch 371/2546] Loss: 3.7380\n",
            "[Batch 372/2546] Loss: 3.8187\n",
            "[Batch 373/2546] Loss: 3.7052\n",
            "[Batch 374/2546] Loss: 3.4783\n",
            "[Batch 375/2546] Loss: 3.5960\n",
            "[Batch 376/2546] Loss: 3.9641\n",
            "[Batch 377/2546] Loss: 3.7746\n",
            "[Batch 378/2546] Loss: 3.7949\n",
            "[Batch 379/2546] Loss: 3.5626\n",
            "[Batch 380/2546] Loss: 3.7158\n",
            "[Batch 381/2546] Loss: 3.7269\n",
            "[Batch 382/2546] Loss: 3.8702\n",
            "[Batch 383/2546] Loss: 3.9559\n",
            "[Batch 384/2546] Loss: 3.6140\n",
            "[Batch 385/2546] Loss: 3.9750\n",
            "[Batch 386/2546] Loss: 3.7995\n",
            "[Batch 387/2546] Loss: 3.6767\n",
            "[Batch 388/2546] Loss: 3.8153\n",
            "[Batch 389/2546] Loss: 3.6675\n",
            "[Batch 390/2546] Loss: 3.6098\n",
            "[Batch 391/2546] Loss: 3.5744\n",
            "[Batch 392/2546] Loss: 3.6163\n",
            "[Batch 393/2546] Loss: 3.5855\n",
            "[Batch 394/2546] Loss: 3.9158\n",
            "[Batch 395/2546] Loss: 3.4252\n",
            "[Batch 396/2546] Loss: 3.6346\n",
            "[Batch 397/2546] Loss: 3.4996\n",
            "[Batch 398/2546] Loss: 3.5566\n",
            "[Batch 399/2546] Loss: 3.7110\n",
            "[Batch 400/2546] Loss: 3.7409\n",
            "[Batch 401/2546] Loss: 3.8005\n",
            "[Batch 402/2546] Loss: 3.6950\n",
            "[Batch 403/2546] Loss: 3.8293\n",
            "[Batch 404/2546] Loss: 3.7457\n",
            "[Batch 405/2546] Loss: 3.8204\n",
            "[Batch 406/2546] Loss: 3.5939\n",
            "[Batch 407/2546] Loss: 3.8119\n",
            "[Batch 408/2546] Loss: 3.7248\n",
            "[Batch 409/2546] Loss: 3.7433\n",
            "[Batch 410/2546] Loss: 3.5387\n",
            "[Batch 411/2546] Loss: 3.6133\n",
            "[Batch 412/2546] Loss: 3.6275\n",
            "[Batch 413/2546] Loss: 3.4806\n",
            "[Batch 414/2546] Loss: 3.6995\n",
            "[Batch 415/2546] Loss: 3.4749\n",
            "[Batch 416/2546] Loss: 3.6191\n",
            "[Batch 417/2546] Loss: 3.6379\n",
            "[Batch 418/2546] Loss: 3.6348\n",
            "[Batch 419/2546] Loss: 3.5552\n",
            "[Batch 420/2546] Loss: 3.6508\n",
            "[Batch 421/2546] Loss: 3.6574\n",
            "[Batch 422/2546] Loss: 3.4592\n",
            "[Batch 423/2546] Loss: 3.7690\n",
            "[Batch 424/2546] Loss: 3.8517\n",
            "[Batch 425/2546] Loss: 3.5632\n",
            "[Batch 426/2546] Loss: 3.6661\n",
            "[Batch 427/2546] Loss: 3.6490\n",
            "[Batch 428/2546] Loss: 3.6997\n",
            "[Batch 429/2546] Loss: 3.6545\n",
            "[Batch 430/2546] Loss: 3.7192\n",
            "[Batch 431/2546] Loss: 3.6810\n",
            "[Batch 432/2546] Loss: 3.6190\n",
            "[Batch 433/2546] Loss: 3.8902\n",
            "[Batch 434/2546] Loss: 3.7527\n",
            "[Batch 435/2546] Loss: 3.6962\n",
            "[Batch 436/2546] Loss: 3.5012\n",
            "[Batch 437/2546] Loss: 3.7266\n",
            "[Batch 438/2546] Loss: 3.7585\n",
            "[Batch 439/2546] Loss: 3.6133\n",
            "[Batch 440/2546] Loss: 3.5232\n",
            "[Batch 441/2546] Loss: 3.6154\n",
            "[Batch 442/2546] Loss: 3.4246\n",
            "[Batch 443/2546] Loss: 3.4364\n",
            "[Batch 444/2546] Loss: 3.7340\n",
            "[Batch 445/2546] Loss: 3.5714\n",
            "[Batch 446/2546] Loss: 3.5070\n",
            "[Batch 447/2546] Loss: 3.5869\n",
            "[Batch 448/2546] Loss: 3.7151\n",
            "[Batch 449/2546] Loss: 3.5963\n",
            "[Batch 450/2546] Loss: 3.4920\n",
            "[Batch 451/2546] Loss: 3.6738\n",
            "[Batch 452/2546] Loss: 3.5869\n",
            "[Batch 453/2546] Loss: 3.6723\n",
            "[Batch 454/2546] Loss: 3.6135\n",
            "[Batch 455/2546] Loss: 3.7006\n",
            "[Batch 456/2546] Loss: 3.5653\n",
            "[Batch 457/2546] Loss: 3.5352\n",
            "[Batch 458/2546] Loss: 3.8149\n",
            "[Batch 459/2546] Loss: 3.7220\n",
            "[Batch 460/2546] Loss: 3.6955\n",
            "[Batch 461/2546] Loss: 3.6609\n",
            "[Batch 462/2546] Loss: 3.7569\n",
            "[Batch 463/2546] Loss: 3.5543\n",
            "[Batch 464/2546] Loss: 3.4911\n",
            "[Batch 465/2546] Loss: 3.6538\n",
            "[Batch 466/2546] Loss: 3.5777\n",
            "[Batch 467/2546] Loss: 3.5805\n",
            "[Batch 468/2546] Loss: 3.4853\n",
            "[Batch 469/2546] Loss: 3.5396\n",
            "[Batch 470/2546] Loss: 3.5362\n",
            "[Batch 471/2546] Loss: 3.5512\n",
            "[Batch 472/2546] Loss: 3.8134\n",
            "[Batch 473/2546] Loss: 3.4044\n",
            "[Batch 474/2546] Loss: 3.8439\n",
            "[Batch 475/2546] Loss: 3.6279\n",
            "[Batch 476/2546] Loss: 3.6696\n",
            "[Batch 477/2546] Loss: 3.5784\n",
            "[Batch 478/2546] Loss: 3.5608\n",
            "[Batch 479/2546] Loss: 3.6357\n",
            "[Batch 480/2546] Loss: 3.4662\n",
            "[Batch 481/2546] Loss: 3.7992\n",
            "[Batch 482/2546] Loss: 3.5084\n",
            "[Batch 483/2546] Loss: 3.7541\n",
            "[Batch 484/2546] Loss: 3.8931\n",
            "[Batch 485/2546] Loss: 3.4904\n",
            "[Batch 486/2546] Loss: 3.5439\n",
            "[Batch 487/2546] Loss: 3.6044\n",
            "[Batch 488/2546] Loss: 3.6911\n",
            "[Batch 489/2546] Loss: 3.7233\n",
            "[Batch 490/2546] Loss: 3.7316\n",
            "[Batch 491/2546] Loss: 3.4979\n",
            "[Batch 492/2546] Loss: 3.7656\n",
            "[Batch 493/2546] Loss: 3.5852\n",
            "[Batch 494/2546] Loss: 3.4966\n",
            "[Batch 495/2546] Loss: 3.4311\n",
            "[Batch 496/2546] Loss: 3.6876\n",
            "[Batch 497/2546] Loss: 3.5860\n",
            "[Batch 498/2546] Loss: 3.5330\n",
            "[Batch 499/2546] Loss: 3.6177\n",
            "[Batch 500/2546] Loss: 3.5009\n",
            "[Batch 501/2546] Loss: 3.7778\n",
            "[Batch 502/2546] Loss: 3.5160\n",
            "[Batch 503/2546] Loss: 3.4620\n",
            "[Batch 504/2546] Loss: 3.6673\n",
            "[Batch 505/2546] Loss: 3.4971\n",
            "[Batch 506/2546] Loss: 3.6619\n",
            "[Batch 507/2546] Loss: 3.5749\n",
            "[Batch 508/2546] Loss: 3.6597\n",
            "[Batch 509/2546] Loss: 3.6688\n",
            "[Batch 510/2546] Loss: 3.5233\n",
            "[Batch 511/2546] Loss: 3.7382\n",
            "[Batch 512/2546] Loss: 3.4652\n",
            "[Batch 513/2546] Loss: 3.6543\n",
            "[Batch 514/2546] Loss: 3.6112\n",
            "[Batch 515/2546] Loss: 3.8107\n",
            "[Batch 516/2546] Loss: 3.5054\n",
            "[Batch 517/2546] Loss: 3.6639\n",
            "[Batch 518/2546] Loss: 3.3264\n",
            "[Batch 519/2546] Loss: 3.5032\n",
            "[Batch 520/2546] Loss: 3.6744\n",
            "[Batch 521/2546] Loss: 3.5208\n",
            "[Batch 522/2546] Loss: 3.6007\n",
            "[Batch 523/2546] Loss: 3.6132\n",
            "[Batch 524/2546] Loss: 3.4597\n",
            "[Batch 525/2546] Loss: 3.6537\n",
            "[Batch 526/2546] Loss: 3.6596\n",
            "[Batch 527/2546] Loss: 3.6115\n",
            "[Batch 528/2546] Loss: 3.6541\n",
            "[Batch 529/2546] Loss: 3.4055\n",
            "[Batch 530/2546] Loss: 3.6546\n",
            "[Batch 531/2546] Loss: 3.5018\n",
            "[Batch 532/2546] Loss: 3.4837\n",
            "[Batch 533/2546] Loss: 3.6228\n",
            "[Batch 534/2546] Loss: 3.3928\n",
            "[Batch 535/2546] Loss: 3.4511\n",
            "[Batch 536/2546] Loss: 3.5806\n",
            "[Batch 537/2546] Loss: 3.3120\n",
            "[Batch 538/2546] Loss: 3.6287\n",
            "[Batch 539/2546] Loss: 3.3871\n",
            "[Batch 540/2546] Loss: 3.6908\n",
            "[Batch 541/2546] Loss: 3.5155\n",
            "[Batch 542/2546] Loss: 3.5068\n",
            "[Batch 543/2546] Loss: 3.4596\n",
            "[Batch 544/2546] Loss: 3.4835\n",
            "[Batch 545/2546] Loss: 3.6981\n",
            "[Batch 546/2546] Loss: 3.7253\n",
            "[Batch 547/2546] Loss: 3.7913\n",
            "[Batch 548/2546] Loss: 3.5643\n",
            "[Batch 549/2546] Loss: 3.2217\n",
            "[Batch 550/2546] Loss: 3.4777\n",
            "[Batch 551/2546] Loss: 3.5707\n",
            "[Batch 552/2546] Loss: 3.5811\n",
            "[Batch 553/2546] Loss: 3.4876\n",
            "[Batch 554/2546] Loss: 3.9787\n",
            "[Batch 555/2546] Loss: 3.6106\n",
            "[Batch 556/2546] Loss: 3.7489\n",
            "[Batch 557/2546] Loss: 3.5252\n",
            "[Batch 558/2546] Loss: 3.5356\n",
            "[Batch 559/2546] Loss: 3.4265\n",
            "[Batch 560/2546] Loss: 3.6824\n",
            "[Batch 561/2546] Loss: 3.5237\n",
            "[Batch 562/2546] Loss: 3.4820\n",
            "[Batch 563/2546] Loss: 3.6631\n",
            "[Batch 564/2546] Loss: 3.6996\n",
            "[Batch 565/2546] Loss: 3.2139\n",
            "[Batch 566/2546] Loss: 3.5269\n",
            "[Batch 567/2546] Loss: 3.4893\n",
            "[Batch 568/2546] Loss: 3.7716\n",
            "[Batch 569/2546] Loss: 3.6953\n",
            "[Batch 570/2546] Loss: 3.7064\n",
            "[Batch 571/2546] Loss: 3.5362\n",
            "[Batch 572/2546] Loss: 3.8377\n",
            "[Batch 573/2546] Loss: 3.6954\n",
            "[Batch 574/2546] Loss: 3.4919\n",
            "[Batch 575/2546] Loss: 3.6082\n",
            "[Batch 576/2546] Loss: 3.5464\n",
            "[Batch 577/2546] Loss: 3.5820\n",
            "[Batch 578/2546] Loss: 3.5323\n",
            "[Batch 579/2546] Loss: 3.5025\n",
            "[Batch 580/2546] Loss: 3.4029\n",
            "[Batch 581/2546] Loss: 3.3349\n",
            "[Batch 582/2546] Loss: 3.3763\n",
            "[Batch 583/2546] Loss: 3.4213\n",
            "[Batch 584/2546] Loss: 3.7290\n",
            "[Batch 585/2546] Loss: 3.4507\n",
            "[Batch 586/2546] Loss: 3.5081\n",
            "[Batch 587/2546] Loss: 3.6999\n",
            "[Batch 588/2546] Loss: 3.4781\n",
            "[Batch 589/2546] Loss: 3.3473\n",
            "[Batch 590/2546] Loss: 3.5499\n",
            "[Batch 591/2546] Loss: 3.5445\n",
            "[Batch 592/2546] Loss: 3.3743\n",
            "[Batch 593/2546] Loss: 3.5223\n",
            "[Batch 594/2546] Loss: 3.4854\n",
            "[Batch 595/2546] Loss: 3.5610\n",
            "[Batch 596/2546] Loss: 3.3810\n",
            "[Batch 597/2546] Loss: 3.4596\n",
            "[Batch 598/2546] Loss: 3.5741\n",
            "[Batch 599/2546] Loss: 3.4833\n",
            "[Batch 600/2546] Loss: 3.3553\n",
            "[Batch 601/2546] Loss: 3.4606\n",
            "[Batch 602/2546] Loss: 3.6673\n",
            "[Batch 603/2546] Loss: 3.4681\n",
            "[Batch 604/2546] Loss: 3.4735\n",
            "[Batch 605/2546] Loss: 3.6484\n",
            "[Batch 606/2546] Loss: 3.4770\n",
            "[Batch 607/2546] Loss: 3.3498\n",
            "[Batch 608/2546] Loss: 3.3719\n",
            "[Batch 609/2546] Loss: 3.4674\n",
            "[Batch 610/2546] Loss: 3.3449\n",
            "[Batch 611/2546] Loss: 3.7536\n",
            "[Batch 612/2546] Loss: 3.3850\n",
            "[Batch 613/2546] Loss: 3.4200\n",
            "[Batch 614/2546] Loss: 3.6540\n",
            "[Batch 615/2546] Loss: 3.3212\n",
            "[Batch 616/2546] Loss: 3.4258\n",
            "[Batch 617/2546] Loss: 3.2357\n",
            "[Batch 618/2546] Loss: 3.5437\n",
            "[Batch 619/2546] Loss: 3.3815\n",
            "[Batch 620/2546] Loss: 3.5655\n",
            "[Batch 621/2546] Loss: 3.7208\n",
            "[Batch 622/2546] Loss: 3.5427\n",
            "[Batch 623/2546] Loss: 3.4527\n",
            "[Batch 624/2546] Loss: 3.5090\n",
            "[Batch 625/2546] Loss: 3.5166\n",
            "[Batch 626/2546] Loss: 3.3507\n",
            "[Batch 627/2546] Loss: 3.5588\n",
            "[Batch 628/2546] Loss: 3.4731\n",
            "[Batch 629/2546] Loss: 3.3639\n",
            "[Batch 630/2546] Loss: 3.2789\n",
            "[Batch 631/2546] Loss: 3.6376\n",
            "[Batch 632/2546] Loss: 3.5239\n",
            "[Batch 633/2546] Loss: 3.5071\n",
            "[Batch 634/2546] Loss: 3.6075\n",
            "[Batch 635/2546] Loss: 3.3682\n",
            "[Batch 636/2546] Loss: 3.2782\n",
            "[Batch 637/2546] Loss: 3.4395\n",
            "[Batch 638/2546] Loss: 3.3229\n",
            "[Batch 639/2546] Loss: 3.4373\n",
            "[Batch 640/2546] Loss: 3.6712\n",
            "[Batch 641/2546] Loss: 3.4246\n",
            "[Batch 642/2546] Loss: 3.5462\n",
            "[Batch 643/2546] Loss: 3.4371\n",
            "[Batch 644/2546] Loss: 3.4184\n",
            "[Batch 645/2546] Loss: 3.5000\n",
            "[Batch 646/2546] Loss: 3.4388\n",
            "[Batch 647/2546] Loss: 3.5410\n",
            "[Batch 648/2546] Loss: 3.3796\n",
            "[Batch 649/2546] Loss: 3.4780\n",
            "[Batch 650/2546] Loss: 3.6142\n",
            "[Batch 651/2546] Loss: 3.4939\n",
            "[Batch 652/2546] Loss: 3.5536\n",
            "[Batch 653/2546] Loss: 3.4318\n",
            "[Batch 654/2546] Loss: 3.3555\n",
            "[Batch 655/2546] Loss: 3.4839\n",
            "[Batch 656/2546] Loss: 3.3847\n",
            "[Batch 657/2546] Loss: 3.6484\n",
            "[Batch 658/2546] Loss: 3.4171\n",
            "[Batch 659/2546] Loss: 3.4056\n",
            "[Batch 660/2546] Loss: 3.5013\n",
            "[Batch 661/2546] Loss: 3.5279\n",
            "[Batch 662/2546] Loss: 3.4573\n",
            "[Batch 663/2546] Loss: 3.4858\n",
            "[Batch 664/2546] Loss: 3.2995\n",
            "[Batch 665/2546] Loss: 3.3517\n",
            "[Batch 666/2546] Loss: 3.4436\n",
            "[Batch 667/2546] Loss: 3.3950\n",
            "[Batch 668/2546] Loss: 3.5044\n",
            "[Batch 669/2546] Loss: 3.4235\n",
            "[Batch 670/2546] Loss: 3.5834\n",
            "[Batch 671/2546] Loss: 3.4710\n",
            "[Batch 672/2546] Loss: 3.4768\n",
            "[Batch 673/2546] Loss: 3.4107\n",
            "[Batch 674/2546] Loss: 3.6562\n",
            "[Batch 675/2546] Loss: 3.5076\n",
            "[Batch 676/2546] Loss: 3.3479\n",
            "[Batch 677/2546] Loss: 3.2906\n",
            "[Batch 678/2546] Loss: 3.4310\n",
            "[Batch 679/2546] Loss: 3.7685\n",
            "[Batch 680/2546] Loss: 3.3973\n",
            "[Batch 681/2546] Loss: 3.7361\n",
            "[Batch 682/2546] Loss: 3.2194\n",
            "[Batch 683/2546] Loss: 3.3089\n",
            "[Batch 684/2546] Loss: 3.4841\n",
            "[Batch 685/2546] Loss: 3.5252\n",
            "[Batch 686/2546] Loss: 3.3960\n",
            "[Batch 687/2546] Loss: 3.3954\n",
            "[Batch 688/2546] Loss: 3.4228\n",
            "[Batch 689/2546] Loss: 3.2131\n",
            "[Batch 690/2546] Loss: 3.8828\n",
            "[Batch 691/2546] Loss: 3.2314\n",
            "[Batch 692/2546] Loss: 3.2502\n",
            "[Batch 693/2546] Loss: 3.7135\n",
            "[Batch 694/2546] Loss: 3.4585\n",
            "[Batch 695/2546] Loss: 3.4290\n",
            "[Batch 696/2546] Loss: 3.4582\n",
            "[Batch 697/2546] Loss: 3.2631\n",
            "[Batch 698/2546] Loss: 3.3815\n",
            "[Batch 699/2546] Loss: 3.3903\n",
            "[Batch 700/2546] Loss: 3.4880\n",
            "[Batch 701/2546] Loss: 3.4556\n",
            "[Batch 702/2546] Loss: 3.5699\n",
            "[Batch 703/2546] Loss: 3.4120\n",
            "[Batch 704/2546] Loss: 3.6591\n",
            "[Batch 705/2546] Loss: 3.3179\n",
            "[Batch 706/2546] Loss: 3.5019\n",
            "[Batch 707/2546] Loss: 3.5344\n",
            "[Batch 708/2546] Loss: 3.3435\n",
            "[Batch 709/2546] Loss: 3.3722\n",
            "[Batch 710/2546] Loss: 3.6786\n",
            "[Batch 711/2546] Loss: 3.2175\n"
          ]
        }
      ],
      "source": [
        "for model_type, config in experiment_configs.items():\n",
        "    print(f\"=== Evaluating {model_type} ===\")\n",
        "    model, encoder, decoder, _, save_model_path, save_history_path, test_loader, recipe_vocab = main(\n",
        "        model_type, config, train_df, dev_df, test_df\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=recipe_vocab['<pad>'])\n",
        "    evaluate_and_report(model_type, config, encoder, decoder, save_model_path, save_history_path,\n",
        "                        test_loader, criterion, recipe_vocab)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded token cache.\n",
            "🔎 Loading GloVe vectors...\n",
            "✅ GloVe embedding matrix created.\n",
            "Training model: mild_extension2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91c747ae943a453ab100c41e414e6558",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2876b97f36f347b8bb9483d4e66d368b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training batches:   0%|          | 0/2546 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Batch 1/2546] Loss: 9.0635\n",
            "[Batch 2/2546] Loss: 8.9949\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[139], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmild_extension2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m experiment_configs[model_type]\n\u001b[0;32m----> 4\u001b[0m model, encoder, decoder, loss_history, save_model_path, save_history_path \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdev_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_df\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[137], line 70\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(model_type, config, train_df, dev_df, test_df, glove_path)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLR\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 70\u001b[0m     loss_history \u001b[38;5;241m=\u001b[39m \u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEPOCHS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTRAIN_RATIO\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_history_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_history_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTEACHER_FORCING_RATIO\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTEACHER_FORCING_RATIO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mMAX_LEN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMAX_LEN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mLR_STEP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLR_STEP\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mLR_GAMMA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLR_GAMMA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, encoder, decoder, loss_history, save_model_path, save_history_path, test_loader, recipe_vocab\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "Cell \u001b[0;32mIn[136], line 53\u001b[0m, in \u001b[0;36mTrain\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, EPOCHS, BATCH_SIZE, TRAIN_RATIO, save_model_path, save_history_path, TEACHER_FORCING_RATIO, MAX_LEN, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m     52\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 53\u001b[0m train_epoch_loss, train_batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEACHER_FORCING_RATIO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_LEN\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m loss_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_epoch_loss)\n\u001b[1;32m     59\u001b[0m loss_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_iter\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_batch_loss)\n",
            "Cell \u001b[0;32mIn[136], line 18\u001b[0m, in \u001b[0;36mloss_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, teacher_forcing_ratio, max_len)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[0;32m~/Desktop/FIT5217_NLP/NLP/.venv/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/FIT5217_NLP/NLP/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model_type = \"mild_extension2\"\n",
        "config = experiment_configs[model_type]\n",
        "\n",
        "model, encoder, decoder, loss_history, save_model_path, save_history_path = main(\n",
        "    model_type,\n",
        "    config,\n",
        "    train_df,\n",
        "    dev_df,\n",
        "    test_df\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model was saved at Epoch 4\n",
            "Train Loss: 4.2405 | Val Loss: 5.6251\n",
            "Total training time : 16108.31 s\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 저장된 모델과 학습 이력 불러오기\n",
        "checkpoint = torch.load(save_model_path, map_location=DEVICE)\n",
        "checkpoint2 = torch.load(save_history_path, map_location=DEVICE)\n",
        "\n",
        "# 같은 구조로 모델 재생성 후 state_dict 로드\n",
        "load_baseline1 = Seq2Seq(encoder, decoder, DEVICE, use_attention=config[\"USE_ATTENTION\"]).to(DEVICE)\n",
        "load_baseline1.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "# 저장된 이력 중 loss history 가져오기\n",
        "loss_history_loss_history_baseline1 = checkpoint2[\"loss_history\"]\n",
        "train_elapsed_time = checkpoint2[\"train_elapsed_time\"]\n",
        "\n",
        "#  베스트 모델 정보 출력\n",
        "best_epoch = checkpoint[\"epoch\"] + 1  \n",
        "best_train_loss = checkpoint[\"train_loss\"]\n",
        "best_val_loss = checkpoint[\"val_loss\"]\n",
        "print(f\"Best model was saved at Epoch {best_epoch}\")\n",
        "print(f\"Train Loss: {best_train_loss:.4f} | Val Loss: {best_val_loss:.4f}\")\n",
        "print(f\"Total training time : {train_elapsed_time:.2f} s\")\n",
        "\n",
        "# 그래프 그리기\n",
        "plot_loss_epoch(model_type, loss_history_loss_history_baseline1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96929fc8e6e24f6ca58507e7a12feaaf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training batches:   0%|          | 0/17 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Batch 1/17] Loss: 5.6199\n",
            "[Batch 2/17] Loss: 5.5173\n",
            "[Batch 3/17] Loss: 5.6445\n",
            "[Batch 4/17] Loss: 5.6818\n",
            "[Batch 5/17] Loss: 5.7356\n",
            "[Batch 6/17] Loss: 5.5516\n",
            "[Batch 7/17] Loss: 5.5700\n",
            "[Batch 8/17] Loss: 5.6573\n",
            "[Batch 9/17] Loss: 5.3945\n",
            "[Batch 10/17] Loss: 5.4296\n",
            "[Batch 11/17] Loss: 5.7314\n",
            "[Batch 12/17] Loss: 5.5915\n",
            "[Batch 13/17] Loss: 5.6364\n",
            "[Batch 14/17] Loss: 5.4893\n",
            "[Batch 15/17] Loss: 5.7598\n",
            "[Batch 16/17] Loss: 5.7112\n",
            "[Batch 17/17] Loss: 5.8299\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00702c11aaf945e2afc31827afae216f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating Metrics:   0%|          | 0/17 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss      : 5.6193\n",
            "BLEU-4 Score   : 0.0371\n",
            "METEOR Score   : 0.1711\n",
            "BERTScore (F1) : 0.8249\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(5.619320619337874, 0.0370714945481254, 0.171089240652591, 0.8249096274375916)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 이제 Test 가능\n",
        "Test(load_baseline1, test_loader, criterion, recipe_vocab, MAX_LEN=config[\"MAX_LEN\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "model_type = \"baseline2\"\n",
        "config = experiment_configs[model_type]\n",
        "\n",
        "model, encoder, decoder, loss_history, save_model_path, save_history_path = run_train(model_type, config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seq2Seq(\n",
            "  (encoder): Encoder_GRU(\n",
            "    (embedding): Embedding(8991, 128)\n",
            "    (gru): GRU(128, 256, batch_first=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): AttnDecoderRNN(\n",
            "    (embedding): Embedding(8471, 256)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "    (gru): GRU(256, 256, batch_first=True)\n",
            "    (out): Linear(in_features=512, out_features=8471, bias=True)\n",
            "  )\n",
            ")\n",
            "Training model: baseline2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bca22f035a5647af80c51f041c953a88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2803f25487f3425db11bacf3f15739fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training batches:   0%|          | 0/2546 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Batch 1/2546] Loss: 9.0653\n",
            "[Batch 2/2546] Loss: 8.9686\n",
            "[Batch 3/2546] Loss: 8.8417\n",
            "[Batch 4/2546] Loss: 8.4324\n",
            "[Batch 5/2546] Loss: 8.0412\n",
            "[Batch 6/2546] Loss: 7.6876\n",
            "[Batch 7/2546] Loss: 7.1088\n",
            "[Batch 8/2546] Loss: 6.8261\n",
            "[Batch 9/2546] Loss: 6.4070\n",
            "[Batch 10/2546] Loss: 6.1858\n",
            "[Batch 11/2546] Loss: 6.0495\n",
            "[Batch 12/2546] Loss: 6.0679\n",
            "[Batch 13/2546] Loss: 6.2252\n",
            "[Batch 14/2546] Loss: 6.0745\n",
            "[Batch 15/2546] Loss: 6.0803\n",
            "[Batch 16/2546] Loss: 6.0400\n",
            "[Batch 17/2546] Loss: 6.0598\n",
            "[Batch 18/2546] Loss: 6.1236\n",
            "[Batch 19/2546] Loss: 6.0404\n",
            "[Batch 20/2546] Loss: 6.0834\n",
            "[Batch 21/2546] Loss: 5.9996\n",
            "[Batch 22/2546] Loss: 6.2212\n",
            "[Batch 23/2546] Loss: 6.0195\n",
            "[Batch 24/2546] Loss: 6.1367\n",
            "[Batch 25/2546] Loss: 6.0760\n",
            "[Batch 26/2546] Loss: 5.9808\n",
            "[Batch 27/2546] Loss: 6.0410\n",
            "[Batch 28/2546] Loss: 6.0612\n",
            "[Batch 29/2546] Loss: 5.9883\n",
            "[Batch 30/2546] Loss: 5.9875\n",
            "[Batch 31/2546] Loss: 5.9937\n",
            "[Batch 32/2546] Loss: 5.9385\n",
            "[Batch 33/2546] Loss: 5.9768\n",
            "[Batch 34/2546] Loss: 5.8473\n",
            "[Batch 35/2546] Loss: 5.9090\n",
            "[Batch 36/2546] Loss: 5.8743\n",
            "[Batch 37/2546] Loss: 5.9640\n",
            "[Batch 38/2546] Loss: 5.9075\n",
            "[Batch 39/2546] Loss: 5.9782\n",
            "[Batch 40/2546] Loss: 5.8246\n",
            "[Batch 41/2546] Loss: 5.9762\n",
            "[Batch 42/2546] Loss: 5.8231\n",
            "[Batch 43/2546] Loss: 5.8571\n",
            "[Batch 44/2546] Loss: 5.8217\n",
            "[Batch 45/2546] Loss: 5.8611\n",
            "[Batch 46/2546] Loss: 5.7828\n",
            "[Batch 47/2546] Loss: 5.8853\n",
            "[Batch 48/2546] Loss: 5.8203\n",
            "[Batch 49/2546] Loss: 5.9100\n",
            "[Batch 50/2546] Loss: 5.9898\n",
            "[Batch 51/2546] Loss: 5.9195\n",
            "[Batch 52/2546] Loss: 5.7946\n",
            "[Batch 53/2546] Loss: 5.8546\n",
            "[Batch 54/2546] Loss: 5.6849\n",
            "[Batch 55/2546] Loss: 5.7927\n",
            "[Batch 56/2546] Loss: 5.9408\n",
            "[Batch 57/2546] Loss: 5.8687\n",
            "[Batch 58/2546] Loss: 5.8476\n",
            "[Batch 59/2546] Loss: 5.7462\n",
            "[Batch 60/2546] Loss: 5.7988\n",
            "[Batch 61/2546] Loss: 5.8234\n",
            "[Batch 62/2546] Loss: 5.6887\n",
            "[Batch 63/2546] Loss: 5.9095\n",
            "[Batch 64/2546] Loss: 5.7905\n",
            "[Batch 65/2546] Loss: 5.7677\n",
            "[Batch 66/2546] Loss: 5.7772\n",
            "[Batch 67/2546] Loss: 5.7254\n",
            "[Batch 68/2546] Loss: 5.7291\n",
            "[Batch 69/2546] Loss: 5.6974\n",
            "[Batch 70/2546] Loss: 5.5886\n",
            "[Batch 71/2546] Loss: 5.7190\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaseline2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m experiment_configs[model_type]\n\u001b[0;32m----> 4\u001b[0m model, encoder, decoder, loss_history, save_model_path, save_history_path \u001b[38;5;241m=\u001b[39m \u001b[43mrun_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[13], line 269\u001b[0m, in \u001b[0;36mrun_train\u001b[0;34m(model_type, config)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    267\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLR\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 269\u001b[0m     loss_history \u001b[38;5;241m=\u001b[39m \u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEPOCHS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTRAIN_RATIO\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_history_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_history_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTEACHER_FORCING_RATIO\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTEACHER_FORCING_RATIO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mMAX_LEN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMAX_LEN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mLR_STEP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mLR_GAMMA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_gamma\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, encoder, decoder, loss_history, save_model_path, save_history_path\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "Cell \u001b[0;32mIn[13], line 53\u001b[0m, in \u001b[0;36mTrain\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, EPOCHS, BATCH_SIZE, TRAIN_RATIO, save_model_path, save_history_path, TEACHER_FORCING_RATIO, MAX_LEN, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m     52\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 53\u001b[0m train_epoch_loss, train_batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEACHER_FORCING_RATIO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_LEN\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m loss_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_epoch_loss)\n\u001b[1;32m     59\u001b[0m loss_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_iter\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_batch_loss)\n",
            "Cell \u001b[0;32mIn[13], line 5\u001b[0m, in \u001b[0;36mloss_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, teacher_forcing_ratio, max_len)\u001b[0m\n\u001b[1;32m      2\u001b[0m rloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m batch_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (src_batch, trg_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining batches\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)):\n\u001b[1;32m      6\u001b[0m     src_batch \u001b[38;5;241m=\u001b[39m src_batch\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      7\u001b[0m     trg_batch \u001b[38;5;241m=\u001b[39m trg_batch\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# 전처리된 텍스트 토큰화\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     ingredient_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer_ingredient\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIngredients\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     recipe_tokens \u001b[38;5;241m=\u001b[39m tokenizer_recipe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecipe\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# 인덱스로 변환\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[5], line 18\u001b[0m, in \u001b[0;36mtokenizer_ingredient\u001b[0;34m(text, remove_stopwords, lemmatize)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m spacy_en\u001b[38;5;241m.\u001b[39mselect_pipes(disable\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtagger\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m text_list:\n\u001b[0;32m---> 18\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mspacy_en\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n\u001b[1;32m     20\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mis_punct:\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/spacy/language.py:1011\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/spacy/pipeline/tok2vec.py:125\u001b[0m, in \u001b[0;36mTok2Vec.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39malloc((\u001b[38;5;241m0\u001b[39m, width)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[0;32m--> 125\u001b[0m tokvecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokvecs\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/thinc/model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/thinc/layers/with_array.py:42\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m](Xseq, is_train)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_list_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/thinc/layers/with_array.py:77\u001b[0m, in \u001b[0;36m_list_forward\u001b[0;34m(model, Xs, is_train)\u001b[0m\n\u001b[1;32m     75\u001b[0m lengths \u001b[38;5;241m=\u001b[39m NUMPY_OPS\u001b[38;5;241m.\u001b[39masarray1i([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m Xs])\n\u001b[1;32m     76\u001b[0m Xf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(Xs, pad\u001b[38;5;241m=\u001b[39mpad)\n\u001b[0;32m---> 77\u001b[0m Yf, get_dXf \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbackprop\u001b[39m(dYs: ListXd) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ListXd:\n\u001b[1;32m     80\u001b[0m     dYf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(dYs, pad\u001b[38;5;241m=\u001b[39mpad)\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/thinc/layers/residual.py:41\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m d_output \u001b[38;5;241m+\u001b[39m dX\n\u001b[0;32m---> 41\u001b[0m Y, backprop_layer \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [X[i] \u001b[38;5;241m+\u001b[39m Y[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))], backprop\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
            "    \u001b[0;31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001b[0m\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/repos/jkim0094-CookingRecipeGenerato/.venv/lib/python3.10/site-packages/thinc/layers/maxout.py:52\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     50\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape2f(W, nO \u001b[38;5;241m*\u001b[39m nP, nI)\n\u001b[0;32m---> 52\u001b[0m Y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m Y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape1f(b, nO \u001b[38;5;241m*\u001b[39m nP)\n\u001b[1;32m     54\u001b[0m Z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape3f(Y, Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], nO, nP)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model_type = \"baseline2\"\n",
        "config = experiment_configs[model_type]\n",
        "\n",
        "model, encoder, decoder, loss_history, save_model_path, save_history_path = run_train(model_type, config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model was saved at Epoch 4\n",
            "Train Loss: 3.9742 | Val Loss: 5.8047\n",
            "Total training time : 15572.66 s\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAEiCAYAAAAPh11JAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQJtJREFUeJzt3Qd4FNXaB/A3vdGLdBCkB0IXQZDeggUsIKJgAUVBwYb90vQCghTFjoh6RS6ooFdpoSNNFJCAGAGpAtJLCISQzPf8z36z2d1sktnNbnZ39v97nmOys7O7c2ZW5s173jkTommaJkRERESUr9D8VyEiIiIiYOBEREREZBADJyIiIiKDGDgRERERGcTAiYiIiMggBk5EREREBjFwIiIiIjKIgRMRERGRQQyciIiIiAxi4ERElIsHH3xQihQp4tH3PHDggISEhMjkyZPFH82ePVttH7ZT1759e9WIiIETkddOPL/88ouvNyUgAhPsK2ctOjra15tHhWjLli0ybNgwiY+Pl7i4OKlatar06dNH/vzzT19vGpGdcPuHRESFKyoqSmbOnJljeVhYmE+2h3JatmyZ1z9j4sSJsn79ernnnnskISFBjh8/LjNmzJCmTZvKpk2bpEGDBl7fBiIjGDgRkdfgHuJXrlyRmJiYXNcJDw+X+++/v1C3i1wTGRnp9c945plnZM6cOXaf1bdvX2nYsKFMmDBB/vOf/3h9G4iM4FAdkY9s27ZNevToIcWKFVN1NJ06dVJ/WdvKyMiQMWPGSK1atdTQVenSpaVNmzaSlJRkXQd/mT/00ENSuXJllb2pUKGC3HHHHXY1KnnV7/z111/SrVs3NTxSsWJFGTt2rAp4bGVlZcm0adPUMAq2o1y5cvLYY4/J2bNn7da7/vrr5dZbb5WlS5dK8+bNVcD04Ycfemz4c+3atepzsR+w3wYMGJBjG+C9995T24r9gT4NHTpUzp07l2O9zZs3S2JiopQsWVL1H5mO6dOn51jv77//ll69eqn9VbZsWXnuueckMzPTbp1jx47JH3/8oY6ZUVOnTpVq1aqp/dSuXTvZuXOn3fM7duxQx6lGjRpqv5cvX14efvhhOX36tN16Fy9elBEjRqj9jz5fd9110qVLF9m6dWuO/nbv3l2KFy8usbGx6jOR5cmPY43T6tWr1fGYN2+evPHGG+q7h+3Dd3jv3r05Xm/kc1u3bp0jQMP3Hsdx9+7d+W4jUWFhxonIB3bt2iVt27ZVJ/+RI0dKRESECjBwclqzZo20bNlSrTd69GgZP368DBo0SG688Ua5cOGCqp3CCREnRrjrrrvU+z355JPqxHnixAkVWB06dEg9zgtO/jih3XTTTfLmm2/KkiVLZNSoUXLt2jUVQOkQrCB4QYD21FNPyf79+9UwCoI/nACx/bqUlBTp16+fes3gwYOlTp06+e6PU6dO5ViGkyj2jy3UwJQoUULtF3zO+++/LwcPHrSeyPV9hmCzc+fO8vjjj1vXQw2N7bZiHyHIQ6A5fPhwFZTgBP3DDz+ox7b7CIEljgkKupcvXy5vvfWW3HDDDer9dS+99JJ89tlnat/kt9/h888/VwEPgjpk5RCwdezYUZKTk1Vgqm8jAlvsd2wfjvNHH32kfiLI1vs8ZMgQ+frrr9X+qV+/vgqsfvrpJ9UfDHXBypUrVaDerFkzdYxDQ0Pl008/VZ+5bt069f1yFTJBeB8EkufPn1ffof79+6tASVeQz0UA/88//6jgichvaETkUZ9++inSNdqWLVtyXadXr15aZGSktm/fPuuyo0ePakWLFtVuueUW67JGjRppPXv2zPV9zp49qz5r0qRJLm/nwIED1WuffPJJ67KsrCz1edi2kydPqmXr1q1T63355Zd2r1+yZEmO5dWqVVPL8Jwr2+CsdevWLcc+bdasmXb16lXr8jfffFMt/+6779TjEydOqG3v2rWrlpmZaV1vxowZar1Zs2apx9euXdOqV6+uthf70Bb2geP2jR071m6dJk2aqG1x1pf9+/fn2Wc8j/ViYmK0I0eOWJdv3rxZLX/66aety9LS0nK8/quvvlLrrV271rqsePHi2tChQ3P9TPSpVq1aap/a9g/vj/3QpUuXHPvath/t2rVTTbdq1Sq1Tr169bT09HTr8unTp6vlycnJLn+uM1988YV6v08++STP9YgKE4fqiAoZMhgotsXQD4ZgdMh83HfffSpTgMwSILuC7MKePXucvheGeJCZQcbF2ZCVEchS6JDBwOOrV6+qzArMnz9fDbEgw4XMkN6QQcDQ1apVq+zer3r16ipDYxSGeJBZcWzIZjh69NFH7bJbyPigRmrRokXqMbYZ245hK2Q2dMh8IXv1448/qsfIlCEzhPWwj23pWRxbyOjYQrYQmSBbyMghQ2Ik2wQ4/pUqVbI+RuYFWS29L2BbG4asFPY7soNgOwyHPiDLc/ToUaeftX37dvUdwvcL2Sj9GF66dEkNr2EIFMOxrkImzHZ4DfsF9H1TkM/FsCeyca1atZKBAwe6vG1E3sKhOqJCdvLkSUlLS3M6hFWvXj11Ijl8+LAansBwGeqVateura4qwrDaAw88oGpxAPUsuBrp2WefVcM7OKli+Am1PxjayQ+CC9vgDfBZoNdI4cSHYRjUzTiDoUHHwMkVuHoOw2pGoObFFgI3BJz6tmLYDhz3LU7u6Kf+/L59+9RPI1dqIbBDXZMt1ES5G6jm1hd936NuSHfmzBk17Dh37twc+xnHRIchMgQXVapUUQEt6rbwHdCPrR545xWA4P3QL1dgygBb+uv1fePu56Jur2fPnipgxxAkr7Akf8LAiciP3XLLLeok/91336ksFS7bR0HxBx98oOqeAFmT2267TRYuXKiKsl977TVVF4XakiZNmhR4GxDIIWj68ssvnT7vGFTkdQVdIPLlSRvzGG3YsEGef/55ady4sQoUcTwQQNtmarAesj0LFixQ35NJkyapgPrbb79V9UX6uliO93HGnYk+c9s3+sUF7nwuAilsM4r5UQOF4n4if8LAiaiQIdDAlUUoWnY2PIEsEDIHulKlSqkhEbTU1FQVTKEAWg+cAIXKyDqh4a98nKRQwJzfJdw4sWFYRc8ygT7hoD7khPfGENjNN9/s86AIfevQoYP1MfYHrmZDhgVwhRpg39pm0jB8h6E5PbOFPgGuYjOa7fI0Z8Ov2Pf6fkfWZsWKFSrj9K9//SvP1wEyb0888YRqyE6hKBxXvCEI0fuL4crC7K+rn4vhSPwRgP2A7xwK3Yn8DWuciAoZ/krv2rWryiLZThmAq4cwjw2mG9CvJnO87Bx/ndesWVPS09PVYwz54WTjeLIqWrSodZ384Oo420wBHqOOCDUoejYDdVnjxo3L8VpcfefsMn9vwRVltpf742o5bAOCA8DJGcNyb7/9tt2UCp988onKZGD4BxBUYEgRUyw4br/jVAxGuTodATKEmOZA9/PPP6s6Jb0vejbHcXuwzbZwbGyH7QAZQmRq9O8Ahu/wvcBVgQg2nQ0fe4Mrn4t+YN6mjRs3qro61DYR+SNmnIi8ZNasWeryfke41P31119XBdAIkpAhQIEzpiPAiQ71Kjr8xY0pCnACQuYJUxHol50D/jJHgIPgBuvifTBcgyDs3nvvNVS/g21EDQoKkxcvXqwKqF9++WXrEBzm3MHUAhj+Q7Evgj4EVsh84ASHy+jvvvtut/cTAp/cMmO9e/dW8yvZZo70/iKrhPmasA9vv/129Ty2GdMCIEuD4Sws19dr0aKFdaJNZPUQdCG7gewcsnnI2CDwQTE+hjxd5ep0BAiAse0ocMdxR0CE+akwPQUgeEZ2Ed8HBGMoJMcwHN7fFqY0wDxKOAaNGjVSwTWyNZh+AVlHvb8Y5kVQhto59Bfvh8ANxf34rP/973/iaa58LrKl33//vTomqO1y/E5wklTyG4V6DR9RENAv586tHT58WK23detWdZl2kSJFtNjYWK1Dhw7ahg0b7N7r9ddf12688UatRIkS6vL1unXram+88Yb1kvxTp06py9CxPC4uTl2W3rJlS23evHn5bicun8drMCUCLt/HNpQrV04bNWqU3aX8uo8++khdgo/twLQJDRs21EaOHKmmUdDh8v68pk9wtg157Sv9knh9n65Zs0Z79NFHtZIlS6r91r9/f+306dM53hfTD2CfREREqD49/vjjOaYdgJ9++kldEo/+YF8kJCRo77zzTo595Aj7yPGfT1enI8AUEm+99ZZWpUoVLSoqSmvbtq3222+/2a2L6Qp69+6tjj+O7T333KP2N16PbQBMB/D888+rqSv0fuD39957L8dnb9u2Tbvzzju10qVLq8/E8erTp4+2YsUKt6YjmD9/vtO+4T1c/Vy8f17fBSJ/EYL/+Dp4I6LChxmpkb1yNoTib/TJN5FFwYzkRES+whonIiIiIoMYOBEREREZxMCJiIiIyCDWOBEREREZxIwTERERkUEMnIiIiIgMCroJMHGLCdxBHDMrO7sLOhEREQUXTdPUZLKYcR8Tt+Yl6AInBE229wEjIiIigsOHD6uZ+PMSdIETMk36ztHvB+ZJuDUCboug35bCzNhXc2JfzSlY+hos/QT21XMuXLigkip6jJCXoAuc9OE5BE3eCpxw53u8dzB8kdlX82FfzSlY+hos/QT21fOMlPCwOJyIiIjIIAZORERERAYxcCIiIiIyiIETERERkUEMnIiIiIgMCrqr6igI4XaMGRki6ekiV69mt7weOzwXevmyVE1JkZCLF3FJpkhcXHaLjbX/PSzM1z0mIiIvYeBEPg9KvPZa/XdsXwEhFGqCX959N/+Vo6JyBlTOAiwjzzmuFxPDwIyIyIcYOPl7UJKZWfiBhsHH4enpcgd+DzQIPCIjLQ1BjrPfHR5nhYXJicOH5boiRVT2SS5dym5paZafOF6A/YR25ox3tj862nOBmONzaERElCsGTp60a5eEfvaZxKekSOiyZZZMR0GDFP1k7IecThOGycP0gCOPQMSVoMWj66K5kbHJzMiQzYsWSWJiooQ6m3wNx+nKlewgyjagcvbY1fXwU/8u4HPQTp8WbwiPiZHu4eESXrJkwQMxx9+RMcvnPlBERP6MgZMn7d0rYZMmSU1vfobRAKIQApGMkBBZvm6ddE5MlAicFLE8PEi/UggYERSglS7t+fdH0IRMV0GCr7zWw0+9K5cvSxR+QT2XN+iZLU8OYdoGZrx5NxF5UZCe5bykVi3JfOop+evIEalRt66E4R9xTwYtCEr86aSQkSFXS5QQQTP5dP8+h+OuBxxlynj+/bOyLFmsS5ck49w5WbdkidzSvLkajnU7SLN9jKBPh2Vop06JV7gQbIVGR0utAwck9M8/LUGX/v8avs+OmUqjDa9lVo3ItBg4eVL9+pI1ebL8vmiRXJ+YKGEMJihQ4ESvB2YlSsjFqlVFa97ccwExAjPb2jB3gq+8fkfQ5xiYnTyZ72Zh0La+Z3qYex2du8GXp18fEiLRqLtDwKpniNEY5JErNbfXrlma7e+F8Dj06lWp/fvvElKqlEjbtj7dFQyciMj7cHLWMz3egH9kcwvM8gjEMlNT5ci+fVKlXDkJxT/QtnWGzppj3aLe8PnOtsc20+ZjCIG7GQ3yvBG4eeL1hXlFKYJ9HwYKRh6HXb0qTQ4ckLB58yyBjTc/D/vDh8JEpB7+10pIYOBERFRgOKEWKWJpLsjKyJDtixZJxdyK/o3CSSa3oMpI4GWkFeS1V6+KlpEhWnq6hAZAkJdnAJ5P0IVMf5sLFyRs4sSCBT5+fGGODrnCquIHwsPtG/5/NPrY4LpZoaFy6OhRqdygga97y8CJiKjA8I87GqaK8FPXMjJkEa4M7d5dZZ8KLWgryGsRwDirxbMdmnUSTHjh8gybDwh1LTDw4uPMkBD5Y+9eqRsfL2Goh/XF9oQWzlAvrmz+bdEiqdS1q/gaAyciomCCEx2yazjRBsoEuy4EW9fS0mTr5s3StEULCUcg68lAAT/9qCYMGdO9ixZJbdbUFioGTkRE5L9Xk+rDcAZhSPJYWJhoiYm82pe8wn9CZyIiIiI/x8CJiIiIyCAGTkREREQGMXAiIiIiMoiBExEREZFBDJyIiIiIDGLgRERERGQQAyciIiIigxg4ERERERnEwImIiIjIIAZORERERAYxcCIiIiIyiIETERERkUEMnIiIiIgMYuBEREREZBADJyIiIiKDGDgRERERGcTAiYiIiMggBk5EREREBjFwIiIiIjKIgRMRERGRQQyciIiIiAIhcBo9erSEhITYtbp16+a6/uzZs3OsHx0dXajbTERERMEr3NcbEB8fL8uXL7c+Dg/Pe5OKFSsmKSkp1scInoiIiIiCInBCoFS+fHnD6yNQcmV9IiIiItMETnv27JGKFSuqIbdWrVrJ+PHjpWrVqrmun5qaKtWqVZOsrCxp2rSp/Pvf/1ZZq9ykp6erprtw4YL6mZGRoZqn6e/pjff2N+yrObGv5hQsfQ2WfgL76jmuvG+Ipmma+MjixYtVIFSnTh05duyYjBkzRv7++2/ZuXOnFC1aNMf6GzduVIFWQkKCnD9/XiZPnixr166VXbt2SeXKlXOto8L7OpozZ47ExsZ6pV9EREQUONLS0uS+++5TsQVKgvw2cHJ07tw5lU2aMmWKPPLII4YixHr16km/fv1k3LhxhjNOVapUkVOnTuW7c9yBbUpKSpIuXbpIRESEmBn7ak7sqzkFS1+DpZ/AvnoOYoMyZcoYCpx8PlRnq0SJElK7dm3Zu3evofWx85o0aZLn+lFRUao5e603v2jefn9/wr6aE/tqTsHS12DpJ7CvBefKe/rVPE4Yttu3b59UqFDB0PqZmZmSnJxseH0iIiKigvBp4PTcc8/JmjVr5MCBA7Jhwwbp3bu3hIWFqaE3GDBggLz00kvW9ceOHSvLli2Tv/76S7Zu3Sr333+/HDx4UAYNGuTDXhAREVGw8OlQ3ZEjR1SQdPr0aSlbtqy0adNGNm3apH6HQ4cOSWhodmx39uxZGTx4sBw/flxKliwpzZo1UwFX/fr1fdgLIiIiChY+DZzmzp2b5/OrV6+2ezx16lTViIiIiHzBr2qciIiIiPwZAyciIiIigxg4ERERERnEwImIiIjIIAZORERERAYxcCIiIiIyiIETERERkUEMnIiIiIgMYuBEREREZBADJyIiIiKDGDgRERERGcTAiYiIiMggBk5EREREBjFwIiIiIjKIgRMRERGRQQyciIiIiAxi4ERERERkEAMnIiIiIoMYOBEREREZxMCJiIiIyCAGTkREREQGMXAiIiIiMoiBExEREZFBDJyIiIiIDGLgRERERGQQAyciIiIig8J9vQFERES6zMxMycjIcPv1eG14eLhcuXJFvZeZsa/GhYWFqdeHhIRIQTFwIiIiv5CamipHjhwRTdPcfg+8tnz58nL48GGPnCT9GfvqmtjYWKlQoYJERkZKQTBwIiIin0MWAUETTm5ly5Z1++SYlZWlArAiRYpIaKi5q1HYV+NB19WrV+XkyZOyf/9+qVWrVoH2FwMnIiLyi6EYnOAQNMXExBToBIuTZHR0dFAEE+yrMfhORUREyMGDB63v4y5z72kiIgooZh9yIt/xVHDJwImIiIjIIAZOREREfuT666+XadOm+XozKBcMnIiIiNwcVsyrjR492q333bJlizz66KMF2rb27dvLiBEjCvQe5ByLw4mIiNxw7Ngx6+///e9/5V//+pekpKRYl+EKMB0K33HlIOYSyg8K5Ml/MeNERETkBswrpLfixYurLJP++I8//pCiRYvK4sWLpVmzZhIVFSU//fST7Nu3T+644w4pV66cCqxatGghy5cvz3OoDu87c+ZM6d27t5quAZfTf//99wXa9m+++Ubi4+PVduHz3nrrLbvn33vvPfU5uPoM23r33Xdbn/v666+lYcOG6kq10qVLS+fOneXSpUsSLHwaOCGN6ZjarFu3bp6vmT9/vloHBxMHbtGiRYW2vUREVDgwBybOxb5oBZh/M4cXX3xRJkyYILt375aEhAQ1F1FiYqKsWLFCtm3bJt27d5fbbrtNDh06lOf7jBkzRvr06SM7duxQr+/fv7+cOXPGrW369ddf1Xvde++9kpycrM7Fr732msyePVs9/8svv8hTTz0lY8eOVRm0JUuWyC233GLNsvXr108efvhh1afVq1fLnXfeWaBJSwONz4fqEPHaRtt5pTE3bNigDtj48ePl1ltvlTlz5kivXr1k69at0qBBg0LaYiIi8ra0NAx1uZsPKFGgz05NFYmLE49A8NGlSxfr41KlSkmjRo2sj8eNGycLFixQGaRhw4bl+j4PPvigOv/Bv//9b3n77bfl559/ltatW7u8TVOmTJFOnTqpYAlq164tv//+u0yaNEl9DoK4uLg4dZ5F1qxatWrSpEkTa+B07do1FSxhOSCJEUzcyjhhynPM8KrDwUMR2kcffeTyeyFQsk13lilTJtd1p0+frqLz559/XurVq6e+cE2bNpUZM2a40w0iIiKvat68ud1jZJyee+45dQ4rUaKEGq5D5ia/jBOyVToENcWKFZMTJ064tU34vJtvvtluGR7v2bNH1WEh0ENQVKNGDXnggQfkyy+/lDREsiIq6EPQhWDpnnvukY8//ljOnj0rwcStwOm+++6TVatWqd+PHz+udjKCp1deeUVF167AgapYsaI6QEg95vXl2bhxoxpLtdWtWze1nIiIzCM21pL5cbVduJAlR46cUz/deT0aPttTEOTYQtCEDBOyRuvWrZPt27erIASzWecFs17bQmkLZtP2BmSZMJLz1VdfqXu7oegdAdO5c+fUzXKTkpJU7Vb9+vXlnXfekTp16qhbmQQLt4bqdu7cKTfeeKP6fd68eWqYbP369bJs2TIZMmSI2slGtGzZUo2pYqcj/Ycx3LZt26r3x4FzhCANRWq28BjLc5Oenq6a7sKFC9bp/QtyB+7c6O/pjff2N+yrObGv5uTvfdVvuYJgQA8I3LnziuXqNQQ/moSEuBdYoFzH1ZIdfZud/bQNcHCuHDhwoCoQ1zNQBw4csPbdth+2jx3fx3577dfNbznqhFGobvscHmPITg/IMMt2x44dVcOQHoYYUVZz5513qvVbtWql2quvvirVq1eXb7/9Vp5++mnxFr2GKrc+GYHX4fX4riEAtOXK/xduBU74AFTiA3bk7bffbj0Ytpdn5qdHjx52aUgEUkgPIhh75JFHxBNQD4WAzBGCPFyd4C2IyIMF+2pO7Ks5+Wtf9bINBBL5ZV+MuHjxohSmK1euqJOy/se5PrSF7bC91QeuYMNVaR06dFCPkXnS78OmvxaP8X76Y7h8+bLdY3wW1tE/wxHqkI4ePaoCNcdkw2OPPWYNiHClHuaNevfdd2Xy5MnqM1AMjnu6oX4KVwviO4NtqlSpkqxcuVLWrFmjXo/SGhSa4+a5VatWtds+bynIccU+xn5cu3at2j+29OPltcAJBd0ffPCB9OzZU+1Q1BoBDhIuTXQXxnsR8e7du9fp8/if6p9//rFbhsdYnpuXXnpJnnnmGetjHNgqVapI165d1RixpyGoxD7B8KVjatVs2FdzYl/Nyd/7iiAA9bOo+SnIDVgRUODkilGLwrzvHbYZn6efV/Q/zLEdtuca1OoOGjRIlZkg8Bg5cqQ6mUdGRlrXQ6CF97N9HS79t32Mz9L3k7O+IhBFgIZmC+U0KKuZO3euupoOBeEYjkOCASNGgPIZnOMnTpyojgumJUCdE5Ibu3fvVqU5H374oTqfItmBgOuuu+4Sb/LEcUVfsB9xhaDjd8yVoM+twAk7E1EqdjhSjvoVArgqQB/Ccwf+0sAcFyhGcwZpQVzCaTsbKv4hwPLcIDOmZ8ds4R8Ob/7j4e339yfsqzmxr+bkr31FUTJOiAgaCnIzVn0YR3+vwoLL89F0yMg4u0Qf9bzI2thyvJoOQ3e2nL0P6o3QV5zwnfUV0wTkBYXdaM4gsMjt9fHx8bJ06VIpbJ44rngdXu/s/wFX/p8Id3cq91OnTqkDVrJkSetyTBHvyvAXiuQwfwUiVmSrRo0apcYd9UsuBwwYoFKDGG6D4cOHS7t27dREXch2IWLGfBPuXM1HRERE5Cq3AiekFREB60ETxkJxlQAur0T60ShMaYAg6fTp02qK+TZt2simTZus083jCjvbyBLjrZi7CcVoL7/8skofLly4kHM4ERERkf8GTrgaAJX1GA9FuhDjnkhzIQuFibUef/xxQ++DjFFenKUK80ovEhEREXmTWwOFmN8B0wYACs9QpY+s0+eff65mMyUiIiIyI7cCJ1y2p8+zhMv6kX3CkNpNN92kAigiIiIiM3IrcKpZs6aqLcKlo6iux6X9gOnfvXGJPxEREVHABk6YGRxXxGEiL0w/oE8HgOyTfiNAIiIiIrNxqzj87rvvVlfAYZZw27s848Z/mN+JiIiIyIzcCpwAs3WjYUoBqFy5coEmvyQiIiIy5VAdZvDEtO24hw0mr0TD7VJw6xVv3a2ZiIjIjDCptO0dMVAGM23atDxfgxmwUWtcUJ56n2DiVuCE+9zMmDFDJkyYINu2bVMNNyp855131E0DiYiIzA53vujevbvT59atW6eCkh07drj8vrjpLu7E4Um4L13jxo1zLEfJTY8ePcSbZs+erZIrQT1U99lnn8nMmTPl9ttvty5LSEhQt0d54okn5I033vDkNhIREfmdRx55RN3cFiUrKFex9emnn0rz5s3VudFV+t0zCgNKbqgQMk5nzpyRunXr5liOZXiOiIjI7G699VYV5CCj4njD+vnz56vACrcUw63FkFjAvVwbNmwoX331VZ7v6zhUt2fPHnXj3ejoaKlfv766ub2jF154QWrXrq0+AzcSxuhPRkaGeg7bN2bMGPntt99UFgxN32bHobrk5GR1g+KYmBgpXbq0ynyhP7oHH3xQevXqJZMnT5YKFSqodYYOHWr9LHfg9mq4I0mRIkXUlEZ9+vSRf/75x/o8thsXn1WpUkVlrpo1a6buUwuYOxKZP9wCLi4uTt2EeNGiReJ3GSdcSYehOsdZwrHMneiaiIjIjqZhtmXXX4c620uXRMLCRGzudeoS3Kw+JCTf1cLDw9XN6BGEoIQFQQggaMrMzFQBE4IOnOgR2CAo+PHHH+WBBx6QG264wdAFVagbxiTTuEPH5s2b5fz583b1UDpMSo3tqFixogp+Bg8erJaNHDlS+vbtKzt37pQlS5bI8uXL1fqoUXZ06dIldb9ZTDGE4ULMzTho0CAZNmyYXXC4atUqFTTh5969e9X7YxgQn+kq9E8PmtasWSPXrl1TgRjeU7/tWv/+/dX7T5w4UW03hj9xmzfAulevXpW1a9eqwOn3339X7+V3gdObb74pPXv2VAdAn8Np48aNakJMb0d6REQUBBA0uXECRKhU4GoaZFji4gyt+vDDD8ukSZPUSR9F3vowHYbwcJJHw7yHuieffFJNHD1v3jxDgRPOs3/88Yd6DYIiQE2xY13Sq6++apexwmfifrAInJA9QjCBQC+vobk5c+bIlStX1O3TEIToCRFkdBC0IHgDZHewPCwsTI00IR5YsWKFW4ETXodAb//+/SqjBPh8ZI4QvLVo0UJlpJ599lmVUUPwWadOHevr8Rz2NTJ5gGybt7kVjrdr107+/PNPNWcTbvKLhoh4165d8sUXX3h+K4mIiPwQAofWrVvLrFmz1GNkYFAYjmE6QOYJV5zjxF6qVCkVwCAIwgnfiN27d6uAQg+aQE9Y2Prvf/8rN998swqM8BkIpIx+hu1nYURJD5oA74msUEpKinVZfHy8Cpp0yD4hO+UOvX960AQYjsSQHJ6DZ555Rg0ZYogQAdy+ffus6z711FPy+uuvq+0cNWqUW8X4rnIzjynqIKII/JtvvlENG3727Fn55JNPPLuFREQUfDBchsyPiy3rwgU5d+SI+unO61XDZ7sAQRLOgxcvXlTZJgzDIcEAyEZNnz5dDdVhaGv79u1qOAzDS56CER8MZyUmJsoPP/ygrnTH0KEnP8NWxP8Pk+kwROnNqYhwRSCyUri928qVK1VgtWDBAvUchhL/+usvNfyJdVCQjyv8/TJwIiIi8hrUCyHz4YtmoL7JFoqZcaN7DHVhmAnDd3q90/r161UNz/3336+yORhKwoiNUfXq1VNlMJg2QLdp06YcgRPmU0SwhMChVq1aqmjaVmRkpMp+5fdZKMRGrZMO24++2Q6PeZLePzQd6pQwkoUASYdhOly1j2wdRrgQoOqQrRoyZIh8++23akjv448/Fm9i4ERERFQAGBpDMfNLL72kAhxceaZDEIOr4DZs2KCGnh577DG7K8by07lzZxU0DBw4UAU1GAZEgGSrZs2aalgONU0YxsKFW3pGxrbuCXVEyHidOnVK0tPTc3wWsla4cg+fhWJyZMhQk4Vsjl7f5C4Ebfhs24b9gf5hGBOfvXXrVvn5559VwT0ydggCL1++rIrTUSiOPiKQQ+0TAi5AoTyCKfQNr8c26895CwMnIiKiAsJwHcpVMAxnW4+EWqOmTZuq5SgeRw0SanWMQrYHQRACCBSTY2jKca5EzKn49NNPqwADV58hSHOcjBoF1Jiss0OHDmoKBWdTImAqAwQhmFYIRdm4Ly2mAUAheEGlpqZKkyZN7BqKzpGZ++6771TBOaZcQCCFrBxqtgC1VJjSAcEotunee+9VhfGYXkEPyHBlHYIl9A9B5nvvvSfeFKJpuObTGKTH8oLUGq4syC8d6EsXLlxQVzngkk5U53sa5rLAlYUYa3YcBzYb9tWc2Fdz8ve+4mouZA2qV6+ush7uQq0N/p3Hv+8IOsyMffXcd8yV2MCl6Qiczfvg+DxSbERERERm5FLgZFuMRURERBRszJ3bIyIiIvIgBk5EREREBjFwIiIiIjKIgRMREfkNFy70JvLJd4uBExER+Zx+7zNv3SaEKA03jnZyyxivXlVHRETkDeHh4WoCxpMnT6oTm7tz9WC+HwRfmLMnGOY2Yl+NZZoQNOFGxLh5sO0Nit3BwImIiHwOM0hXqFBBTVDoeJ81V0+SmGU7JibGer84s2JfXYOgCTO3FxQDJyIi8gu4ES3u7VaQ4TrMkL527Vp1+w5/nCHdk9hX4/CagmaadAyciIjIb2AYpiC3XMHJ8dq1a+o9zB5MsK++Ye5BUSIiIiIPYuBEREREZBADJyIiIiKDGDgRERERGcTAiYiIiMggBk5EREREBjFwIiIiIjKIgRMRERFRoAVOEyZMUNOojxgxItd1Zs+erdaxbQWZKI2IiIgo4GYO37Jli3z44YeSkJCQ77rFihWTlJQU62Oz35+HiIiI/IfPM06pqanSv39/+fjjj6VkyZL5ro9ACTfp01u5cuUKZTuJiIiIfB44DR06VHr27CmdO3c2HGhVq1ZNqlSpInfccYfs2rXL69tIRERE5POhurlz58rWrVvVUJ0RderUkVmzZqkhvfPnz8vkyZOldevWKniqXLmy09ekp6erprtw4YL1Tstonqa/pzfe29+wr+bEvppTsPQ1WPoJ7KvnuPK+IZqmaeIDhw8flubNm0tSUpK1tql9+/bSuHFjmTZtmuGO1qtXT/r16yfjxo1zus7o0aNlzJgxOZbPmTNHYmNjC9gLIiIiCnRpaWly3333qaQMaqn9MnBauHCh9O7dW8LCwqzLMjMzVQ1TaGioyhLZPpebe+65R8LDw+Wrr74ynHHCMN+pU6fy3TnuQDCHYLBLly4SEREhZsa+mhP7ak7B0tdg6Sewr56D2KBMmTKGAiefDdV16tRJkpOT7ZY99NBDUrduXXnhhRcMBU0ItPAeiYmJua4TFRWlmiPseE/v/PPnRcaMCZXixctKx44REhtr7i+yN/elv2JfzYl9NZ9g6SewrwXnynv6LHAqWrSoNGjQwG5ZXFyclC5d2rp8wIABUqlSJRk/frx6PHbsWLnpppukZs2acu7cOZk0aZIcPHhQBg0aJP5gxQqRqVMR8LWWiRM1addOpFs3S6tbF1cE+noLiYiIKODnccrNoUOH1LCd7uzZszJ48GA5fvy4mrqgWbNmsmHDBqlfv774gypVRAYOzJL//S9dzpyJkSVLRDX9OT2IwgWEJUr4emuJiIgooAOn1atX5/l46tSpqvmrFi1EPv44U378cZlUrZooK1dGyNKlIuvWoRheZOZMS0Ms2LJldiCF1xkYmSQiIqJgn8fJjDAk17ChyHPPiSQliZw5I7Jokcjw4ZYhu6wskY0bccWfSKtWImXLivTtKzJrlsiRI77eeiIiIgqIjJNZYdaDHj0sDQ4dEpWJQlu+HEOQIvPmWRrEx2dno9q2FYmJ8enmExER0f9j4OQDVauKDB5sadeuifz8c3Yghd8xGTralCkiuIexbZF5vXosMiciIvIVBk4+Fh4u0rq1pWGeTgzrIQulB1J//539O2CCdNsicwO39yMiIiIPYeDkZ0qVEunTx9IwNenvv2cHTmvWWGqgPvnE0lBkfuON2YEUfmeRORERkfewONyPYUgO9U7PPGMJnJCNWrxYZMQIy5Adisw3bbJkqpCxKlMGM6lbrtzDVXxERETkWcw4BViReffulgYIjmyLzM+dE/n6a0sDTG+lZ6NuuYVF5kRERAXFwCmAYVJNTJqOhiLzLVvsi8wxzIeGqa9QZI7gSQ+kEFSxyJyIiMg1HKozUZE55oTC3FCYI+rkScv0Bo88Yikov3JFZNkykWefFcEdbXBlH57DOhgCJCIiovwx42TiInPUO6GhyHz37pxF5phwEw1F5pi93LbIHIEYERER2ePpMQhgSA5Dc2hPPy1y+bLlNjB6IIU5ozZvtrSxYy330evUKTuQQnaKiIiIOFQXlFAk3rWryFtviezcmX0fPWSnMC8Uisy/+Ubk0UdFqlWzXMGHK/lwRV9amq+3noiIyHeYcSJVA4V6J7TMTPsic2Sh/vjD0qZPF4mKstwGBpmojh0tw4BERETBgoET2cEEmjfdZGmjRlnuo7diRXYghewUpj5AE4mQUqW6ym23han78GEm89Klfd0DIiIi72HgRHnC0N3dd1saskvIPGUXmWty5kyMfPaZqIZaKtsi85YtWWRORETmwtMaGYbACPVOes3TxYvXZMqULXL+fEtJSgpT9VKYPwpt3DiR4sXti8xRL0VERBTIWBxObsOkmo0bn5SJE7MkOTl7ioO+fS3TIZw/L/LttyKPPSZy/fUideuKDB8usmgRi8yJiCgwMeNEHlOpkshDD1kaisx//dUypLdkiaXIPCXF0t5+WyQyMrvIHK1hQ85kTkRE/o8ZJ/JakTkm0nztNZH160VOnbLcQ2/wYMu8UFevWorOR44UadTIEnQ9+KDI3Lkip0/7euuJiIicY8aJCgUm1bzrLktDkTkyT3qR+erVIseOiV2RefPm9kXmERG+7gEREREDJ/IBBEaod9JrnnAfvZ9+yg6kUC+FuaTQXn9dpFgx+yJz1EsRERH5AgMn8osic8wBhTZpksjRo5YbEiOISkqyDN0tWGBpULt2dhDVvr1IXJyve0BERMGCgRP5nYoVLfVOaCgy37o1Oxu1caPIn39a2jvvWIrM27TJDqQSElhkTkRE3sPicPL7InNMqvnqq5YbEyP7ZHsfPRSZr1wp8sILmBrBEnQNHCgyZ47IyZO+3noiIjIbZpwooGBSzTvvtDQUmSPzZFtkfvy4yOefWxoyT82aZWejcBsZFpkTEVFBMHCigIXAqE4dS3vqKZH0dPsi8x07RH75xdLeeEOkaFH7IvPq1X3dAyIiCjQMnMg0oqIsgRHam29apjiwLTLHXFILF1oa1KplX2RepIive0BERP6OgROZVoUKlnontKysnEXme/ZY2owZliE82yJzTMrJInMiInLE4nAKCqGhlkk1X3lFZO3a7CkOhgyxDNllZIisWiXy4osiTZpYgq4BA0S+/FLkxAlfbz0REfkLZpwoKGFSzV69LA1F5nv3ZmejEED984/IF19YGjRpEi6xsc1l8eJQKVPGchNjvZUunf17yZKWKRKIiMicGDhR0MOQHOqd0IYNsxSZb9hguTkxAqnffhPZtg3jdpXUfffyg1opx4Aqt0DLtqFGi4iI/BsDJyIHCGA6dLC0iRMtUxysWHFNVq/eLeXL15dz58LkzBnJ0c6etWSvUlMt7dAh1z4XM6C7EmjpyzHzOhERFQ4GTkT5KF9epE8fTYoU+UsSE+tKRESY0/Uwy/n58/bBFGqpnAVZtssRcKF4/dIlSzt82LXti4lxLdDSG17HAngiItcwcCLy4CznelDiCgRNFy4YD7RsG4K1y5dF/v7b0lzNrDkLqEqUCJUTJ2rJ0aMhUrZszgAMmTEGXEQUrBg4EfnBFX8lSlhajRrGX4dhQT3gMhpo6cuvXbPUcmGuKzR7yKjVl//8x/nnovjdnSFF1H4x4CKiQMfAiShAIQjBLWjQXJkFXa/Dyi3IOnUqU3bsOCIxMVXk3LlQu/UwbQPuD4i6LzRXhIe7N6SIKyAZcBGRv2DgRBRkEITg9jNouFGyo4yMLFm0aLskJlaUiIhQu4ArLc314UQsR3YLWS7MieXqvFgYAsU0D65epYiAEtk8IiJTBk4TJkyQl156SYYPHy7Tpk3Ldb358+fLa6+9JgcOHJBatWrJxIkTJTExsVC3lShYAy7UN6FVrWr8dQi4UIfl6nAiGl6HOi7cLgfNFQiaEHDlFWgVLRoiKSnlJTIyRAVasbGW/tn+5LxcROR3gdOWLVvkww8/lISEhDzX27Bhg/Tr10/Gjx8vt956q8yZM0d69eolW7dulQYNGhTa9hKRawEXAhC0ypVdey0CJ1x16GqWC1cnougez6Pl/U9gS5kwIY81wnMGU7Y/jS7L7TkU6XMokihw+DxwSk1Nlf79+8vHH38sr7/+ep7rTp8+Xbp37y7PP/+8ejxu3DhJSkqSGTNmyAcffFBIW0xEhQVTJqBVrOja665csQRc+QVap09nyd9/n5OIiJJy+XKICrgwHImfyHQBhhgxzQSaNyAz5k7AZXR9TjtBZLLAaejQodKzZ0/p3LlzvoHTxo0b5ZlnnrFb1q1bN1mo3+7eifT0dNV0F3AZkqrjyFDN0/T39MZ7+xv21ZzM0FfUReHWOGh5QR+TktZJly5dJAJ3erYZXkQRvB5E6T/14EpfZmnZAZdlvRCb9fX5uSzLstfBZ1uiGWTG9ElTvSU2VpPY2HAJCekiZcqESVxcljXAQmBlCbI0a7CVHYThdfbBWExM9nr6T3+qJTPD99co9tVzXHlfnwZOc+fOVcNsGKoz4vjx41KuXDm7ZXiM5bnBsN6YMWNyLF+2bJnE4v94L0EmLFiwr+bEvuYOdU9omELCXdeuhUh6epikp4ern1euhOX6OPt3S7tyJdzp77brZ2RkT9SK4A4Bm0isnDwpHhcZmSlRUdckKipToqMz1ePo6Gs2v2c/r69ju75l+bVcnwsL01zeJn5/zSnJS31Ns/wP4t+B0+HDh1UhOHZCtBfvGYGCc9ssFTJOVapUka5du0oxXOfsYZa/YJNy/AVrRuyrObGvZpAlmZlZdhmuCxdw26AtEh9/o1y9Gm6TEcvOkDlmzWxfj2V6Bk3PtOmuXg1T7eJF7/QmMtI+G2bJdtlnw/RlUVFZcvToHmnQoKYUKRIm0dGaui2R3pBhs/yuOVkmEkhfA/N+fwu/r/polF8HTr/++qucOHFCmjZtal2WmZkpa9euVTVLGF4LQ77dRvny5eUf3LbeBh5jeW6ioqJUc4Qd780vmrff35+wr+bEvgY2dAeBgD6TPUYijh49K126hElERMH/6ccQI2rJbIcubX86W+bqcxgyhatXQ9TQKerWsuVWuGWZwNVdOO04BlP670aXufsad2vRzPj9Ley+uvKePgucOnXqJMnJyXbLHnroIalbt6688MILOYImaNWqlaxYsUJGjBhhXYYIFMuJiKjw6EXtaLg1j6chaEJ5qqsB18WLmZKSckTKlq0i6emhKrhDpgw/bX+3XWZTBqsuCtDfr7Dhb3xXgrKoqFA5erS+bN4cqmbmdze4C5KYy2N8FjgVLVo0xxQCcXFxUrp0aevyAQMGSKVKlVSdEmBor127dvLWW2+pgnLUSP3yyy/y0Ucf+aQPRETkHci+6Cd4zL1lVG4TuOaXPUPw5CzAyi3YcnWZs+fxE5+twzagGb+CEwmGWpLH9VEuZdkKK7sWYw38/OvCgoC5qi4vhw4dklCbvdq6dWs1d9Orr74qL7/8spoAE1fUcQ4nIiJyF04z+tQXhQ1DqO4GYKmpmbJ7919SsWKN/79QwPj7+GuWLTrX7FqYnDzZWEJDQ+S228Sn/CpwWr16dZ6P4Z577lGNiIgo0GGYDA23QHKVJbv2uyQmXi8RETnLW4xk2byRSbuSxzL3s2xIolSTTp0yGTgRERGR77JsuDWRL7JsV1wIwJBd++23FGnTprb4GgMnIiIi8ussW4bKru2Rli1ria8FYFkWERERkW8wcCIiIiIyiIETERERkUEMnIiIiIgMYuBEREREZBADJyIiIiKDgm46Au3/7xrpyp2QXb2Dc1pamnp/s990kX01J/bVnIKlr8HST2BfPUePCfQYIS9BFzhdvHhR/axSpYqvN4WIiIj8LEYoXrx4nuuEaEbCKxPJysqSo0ePqpsMh+Aukl6IWhGUHT58WIoVKyZmxr6aE/tqTsHS12DpJ7CvnoNQCEFTxYoV7e6R60zQZZywQypXruz1z8GBNfsXWce+mhP7ak7B0tdg6Sewr56RX6ZJx+JwIiIiIoMYOBEREREZxMDJw6KiomTUqFHqp9mxr+bEvppTsPQ1WPoJ7KtvBF1xOBEREZG7mHEiIiIiMoiBExEREZFBDJyIiIiIDGLg5IK1a9fKbbfdpibIwuSZCxcuzPc1q1evlqZNm6qCtpo1a8rs2bPFjH1FP7GeYzt+/Lj4u/Hjx0uLFi3UpKjXXXed9OrVS1JSUvJ93fz586Vu3boSHR0tDRs2lEWLFokZ+4rvrONxRZ/93fvvvy8JCQnWeV9atWolixcvNt0xdaevgXpMHU2YMEFt+4gRI0x5XF3tayAf19GjR+fYdhwzfzyuDJxccOnSJWnUqJG8++67htbfv3+/9OzZUzp06CDbt29XX/hBgwbJ0qVLxWx91eEkfOzYMWvDydnfrVmzRoYOHSqbNm2SpKQkdU+krl27qn2Qmw0bNki/fv3kkUcekW3btqkABG3nzp1itr4CTsa2x/XgwYPi7zDRLU42v/76q/zyyy/SsWNHueOOO2TXrl2mOqbu9DVQj6mtLVu2yIcffqgCxrwE8nF1ta+Bflzj4+Pttv2nn37yz+OKq+rIddh1CxYsyHOdkSNHavHx8XbL+vbtq3Xr1k0zW19XrVql1jt79qwW6E6cOKH6smbNmlzX6dOnj9azZ0+7ZS1bttQee+wxzWx9/fTTT7XixYtrZlCyZElt5syZpj6mRvoa6Mf04sWLWq1atbSkpCStXbt22vDhw3NdN9CPqyt9DeTjOmrUKK1Ro0aG1/flcWXGyYs2btwonTt3tlvWrVs3tdysGjduLBUqVJAuXbrI+vXrJRCdP39e/SxVqpTpj62RvkJqaqpUq1ZN3Ssqv0yGP8rMzJS5c+eqzBqGscx8TI30NdCPKbKmyOY7Hi8zHldX+hrox3XPnj2qPKRGjRrSv39/OXTokF8e16C7V11hQn1PuXLl7JbhMW5WePnyZYmJiRGzQLD0wQcfSPPmzSU9PV1mzpwp7du3l82bN6sar0C6CTSGVG+++WZp0KCBy8c2EGq6XO1rnTp1ZNasWWqYAIHW5MmTpXXr1uof5MK472NBJCcnq+DhypUrUqRIEVmwYIHUr1/flMfUlb4G8jFFULh161Y1fGVEIB9XV/sayMe1ZcuWqkYLfcAw3ZgxY6Rt27Zq6A01mf50XBk4kUfgy46mw/+s+/btk6lTp8oXX3whgQJ/3eF/1LzG1s3CaF9xMrbNXODY1qtXT9VcjBs3TvwZvpOoL8RJ5Ouvv5aBAweqOq/cAopA5kpfA/WYHj58WIYPH67q8wKl6Lkw+xqoxxV69OghOgR+CKSQOZs3b56qY/InDJy8qHz58vLPP//YLcNjFO+ZKduUmxtvvDGgApBhw4bJDz/8oK4ozO+vs9yOLZabra+OIiIipEmTJrJ3717xd5GRkepqVmjWrJn6y3369OnqRGK2Y+pKXwP1mKL4/cSJE3ZZbAxN4ns8Y8YMle0OCwszxXF1p6+BelydKVGihNSuXTvXbfflcWWNkxch8l+xYoXdMvz1kFfdgZngr18M4fk71L8jkMDQxsqVK6V69eqmPbbu9NUR/vHGsFAgHFtnw5M44ZjpmLrT10A9pp06dVLbiX9b9IbyANTD4HdngUSgHld3+hqoxzW3Wi2MWuS27T49rl4vPzcRXN2wbds21bDrpkyZon4/ePCgev7FF1/UHnjgAev6f/31lxYbG6s9//zz2u7du7V3331XCwsL05YsWaKZra9Tp07VFi5cqO3Zs0dLTk5WV36EhoZqy5cv1/zd448/rq5EWb16tXbs2DFrS0tLs66DvqLPuvXr12vh4eHa5MmT1bHFFSERERGq72br65gxY7SlS5dq+/bt03799Vft3nvv1aKjo7Vdu3Zp/gx9wNWC+/fv13bs2KEeh4SEaMuWLTPVMXWnr4F6TJ1xvNLMTMfV1b4G8nF99tln1b9L+A7jmHXu3FkrU6aMuvLX344rAycX6JfcO7aBAweq5/ETX2zH1zRu3FiLjIzUatSooS4XNWNfJ06cqN1www3qf9JSpUpp7du311auXKkFAmf9RLM9Vuir3nfdvHnztNq1a6tji2knfvzxR82MfR0xYoRWtWpV1c9y5cppiYmJ2tatWzV/9/DDD2vVqlVT2122bFmtU6dO1kDCTMfUnb4G6jE1EkyY6bi62tdAPq59+/bVKlSooLa9UqVK6vHevXv98riG4D/ez2sRERERBT7WOBEREREZxMCJiIiIyCAGTkREREQGMXAiIiIiMoiBExEREZFBDJyIiIiIDGLgRERERGQQAyciIiIigxg4ERG5KCQkRBYuXOjrzSAiH2DgREQB5cEHH1SBi2Pr3r27rzeNiIJAuK83gIjIVQiSPv30U7tlUVFRPtseIgoezDgRUcBBkFS+fHm7VrJkSfUcsk/vv/++9OjRQ2JiYqRGjRry9ddf270+OTlZOnbsqJ4vXbq0PProo5Kammq3zqxZsyQ+Pl59VoUKFWTYsGF2z586dUp69+4tsbGxUqtWLfn+++8LoedE5GsMnIjIdF577TW566675LfffpP+/fvLvffeK7t371bPXbp0Sbp166YCrS1btsj8+fNl+fLldoERAq+hQ4eqgApBFoKimjVr2n3GmDFjpE+fPrJjxw5JTExUn3PmzJlC7ysRFTKNiCiADBw4UAsLC9Pi4uLs2htvvKGexz9rQ4YMsXtNy5Yttccff1z9/tFHH2klS5bUUlNTrc//+OOPWmhoqHb8+HH1uGLFitorr7yS6zbgM1599VXrY7wXli1evNjj/SUi/8IaJyIKOB06dFBZIVulSpWy/t6qVSu75/B4+/bt6ndknho1aiRxcXHW52+++WbJysqSlJQUNdR39OhR6dSpU57bkJCQYP0d71WsWDE5ceJEgftGRP6NgRMRBRwEKo5DZ56CuicjIiIi7B4j4ELwRUTmxhonIjKdTZs25Xhcr1499Tt+ovYJtU669evXS2hoqNSpU0eKFi0q119/vaxYsaLQt5uI/B8zTkQUcNLT0+X48eN2y8LDw6VMmTLqdxR8N2/eXNq0aSNffvml/Pzzz/LJJ5+o51DEPWrUKBk4cKCMHj1aTp48KU8++aQ88MADUq5cObUOlg8ZMkSuu+46dXXexYsXVXCF9YgouDFwIqKAs2TJEjVFgC1ki/744w/rFW9z586VJ554Qq331VdfSf369dVzmD5g6dKlMnz4cGnRooV6jCvwpkyZYn0vBFVXrlyRqVOnynPPPacCsrvvvruQe0lE/igEFeK+3ggiIk9BrdGCBQukV69evt4UIjIh1jgRERERGcTAiYiIiMgg1jgRkamw+oCIvIkZJyIiIiKDGDgRERERGcTAiYiIiMggBk5EREREBjFwIiIiIjKIgRMRERGRQQyciIiIiAxi4ERERERkEAMnIiIiIjHm/wAIB/wtK//GxwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 600x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 저장된 모델과 학습 이력 불러오기\n",
        "checkpoint = torch.load(save_model_path, map_location=DEVICE)\n",
        "checkpoint2 = torch.load(save_history_path, map_location=DEVICE)\n",
        "\n",
        "# 같은 구조로 모델 재생성 후 state_dict 로드\n",
        "load_baseline2 = Seq2Seq(encoder, decoder, DEVICE, use_attention=config[\"USE_ATTENTION\"]).to(DEVICE)\n",
        "load_baseline2.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "# 저장된 이력 중 loss history 가져오기\n",
        "loss_history_loss_history_baseline2 = checkpoint2[\"loss_history\"]\n",
        "train_elapsed_time = checkpoint2[\"train_elapsed_time\"]\n",
        "\n",
        "#  베스트 모델 정보 출력\n",
        "best_epoch = checkpoint[\"epoch\"] + 1  \n",
        "best_train_loss = checkpoint[\"train_loss\"]\n",
        "best_val_loss = checkpoint[\"val_loss\"]\n",
        "print(f\"Best model was saved at Epoch {best_epoch}\")\n",
        "print(f\"Train Loss: {best_train_loss:.4f} | Val Loss: {best_val_loss:.4f}\")\n",
        "print(f\"Total training time : {train_elapsed_time:.2f} s\")\n",
        "\n",
        "# 그래프 그리기\n",
        "plot_loss_epoch(model_type, loss_history_loss_history_baseline1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jeeeunkim/Desktop/FIT5217_NLP/NLP/.venv/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "Evaluating Metrics: 100%|██████████| 17/17 [00:23<00:00,  1.37s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss      : 5.7656\n",
            "BLEU-4 Score   : 0.0527\n",
            "METEOR Score   : 0.2104\n",
            "BERTScore (F1) : 0.8421\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(5.76562103356177, 0.05269045547005537, 0.21038731724608936, 0.842138946056366)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 이제 Test 가능\n",
        "Test(load_baseline1, test_loader, criterion, recipe_vocab, MAX_LEN=config[\"MAX_LEN\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# mild_extension1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seq2Seq(\n",
            "  (encoder): Encoder_GRU(\n",
            "    (embedding): Embedding(8991, 256)\n",
            "    (gru): GRU(256, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): AttnDecoderRNN(\n",
            "    (embedding): Embedding(8471, 512)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "    (gru): GRU(512, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
            "    (out): Linear(in_features=1024, out_features=8471, bias=True)\n",
            "  )\n",
            ")\n",
            "Training model: mild_extension1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce6bf6616d354c24a0ae2a05c09d6d40",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12d265bda93c492186bed7c78c2cc254",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training batches:   0%|          | 0/2546 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jeeeunkim/Desktop/FIT5217_NLP/NLP/.venv/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[78], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmild_extension1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m experiment_configs[model_type]\n\u001b[0;32m----> 4\u001b[0m model, encoder, decoder, loss_history, save_model_path, save_history_path \u001b[38;5;241m=\u001b[39m \u001b[43mrun_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[70], line 269\u001b[0m, in \u001b[0;36mrun_train\u001b[0;34m(model_type, config)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    267\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLR\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 269\u001b[0m     loss_history \u001b[38;5;241m=\u001b[39m \u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEPOCHS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTRAIN_RATIO\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_history_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_history_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTEACHER_FORCING_RATIO\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTEACHER_FORCING_RATIO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mMAX_LEN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMAX_LEN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mLR_STEP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mLR_GAMMA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_gamma\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, encoder, decoder, loss_history, save_model_path, save_history_path\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "Cell \u001b[0;32mIn[70], line 53\u001b[0m, in \u001b[0;36mTrain\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, EPOCHS, BATCH_SIZE, TRAIN_RATIO, save_model_path, save_history_path, TEACHER_FORCING_RATIO, MAX_LEN, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m     52\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 53\u001b[0m train_epoch_loss, train_batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEACHER_FORCING_RATIO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_LEN\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m loss_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_epoch_loss)\n\u001b[1;32m     59\u001b[0m loss_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_iter\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_batch_loss)\n",
            "Cell \u001b[0;32mIn[70], line 18\u001b[0m, in \u001b[0;36mloss_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, teacher_forcing_ratio, max_len)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[0;32m~/Desktop/FIT5217_NLP/NLP/.venv/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/FIT5217_NLP/NLP/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model_type = \"mild_extension1\"\n",
        "config = experiment_configs[model_type]\n",
        "\n",
        "model, encoder, decoder, loss_history, save_model_path, save_history_path = run_train(model_type, config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 저장된 모델과 학습 이력 불러오기\n",
        "checkpoint = torch.load(save_model_path, map_location=DEVICE)\n",
        "checkpoint2 = torch.load(save_history_path, map_location=DEVICE)\n",
        "\n",
        "# 같은 구조로 모델 재생성 후 state_dict 로드\n",
        "load_mild_extension1 = Seq2Seq(encoder, decoder, DEVICE, use_attention=config[\"USE_ATTENTION\"]).to(DEVICE)\n",
        "load_mild_extension1.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "# 저장된 이력 중 loss history 가져오기\n",
        "loss_history_loss_history_mild_extension1 = checkpoint2[\"loss_history\"]\n",
        "train_elapsed_time = checkpoint2[\"train_elapsed_time\"]\n",
        "\n",
        "#  베스트 모델 정보 출력\n",
        "best_epoch = checkpoint[\"epoch\"] + 1  \n",
        "best_train_loss = checkpoint[\"train_loss\"]\n",
        "best_val_loss = checkpoint[\"val_loss\"]\n",
        "print(f\"Best model was saved at Epoch {best_epoch}\")\n",
        "print(f\"Train Loss: {best_train_loss:.4f} | Val Loss: {best_val_loss:.4f}\")\n",
        "print(f\"Total training time : {train_elapsed_time:.2f} s\")\n",
        "\n",
        "# 그래프 그리기\n",
        "plot_loss_epoch(model_type, loss_history_loss_history_mild_extension1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 이제 Test 가능\n",
        "Test(load_mild_extension1, test_loader, criterion, recipe_vocab, MAX_LEN=config[\"MAX_LEN\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# mild_extension2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seq2Seq(\n",
            "  (encoder): Encoder_GRU(\n",
            "    (embedding): Embedding(8991, 100)\n",
            "    (gru): GRU(100, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): AttnDecoderRNN(\n",
            "    (embedding): Embedding(8471, 512)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "    (gru): GRU(512, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
            "    (out): Linear(in_features=1024, out_features=8471, bias=True)\n",
            "  )\n",
            ")\n",
            "Training model: mild_extension2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "513e62617c6d4da28cd96c0f093b2790",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "caa0fb65830448d0a9adce0afe83b529",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training batches:   0%|          | 0/2546 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[76], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmild_extension2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m experiment_configs[model_type]\n\u001b[0;32m----> 4\u001b[0m model, encoder, decoder, loss_history, save_model_path, save_history_path \u001b[38;5;241m=\u001b[39m \u001b[43mrun_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[70], line 269\u001b[0m, in \u001b[0;36mrun_train\u001b[0;34m(model_type, config)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    267\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLR\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 269\u001b[0m     loss_history \u001b[38;5;241m=\u001b[39m \u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEPOCHS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTRAIN_RATIO\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_history_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_history_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTEACHER_FORCING_RATIO\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTEACHER_FORCING_RATIO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mMAX_LEN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMAX_LEN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mLR_STEP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mLR_GAMMA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_gamma\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, encoder, decoder, loss_history, save_model_path, save_history_path\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "Cell \u001b[0;32mIn[70], line 53\u001b[0m, in \u001b[0;36mTrain\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, EPOCHS, BATCH_SIZE, TRAIN_RATIO, save_model_path, save_history_path, TEACHER_FORCING_RATIO, MAX_LEN, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m     52\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 53\u001b[0m train_epoch_loss, train_batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEACHER_FORCING_RATIO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_LEN\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m loss_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_epoch_loss)\n\u001b[1;32m     59\u001b[0m loss_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_iter\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_batch_loss)\n",
            "Cell \u001b[0;32mIn[70], line 18\u001b[0m, in \u001b[0;36mloss_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, teacher_forcing_ratio, max_len)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[0;32m~/Desktop/FIT5217_NLP/NLP/.venv/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/FIT5217_NLP/NLP/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model_type = \"mild_extension2\"\n",
        "config = experiment_configs[model_type]\n",
        "\n",
        "model, encoder, decoder, loss_history, save_model_path, save_history_path = run_train(model_type, config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 저장된 모델과 학습 이력 불러오기\n",
        "checkpoint = torch.load(save_model_path, map_location=DEVICE)\n",
        "checkpoint2 = torch.load(save_history_path, map_location=DEVICE)\n",
        "\n",
        "# 같은 구조로 모델 재생성 후 state_dict 로드\n",
        "load_mild_extension2 = Seq2Seq(encoder, decoder, DEVICE, use_attention=config[\"USE_ATTENTION\"]).to(DEVICE)\n",
        "load_mild_extension2.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "# 저장된 이력 중 loss history 가져오기\n",
        "loss_history_loss_history_mild_extension2 = checkpoint2[\"loss_history\"]\n",
        "train_elapsed_time = checkpoint2[\"train_elapsed_time\"]\n",
        "\n",
        "#  베스트 모델 정보 출력\n",
        "best_epoch = checkpoint[\"epoch\"] + 1  \n",
        "best_train_loss = checkpoint[\"train_loss\"]\n",
        "best_val_loss = checkpoint[\"val_loss\"]\n",
        "print(f\"Best model was saved at Epoch {best_epoch}\")\n",
        "print(f\"Train Loss: {best_train_loss:.4f} | Val Loss: {best_val_loss:.4f}\")\n",
        "print(f\"Total training time : {train_elapsed_time:.2f} s\")\n",
        "\n",
        "# 그래프 그리기\n",
        "plot_loss_epoch(model_type, loss_history_loss_history_mild_extension2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 이제 Test 가능\n",
        "Test(load_mild_extension2, test_loader, criterion, recipe_vocab, MAX_LEN=config[\"MAX_LEN\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plot for all model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_loss_iter(\n",
        "    baseline1=loss_history_loss_history_baseline1,\n",
        "    baseline2=loss_history_loss_history_baseline2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_recipes(ingredient_list, ingredient_vocab, recipe_vocab, max_len=30, is_raw_string=False, **models):\n",
        "\n",
        "    # Tokenizer 적용 여부\n",
        "    if is_raw_string:\n",
        "        tokens = tokenizer_ingredient(str(ingredient_list))\n",
        "    else:\n",
        "        tokens = ingredient_list\n",
        "\n",
        "    # index로 변환\n",
        "    tokens_ids = [ ingredient_vocab[token] if token in ingredient_vocab else ingredient_vocab['<unk>'] for token in tokens]\n",
        "    src_tensor = torch.tensor(tokens_ids, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
        "\n",
        "    print(\"Ingredient :\",', '.join(tokens))\n",
        "    for name, model in models.items():\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            generated = model(src_tensor, target=None, teacher_forcing_ratio=0.0, max_len=max_len)\n",
        "            pred_ids = generated[0].tolist()\n",
        "\n",
        "            # <eos> 기준으로 자르기\n",
        "            if recipe_vocab['<eos>'] in pred_ids:\n",
        "                pred_ids = pred_ids[:pred_ids.index(recipe_vocab['<eos>'])]\n",
        "\n",
        "            pred_tokens = [recipe_vocab.get_itos()[idx] for idx in pred_ids]\n",
        "            print(f\"{name}: {' '.join(pred_tokens[:30])}\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ingredient : sugar, lemon juice,  water,  orange juice, strawberries, icecream\n",
            "baseline1: and sugar and vanilla and pour into a 9 x 13 inch pan\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ingredient : philadelphia, cream, cheese, sweetened, condensed, milk, ts, vanilla,  , lemon, juice, canned, cherries, graham, cracker,  , pie, crusts\n",
            "baseline1: cream cheese and vanilla and vanilla in a large bowl add vanilla and vanilla mix well and pour into a 9 x 13 inch pan\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "models = {\n",
        "    \"baseline1\": load_baseline1,\n",
        "    # \"baseline2\": load_baseline2\n",
        "}\n",
        "\n",
        "# # Sample 1: 전처리된 리스트\n",
        "sample1_raw = \"sugar, lemon juice,  water,  orange juice, strawberries, icecream\"\n",
        "sample1 = sample1_raw.split(\", \")\n",
        "generate_recipes(sample1, ingredient_vocab, recipe_vocab, is_raw_string=False, **models)\n",
        "\n",
        "# Sample2 : \n",
        "sample2_raw =\"8 oz philadelphia cream cheese, 14 oz can sweetened condensed milk, 1 ts vanilla, 1/3 c  lemon juice, 48 oz canned cherries, 8 inch graham cracker,  pie crusts\"\n",
        "sample2 = sample2_raw.split(\", \")\n",
        "generate_recipes(sample2, ingredient_vocab, recipe_vocab, is_raw_string=True, **models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7NLnAN8Mmem"
      },
      "outputs": [],
      "source": [
        "##### 예전꺼(결과있음!!!) #####\n",
        "def compute_bleu(model, dataloader, recipe_vocab, max_len=50):\n",
        "    \"\"\"\n",
        "    Seq2Seq 모델의 BLEU 점수를 계산 (1~4-gram 기준)\n",
        "\n",
        "    Args:\n",
        "        model: 학습된 Seq2Seq 모델\n",
        "        dataloader: 검증 또는 테스트 DataLoader\n",
        "        recipe_vocab: recipe vocab 객체 (인덱스 → 단어 변환용)\n",
        "        max_len: 생성할 최대 문장 길이\n",
        "\n",
        "    Returns:\n",
        "        bleu_score (float): 0~100 사이의 BLEU 점수\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src_batch, trg_batch in tqdm(dataloader, desc=\"Evaluating BLEU\"):\n",
        "            src_batch = src_batch.to(DEVICE)\n",
        "            trg_batch = trg_batch.to(DEVICE)\n",
        "\n",
        "            # 생성된 예측 시퀀스: [batch_size, max_len]\n",
        "            generated = model(src_batch, target=None, teacher_forcing_ratio=0.0, max_len=max_len)\n",
        "\n",
        "            for i in range(src_batch.size(0)):\n",
        "                pred_tokens = generated[i].tolist()\n",
        "                trg_tokens = trg_batch[i].tolist()\n",
        "\n",
        "                # <eos> 토큰이 나오면 거기서 자름\n",
        "                if recipe_vocab['<eos>'] in pred_tokens:\n",
        "                    pred_tokens = pred_tokens[:pred_tokens.index(recipe_vocab['<eos>'])]\n",
        "                if recipe_vocab['<eos>'] in trg_tokens:\n",
        "                    trg_tokens = trg_tokens[:trg_tokens.index(recipe_vocab['<eos>'])]\n",
        "\n",
        "                preds.append(pred_tokens)\n",
        "                targets.append([trg_tokens])  # corpus_bleu는 list of list 필요\n",
        "\n",
        "    bleu = corpus_bleu(targets, preds) * 100\n",
        "    print(f\"\\nBLEU Score: {bleu:.2f}\")\n",
        "    return bleu\n",
        "\n",
        "\n",
        "def loss_epoch(model, dataloader, criterion, optimizer=None, teacher_forcing_ratio=0.5):\n",
        "    \"\"\"\n",
        "    학습 또는 평가 루프에서 1 epoch 동안 loss만 계산하는 함수.\n",
        "\n",
        "    Args:\n",
        "        model: Seq2Seq 모델\n",
        "        dataloader: DataLoader\n",
        "        criterion: 손실 함수 (CrossEntropyLoss)\n",
        "        optimizer: 옵티마이저 (None이면 eval 모드로 동작)\n",
        "        teacher_forcing_ratio: 학습 중 Teacher Forcing 비율\n",
        "\n",
        "    Returns:\n",
        "        epoch_loss: 전체 데이터셋에 대한 평균 loss\n",
        "    \"\"\"\n",
        "    model.train() if optimizer else model.eval()\n",
        "    rloss = 0\n",
        "\n",
        "    for src_batch, trg_batch in tqdm(dataloader, leave=False):\n",
        "        src_batch = src_batch.to(DEVICE)\n",
        "        trg_batch = trg_batch.to(DEVICE)\n",
        "\n",
        "        output = model(src_batch, trg_batch, teacher_forcing_ratio=teacher_forcing_ratio)\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        # CrossEntropyLoss 계산을 위한 reshape\n",
        "        output = output[:, 1:, :].reshape(-1, output_dim)\n",
        "        trg = trg_batch[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        if optimizer:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "            optimizer.step()\n",
        "\n",
        "        rloss += loss.item() * src_batch.shape[0]\n",
        "\n",
        "    return rloss / len(dataloader.dataset)\n",
        "\n",
        "def Train(model, train_loader, val_loader, criterion, optimizer,\n",
        "          EPOCHS, BATCH_SIZE, TRAIN_RATIO,\n",
        "          save_model_path, save_history_path,\n",
        "          ):\n",
        "    \"\"\"\n",
        "    이어서 학습할 수 있게 start_epoch와 best_val_loss를 인자로 받음\n",
        "    \"\"\"\n",
        "\n",
        "    loss_history = {\"train\": [], \"val\": []}\n",
        "    best_val_loss = float('inf')\n",
        "    train_start_time = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "        ep_start_time = time.time()\n",
        "\n",
        "        # Training\n",
        "        train_loss = loss_epoch(model, train_loader, criterion, optimizer, teacher_forcing_ratio=0.5)\n",
        "        loss_history[\"train\"].append(train_loss)\n",
        "\n",
        "        # Validation\n",
        "        val_loss = loss_epoch(model, val_loader, criterion, optimizer=None, teacher_forcing_ratio=0.0)\n",
        "        loss_history[\"val\"].append(val_loss)\n",
        "\n",
        "        ep_elapsed_time = time.time() - ep_start_time\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save({\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"epoch\": epoch,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"train_loss\": train_loss,  # train_loss 저장\n",
        "            }, save_model_path)\n",
        "            print(\"Best model saved!\")\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Time: {ep_elapsed_time:.2f}s\")\n",
        "    train_elapsed_time = time.time() - train_start_time\n",
        "    # Save training history\n",
        "    torch.save({\n",
        "        \"loss_history\": loss_history,\n",
        "        \"EPOCHS\": EPOCHS,\n",
        "        \"BATCH_SIZE\": BATCH_SIZE,\n",
        "        \"TRAIN_RATIO\": TRAIN_RATIO\n",
        "    }, save_history_path)\n",
        "\n",
        "    print(f\"\\nTraining Completed! History saved!| Elapsed Time : {train_elapsed_time}\")\n",
        "    return loss_history\n",
        "\n",
        "\n",
        "def Test(model, test_loader, criterion, recipe_vocab):\n",
        "    \"\"\"\n",
        "    테스트 데이터셋에서 손실과 BLEU 점수를 계산하는 함수.\n",
        "\n",
        "    Args:\n",
        "        model: 학습된 Seq2Seq 모델\n",
        "        test_loader: 테스트 DataLoader\n",
        "        criterion: 손실 함수\n",
        "        recipe_vocab: vocab 객체 (BLEU 계산용)\n",
        "\n",
        "    Returns:\n",
        "        test_loss (float), bleu_score (float)\n",
        "    \"\"\"\n",
        "    print(\"\\n Testing on test set...\")\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loss = loss_epoch(model, test_loader, criterion, optimizer=None, teacher_forcing_ratio=0.0)\n",
        "\n",
        "    bleu_score = compute_bleu(model, test_loader, recipe_vocab)\n",
        "    print(f\"Test Loss: {test_loss:.4f} | BLEU Score: {bleu_score:.2f}\")\n",
        "\n",
        "    return test_loss, bleu_score\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMJsudpjHp_O",
        "outputId": "e6f98c97-7965-4c15-8363-35e66b1a4453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New model training!\n",
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2546 [00:00<?, ?it/s]/home/psarda/repos/NLP/.venv/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model saved!\n",
            "Train Loss: 4.3775 | Val Loss: 4.9679 | Time: 3727.94s\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model saved!\n",
            "Train Loss: 3.9285 | Val Loss: 4.9464 | Time: 3716.95s\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model saved!\n",
            "Train Loss: 3.8209 | Val Loss: 4.8882 | Time: 3724.51s\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.7594 | Val Loss: 4.8932 | Time: 3715.59s\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.7219 | Val Loss: 4.9373 | Time: 3987.15s\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.6916 | Val Loss: 4.8978 | Time: 3766.76s\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.6670 | Val Loss: 4.9374 | Time: 3943.18s\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.6451 | Val Loss: 4.8948 | Time: 4250.35s\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.6278 | Val Loss: 4.9209 | Time: 4195.73s\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.6148 | Val Loss: 4.9444 | Time: 4038.07s\n",
            "\n",
            "Training Completed! History saved!| Elapsed Time : 39066.561606884\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "##### 예전꺼(결과있음!!!) #####\n",
        "\n",
        "# 모델 생성\n",
        "encoder = Encoder_GRU(INPUT_DIM, EMBED_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
        "decoder = Decoder_GRU(OUTPUT_DIM, EMBED_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
        "model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
        "\n",
        "if new_model_train:\n",
        "    print(\"New model training!\")\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "    loss_history = Train(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=dev_loader,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        EPOCHS=EPOCHS,\n",
        "        BATCH_SIZE=BATCH_SIZE,\n",
        "        TRAIN_RATIO=TRAIN_RATIO,\n",
        "        save_model_path=save_model_path,\n",
        "        save_history_path=save_history_path,\n",
        "    )\n",
        "\n",
        "    # 플롯 그리기\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(range(1, EPOCHS+1), loss_history[\"train\"], label=\"Train Loss\", color=\"blue\")\n",
        "    plt.plot(range(1, EPOCHS+1), loss_history[\"val\"], label=\"Validation Loss\", color=\"red\", linestyle='--')\n",
        "    plt.xlabel(\"Epoch\", fontsize=18)\n",
        "    plt.ylabel(\"Loss\", fontsize=18)\n",
        "    plt.title(\"Training and Validation Loss\", fontsize=20)\n",
        "    plt.legend(fontsize=14)\n",
        "    plt.grid()\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
