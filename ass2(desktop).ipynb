{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pnWO37v6oLFg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from bert_score import score as bert_score_fn\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import re\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "import gdown\n",
        "from functools import partial\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import spacy\n",
        "import ast\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mps\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    DEVICE = 'mps'  # Apple GPU 사용\n",
        "elif torch.cuda.is_available():\n",
        "    DEVICE = 'cuda'  # NVIDIA GPU 사용\n",
        "else:\n",
        "    DEVICE = 'cpu'   # CPU fallback\n",
        "\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iuol8CxrHQZC"
      },
      "outputs": [],
      "source": [
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzpEEDthSnhi",
        "outputId": "3677fb2b-d313-4960-fa2a-f5932daa0af8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data size: 162899\n",
            "Dev data size: 1065\n",
            "Test data size: 1081\n",
            "\n",
            "Train data sample:\n",
            "                      Title  \\\n",
            "0       No-Bake Nut Cookies   \n",
            "1               Creamy Corn   \n",
            "2      Reeses Cups(Candy)     \n",
            "3  Cheeseburger Potato Soup   \n",
            "4       Rhubarb Coffee Cake   \n",
            "\n",
            "                                         Ingredients  \\\n",
            "0  [\"1 c. firmly packed brown sugar\", \"1/2 c. eva...   \n",
            "1  [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...   \n",
            "2  [\"1 c. peanut butter\", \"3/4 c. graham cracker ...   \n",
            "3  [\"6 baking potatoes\", \"1 lb. of extra lean gro...   \n",
            "4  [\"1 1/2 c. sugar\", \"1/2 c. butter\", \"1 egg\", \"...   \n",
            "\n",
            "                                              Recipe  \n",
            "0  [\"In a heavy 2-quart saucepan, mix brown sugar...  \n",
            "1  [\"In a slow cooker, combine all ingredients. C...  \n",
            "2  [\"Combine first four ingredients and press in ...  \n",
            "3  [\"Wash potatoes; prick several times with a fo...  \n",
            "4  [\"Cream sugar and butter.\", \"Add egg and beat ...  \n"
          ]
        }
      ],
      "source": [
        "# data 다운\n",
        "train_path = 'Cooking_Dataset/train.csv'\n",
        "dev_path = 'Cooking_Dataset/dev.csv'\n",
        "test_path = 'Cooking_Dataset/test.csv'\n",
        "\n",
        "\n",
        "if not os.path.exists('Cooking_Dataset'):\n",
        "    os.makedirs('Cooking_Dataset')\n",
        "    print(\"Downloading Dataset\") \n",
        "    gdown.download(\"https://drive.google.com/uc?id=1uZdYjvllt0dSdKKtrCgKHUk-APKdmeNU\", train_path, quiet=False)\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1SAMbkdtjGBYgojqobiwe7ZmnEq7SiGsF\", dev_path, quiet=False)\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1v6Rr2et_4WA5mRwwlRxtLhn38pbmr9Yr\", test_path, quiet=False)\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "#train_df = train_df.sample(n=200,random_state=42)\n",
        "dev_df = pd.read_csv(dev_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"Train data size: {len(train_df)}\")\n",
        "print(f\"Dev data size: {len(dev_df)}\")\n",
        "print(f\"Test data size: {len(test_df)}\")\n",
        "print(\"\\nTrain data sample:\")\n",
        "print(train_df.head())\n",
        "#No-Bake Nut Cookies\n",
        "# [\"1 c. firmly packed brown sugar\", \"1/2 c. evaporated milk\", \"1/2 tsp. vanilla\", \"1/2 c. broken nuts (pecans)\", \"2 Tbsp. butter or margarine\", \"3 1/2 c. bite size shredded rice biscuits\"]\n",
        "# [\"In a heavy 2-quart saucepan, mix brown sugar, nuts, evaporated milk and butter or margarine.\", \"Stir over medium heat until mixture bubbles all over top.\", \"Boil and stir 5 minutes more. Take off heat.\", \"Stir in vanilla and cereal; mix well.\", \"Using 2 teaspoons, drop and shape into 30 clusters on wax paper.\", \"Let stand until firm, about 30 minutes.\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtQwUvtETjCF",
        "outputId": "86b57a8d-0dd5-4fa7-abdf-0d96cb367749"
      },
      "outputs": [],
      "source": [
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenizer_ingredient_baseline(text):\n",
        "    \"\"\"\n",
        "    Baseline 전처리: 소문자화 + lemmatization + 간단한 필터링\n",
        "    - stopword 제거 있음\n",
        "    - 숫자, 구두점 제거 있음\n",
        "    - 불필요한 복잡 전처리 없음\n",
        "    \"\"\"\n",
        "    import ast\n",
        "    text_list = ast.literal_eval(text)\n",
        "    tokens = []\n",
        "\n",
        "    with spacy_en.select_pipes(disable=[\"parser\", \"ner\", \"tagger\"]):\n",
        "        for item in text_list:\n",
        "            doc = spacy_en(item.lower())\n",
        "            for token in doc:\n",
        "                if token.is_punct or token.like_num or token.is_stop:\n",
        "                    continue\n",
        "                tokens.append(token.lemma_.strip())  # 기본 lemmatization만 유지\n",
        "    return tokens\n",
        "\n",
        "\n",
        "\n",
        "def tokenizer_recipe_baseline(text):\n",
        "    \"\"\"\n",
        "    Recipe 전처리: 소문자화 + lemmatization만 수행 (구두점, stopword, 숫자는 유지)\n",
        "    - 조리 순서, 동사 등 자연스러운 문장 흐름을 보존해야 하므로 간단한 전처리만 적용\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    with spacy_en.select_pipes(disable=[\"parser\", \"ner\", \"tagger\"]):\n",
        "        doc = spacy_en(text.lower())\n",
        "        for token in doc:\n",
        "            # 너무 공격적인 필터링은 하지 않음\n",
        "            if token.is_space:\n",
        "                continue\n",
        "            tokens.append(token.lemma_.strip())\n",
        "    return tokens\n",
        "\n",
        "\n",
        "\n",
        "def tokenizer_ingredient_checklist(text, remove_stopwords=True, lemmatize=True):\n",
        "    text_list = ast.literal_eval(text)  # 문자열 → 리스트로 변환\n",
        "    unit_keywords = {\n",
        "        'c', 'cup', 'cups', 'tbsp', 'tablespoon', 'tsp', 'teaspoon',\n",
        "        'oz', 'ounce', 'lb', 'pound', 'g', 'kg', 'mg',\n",
        "        'pt', 'qt', 'gal', 'ml', 'l','carton','container',\n",
        "        'package', 'pkg', 'envelope', 'box', 'bag', 'jar', 'can', 'cans', 'bottle',\n",
        "        'dash', 'pinch', 'slice', 'slices', 'head', 'inch', 'inches',\n",
        "        'stick', 'sticks', 'small', 'medium', 'large', 'size','graham'\n",
        "    }\n",
        "\n",
        "    tokens = []\n",
        "\n",
        "    with spacy_en.select_pipes(disable=[\"parser\", \"ner\"]):\n",
        "        for item in text_list:\n",
        "            doc = spacy_en(item.lower())\n",
        "            for token in doc:\n",
        "                if token.is_punct or token.is_space:\n",
        "                    continue\n",
        "                if token.like_num:\n",
        "                    continue\n",
        "                if token.text.strip(\".\") in unit_keywords:\n",
        "                    continue\n",
        "                if remove_stopwords and token.is_stop:\n",
        "                    continue\n",
        "                if token.pos_ in {\"ADJ\", \"VERB\", \"ADV\", \"PRON\"}:\n",
        "                    continue\n",
        "\n",
        "                tokens.append(token.lemma_.strip() if lemmatize else token.text.strip())\n",
        "    \n",
        "\n",
        "    return tokens\n",
        "\n",
        "def tokenizer_recipe_extension(text):  # 디폴트: lemmatize 안 함\n",
        "    text_list = ast.literal_eval(text)\n",
        "    tokens = []\n",
        "\n",
        "    # tagger는 유지해서 lemma 써도 warning 없음\n",
        "    with spacy_en.select_pipes(disable=[\"parser\", \"ner\", \"tagger\"]):\n",
        "        for item in text_list:\n",
        "            doc = spacy_en(item.lower())  # 소문자화\n",
        "            for token in doc:\n",
        "                if token.is_punct or token.is_space:\n",
        "                    continue  # 마침표, 쉼표 제거\n",
        "                if token.like_num:  # 숫자 유지\n",
        "                    tokens.append(token.text.strip())\n",
        "                    continue\n",
        "                \n",
        "                tokens.append(token.text.strip())\n",
        "                \n",
        "    return tokens\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"In a slow cooker, combine all ingredients. Cover and cook on low for 4 hours or until heated through and cheese is melted. Stir well before serving. Yields 6 servings.\"]\n",
            "['in', 'a', 'slow', 'cooker', 'combine', 'all', 'ingredients', 'cover', 'and', 'cook', 'on', 'low', 'for', '4', 'hours', 'or', 'until', 'heated', 'through', 'and', 'cheese', 'is', 'melted', 'stir', 'well', 'before', 'serving', 'yields', '6', 'servings']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jeeeunkim/Desktop/FIT5217_NLP/NLP/.venv/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ],
      "source": [
        "ing = train_df.iloc[1,2]\n",
        "print(ing)\n",
        "print(tokenizer_recipe_extension(ing))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "O5IrjwMXuv3h"
      },
      "outputs": [],
      "source": [
        "def build_vocab(token_lists, min_freq=2):\n",
        "    # vocab 생성: 자주 등장하는 단어만 포함 + 특수 토큰 정의\n",
        "    vocab = build_vocab_from_iterator(\n",
        "        token_lists,  # 토큰 리스트들을 직접 반복\n",
        "        min_freq=min_freq,  # 최소 등장 빈도\n",
        "        specials=['<pad>', '<sos>', '<eos>', '<unk>']  # 특수 토큰 추가\n",
        "    )\n",
        "    vocab.set_default_index(vocab['<unk>'])  # 없는 단어는 <unk>로 처리\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_or_tokenize_data(model_type, train_df, config=None):\n",
        "    use_checklist = config.get(\"USE_CHECKLIST\", False)\n",
        "\n",
        "    tokenizer_ingredient = tokenizer_ingredient_baseline\n",
        "    tokenizer_recipe = tokenizer_recipe_baseline\n",
        "    tokenizer_checklist = tokenizer_ingredient_checklist if use_checklist else None\n",
        "\n",
        "    ingredient_cache_path = \"tokens/ingredient_tokens.pkl\"\n",
        "    recipe_cache_path = \"tokens/recipe_tokens.pkl\"\n",
        "    checklist_cache_path = \"tokens/checklist_tokens.pkl\"\n",
        "\n",
        "    ingredient_cache_download = \"https://drive.google.com/uc?id=1Wxvq-qy4ifbzPKmcm_VuNRRXrEOSqisJ\"\n",
        "    recipe_cache_download = \"https://drive.google.com/uc?id=1IyBlDfL9sE8_Ip3muDyciCK9Hiv_VZF8\"\n",
        "    checklist_cache_download = \"https://drive.google.com/uc?id=1-WVksjq8Cgj6UiJ7wtemU2CfP4hhPrci\"\n",
        "\n",
        "    if not os.path.exists('tokens'):\n",
        "        os.makedirs('tokens')\n",
        "\n",
        "    cache_targets = [\n",
        "        (ingredient_cache_path, ingredient_cache_download),\n",
        "        (recipe_cache_path, recipe_cache_download)\n",
        "    ]\n",
        "    if use_checklist:\n",
        "        cache_targets.append((checklist_cache_path, checklist_cache_download))\n",
        "\n",
        "    for path, url in cache_targets:\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"⬇️  Downloading: {path}\")\n",
        "            try:\n",
        "                r = requests.get(url, allow_redirects=True)\n",
        "                if r.status_code == 200:\n",
        "                    with open(path, 'wb') as f:\n",
        "                        f.write(r.content)\n",
        "                    print(f\"✅ Downloaded: {path}\")\n",
        "                else:\n",
        "                    print(f\"❌ Download failed for {path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Download error: {e}\")\n",
        "\n",
        "    if os.path.exists(ingredient_cache_path):\n",
        "        with open(ingredient_cache_path, \"rb\") as f:\n",
        "            ingredient_token_lists = pickle.load(f)\n",
        "        print(\"✅ Loaded ingredient token cache.\")\n",
        "    else:\n",
        "        print(\"⚠️ No ingredient cache → Tokenizing...\")\n",
        "        ingredient_token_lists = [tokenizer_ingredient(text) for text in tqdm(train_df['Ingredients'], desc=\"Tokenizing ingredients\")]\n",
        "        with open(ingredient_cache_path, \"wb\") as f:\n",
        "            pickle.dump(ingredient_token_lists, f)\n",
        "\n",
        "    if os.path.exists(recipe_cache_path):\n",
        "        with open(recipe_cache_path, \"rb\") as f:\n",
        "            recipe_token_lists = pickle.load(f)\n",
        "        print(\"✅ Loaded recipe token cache.\")\n",
        "    else:\n",
        "        print(\"⚠️ No recipe cache → Tokenizing...\")\n",
        "        recipe_token_lists = [tokenizer_recipe(text) for text in tqdm(train_df['Recipe'], desc=\"Tokenizing recipes\")]\n",
        "        with open(recipe_cache_path, \"wb\") as f:\n",
        "            pickle.dump(recipe_token_lists, f)\n",
        "\n",
        "    ingredient_vocab = build_vocab(ingredient_token_lists, min_freq=1)\n",
        "    recipe_vocab = build_vocab(recipe_token_lists, min_freq=2)\n",
        "\n",
        "    checklist_vocab = None\n",
        "    if use_checklist:\n",
        "        if os.path.exists(checklist_cache_path):\n",
        "            with open(checklist_cache_path, \"rb\") as f:\n",
        "                checklist_token_lists = pickle.load(f)\n",
        "            print(\"✅ Loaded checklist token cache.\")\n",
        "        else:\n",
        "            print(\"⚠️ No checklist cache → Tokenizing...\")\n",
        "            checklist_token_lists = [tokenizer_checklist(text) for text in tqdm(train_df['Ingredients'], desc=\"Checklist tokenize\")]\n",
        "            with open(checklist_cache_path, \"wb\") as f:\n",
        "                pickle.dump(checklist_token_lists, f)\n",
        "\n",
        "        checklist_vocab = build_vocab(checklist_token_lists, min_freq=1)\n",
        "\n",
        "    return ingredient_token_lists, recipe_token_lists, ingredient_vocab, recipe_vocab, tokenizer_ingredient, tokenizer_recipe, checklist_vocab, tokenizer_checklist\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_checklist_tensor(checklist_vocab, checklist_tokenizer, ingredient_texts, device, embedding_layer):\n",
        "    batch_ids = []\n",
        "\n",
        "\n",
        "\n",
        "    for text in ingredient_texts:\n",
        "        tokens = checklist_tokenizer(text)\n",
        "        ids = [checklist_vocab[token] for token in tokens]\n",
        "        batch_ids.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    checklist_padded = pad_sequence(batch_ids, batch_first=True, padding_value=checklist_vocab['<pad>']).to(device)\n",
        "    checklist_mask = (checklist_padded != checklist_vocab['<pad>']).float().to(device)\n",
        "    checklist_embeds = embedding_layer(checklist_padded)\n",
        "\n",
        "\n",
        "\n",
        "    return checklist_embeds, checklist_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "u3aRuH1BcoQQ"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, ingredient_vocab, recipe_vocab, tokenizer_ingredient, tokenizer_recipe):\n",
        "        self.df = df\n",
        "        self.ingredient_vocab = ingredient_vocab\n",
        "        self.recipe_vocab = recipe_vocab\n",
        "        self.tokenizer_ingredient = tokenizer_ingredient\n",
        "        self.tokenizer_recipe = tokenizer_recipe\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ingredient_text = self.df.iloc[idx]['Ingredients']\n",
        "        recipe_text = self.df.iloc[idx]['Recipe']\n",
        "\n",
        "        ingredient_tokens = self.tokenizer_ingredient(ingredient_text)\n",
        "        recipe_tokens = self.tokenizer_recipe(recipe_text)\n",
        "\n",
        "        ingredient_ids = [self.ingredient_vocab['<sos>']] + [self.ingredient_vocab[token] for token in ingredient_tokens] + [self.ingredient_vocab['<eos>']]\n",
        "        recipe_ids = [self.recipe_vocab['<sos>']] + [self.recipe_vocab[token] for token in recipe_tokens] + [self.recipe_vocab['<eos>']]\n",
        "\n",
        "        return torch.tensor(ingredient_ids), torch.tensor(recipe_ids), ingredient_text  # ✅ 3개 반환\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zvtpEXhJyzRN"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch, ingredient_vocab, recipe_vocab, device):\n",
        "    ingredients, recipes, ingredient_texts = zip(*batch)  # ingredient_texts: str\n",
        "    ingredients_padded = pad_sequence(ingredients, batch_first=True, padding_value=ingredient_vocab['<pad>'])\n",
        "    recipes_padded = pad_sequence(recipes, batch_first=True, padding_value=recipe_vocab['<pad>'])\n",
        "    return ingredients_padded.to(device), recipes_padded.to(device), list(ingredient_texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHuvOqpGoe5N"
      },
      "outputs": [],
      "source": [
        "class Encoder_GRU(nn.Module):\n",
        "    def __init__(self, ingredient_vocab_size, embedding_dim, hidden_dim, n_layers, dropout_ratio,\n",
        "                 embedding_weights=None, freeze=False):\n",
        "        super().__init__()\n",
        "        # 임베딩\n",
        "        if embedding_weights is not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=freeze)\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(ingredient_vocab_size, embedding_dim)\n",
        "        \n",
        "\n",
        "        # GRU 레이어\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.gru = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          n_layers,\n",
        "                          dropout=dropout_ratio if n_layers>1 else 0,\n",
        "                          batch_first=True)\n",
        "\n",
        "        # 드롭아웃\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src : [batch_size, src_len]\n",
        "        src = src.long() \n",
        "        # 임베딩\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded : [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        # gru 통과\n",
        "        outputs, hidden = self.gru(embedded) # h0를 따로 주지 않으면, 디폴트로 h0가 0로 초기화되서 들어감\n",
        "        # outputs: [batch_size, src_len, hidden_size]\n",
        "        # hidden: [n_layers, batch_size, hidden_size]\n",
        "\n",
        "        return outputs,hidden\n",
        "\n",
        "class Encoder_GRU_extension(nn.Module):\n",
        "    def __init__(self, ingredient_vocab_size, embedding_dim, hidden_dim, n_layers, dropout_ratio,\n",
        "                 embedding_weights=None, freeze=False):\n",
        "        super().__init__()\n",
        "        if embedding_weights is not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=freeze)\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(ingredient_vocab_size, embedding_dim)\n",
        "        self.ingredient_vocab_size = ingredient_vocab_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers,\n",
        "                          dropout=dropout_ratio if n_layers > 1 else 0,\n",
        "                          batch_first=True)\n",
        "\n",
        "        self.skip_proj = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        self._init_weights()  # ✅ 초기화 적용\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # embedding 초기화 (from_pretrained 아닌 경우만)\n",
        "        if not hasattr(self.embedding, 'weight') or self.embedding.weight.requires_grad:\n",
        "            nn.init.kaiming_normal_(self.embedding.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
        "\n",
        "        # GRU 내부 weight 초기화\n",
        "        for name, param in self.gru.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.kaiming_normal_(param, mode='fan_in', nonlinearity='leaky_relu')\n",
        "            elif 'bias' in name:\n",
        "                nn.init.constant_(param, 0)\n",
        "\n",
        "        # skip connection projection 초기화\n",
        "        nn.init.kaiming_normal_(self.skip_proj.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
        "        nn.init.constant_(self.skip_proj.bias, 0)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = src.long()\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, hidden = self.gru(embedded)\n",
        "        skip = self.skip_proj(embedded)\n",
        "        outputs = self.leaky_relu(outputs + skip)\n",
        "        outputs = self.layer_norm(outputs)\n",
        "        return outputs, hidden\n",
        "\n",
        "\n",
        "\n",
        "class Decoder_GRU(nn.Module):\n",
        "    def __init__(self, recipe_vocab_size, embedding_dim, hidden_dim, n_layers, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        self.recipe_vocab_size = recipe_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "\n",
        "        # 임베딩\n",
        "        self.embedding = nn.Embedding(recipe_vocab_size, embedding_dim)\n",
        "\n",
        "        # GRU 레이어\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.gru = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          n_layers,\n",
        "                          dropout=dropout_ratio if n_layers>1 else 0,\n",
        "                          batch_first=True)\n",
        "\n",
        "        # fc 레이어\n",
        "        self.fc_out = nn.Linear(hidden_dim, recipe_vocab_size)\n",
        "\n",
        "        # 드롭아웃\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input = input.long()\n",
        "        # input : [batch_size]\n",
        "        input = input.unsqueeze(1)\n",
        "        # input : [batch_size, 단어의 개수=1]\n",
        "\n",
        "        # 임베딩\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded : [batch_size, 단어의 개수=1, hidden_dim]\n",
        "\n",
        "        # GRU 통과\n",
        "        outputs, hidden = self.gru(embedded,hidden)\n",
        "        # outputs: [batch_size, 단어의 개수=1, hidden_size]\n",
        "        # hidden: [n_layers, batch_size, hidden_size]\n",
        "\n",
        "        # fc 통과\n",
        "        prediction = self.fc_out(outputs.squeeze(1))\n",
        "        # prediction: [batch_size, vocab_size]\n",
        "\n",
        "        return prediction, hidden\n",
        "\n",
        "# Define a decoder with attention mechanism using PyTorch's nn.Module\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, recipe_vocab_size,embedding_dim, hidden_size, n_layers, dropout_ratio):\n",
        "        # Initialize the base nn.Module class\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "\n",
        "        # Save parameters\n",
        "        self.recipe_vocab_size = recipe_vocab_size              # Size of the output vocabulary\n",
        "        self.embedding_dim = embedding_dim                  # Dropout probability\n",
        "        self.hidden_size = hidden_size              # Size of the hidden state in GRU\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "                  \n",
        "\n",
        "        # Define layers\n",
        "        self.embedding = nn.Embedding(self.recipe_vocab_size, self.hidden_size)  # Converts word indices to dense vectors\n",
        "        self.dropout = nn.Dropout(self.dropout_ratio)                          # Applies dropout for regularization\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, self.n_layers, dropout=self.dropout_ratio if self.n_layers>1 else 0, batch_first=True)\n",
        "             # GRU to process the embedded inputs\n",
        "        self.out = nn.Linear(self.hidden_size * 2, self.recipe_vocab_size)       # Linear layer for generating final output\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        input = input.long() \n",
        "        # input : [batch_size]\n",
        "        input = input.unsqueeze(1)\n",
        "        # input : [batch_size, 단어의 개수=1]\n",
        "\n",
        "        # 임베딩\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded : [batch_size, 단어의 개수=1, hidden_dim]\n",
        "\n",
        "        # Pass through GRU\n",
        "        output, hidden = self.gru(embedded, hidden)  # output: [batch, 1, hidden_size]\n",
        "\n",
        "        # Compute attention weights using dot-product attention:\n",
        "        # hidden[-1]: [batch, hidden_size]\n",
        "        # encoder_outputs: [batch, src_len, hidden_size]\n",
        "    \n",
        "        attn_weights = F.softmax(\n",
        "            torch.bmm(output, encoder_outputs.transpose(1, 2)), # [batch, 1, hidden_size] x [batch, hidden_size, src_len]\n",
        "            dim=-1\n",
        "        )  # [batch, 1, src_len]\n",
        "\n",
        "        # Apply attention weights to encoder outputs to get context vector\n",
        "        # attn_weights: (1, 1, max_length)\n",
        "        # encoder_outputs.unsqueeze(0): (1, max_length, hidden_size)\n",
        "        attn_output = torch.bmm(attn_weights, encoder_outputs)  # [batch, 1, hidden_size]\n",
        "\n",
        "        # Concatenate attention output and decoder hidden state\n",
        "        concat_output = torch.cat((output, attn_output), dim=2)  # [batch, 1, hidden*2]\n",
        "\n",
        "        # Pass through linear layer and softmax to get output word probabilities\n",
        "        output = F.log_softmax(self.out(concat_output).squeeze(1), dim=1)  # [batch, vocab_size]\n",
        "\n",
        "        # Return output word distribution, updated hidden state, and attention weights\n",
        "        return output, hidden, attn_weights.squeeze(1)\n",
        "\n",
        "class AttnDecoderRNN_extension(nn.Module):\n",
        "    def __init__(self, recipe_vocab_size, embedding_dim, hidden_size, n_layers, dropout_ratio):\n",
        "        super().__init__()\n",
        "        self.recipe_vocab_size = recipe_vocab_size\n",
        "        self.embedding = nn.Embedding(recipe_vocab_size, embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_size, n_layers,\n",
        "                          dropout=dropout_ratio if n_layers > 1 else 0,\n",
        "                          batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size * 2, recipe_vocab_size)\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "        self._init_weights()  # ✅ He 초기화 호출\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # Embedding\n",
        "        nn.init.kaiming_normal_(self.embedding.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
        "\n",
        "        # GRU weights & biases\n",
        "        for name, param in self.gru.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.kaiming_normal_(param, mode='fan_in', nonlinearity='leaky_relu')\n",
        "            elif 'bias' in name:\n",
        "                nn.init.constant_(param, 0)\n",
        "\n",
        "        # Linear layer\n",
        "        nn.init.kaiming_normal_(self.out.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
        "        nn.init.constant_(self.out.bias, 0)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        input = input.long().unsqueeze(1)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        output = self.leaky_relu(output)\n",
        "        output = self.layer_norm(output)\n",
        "\n",
        "        attn_weights = F.softmax(torch.bmm(output, encoder_outputs.transpose(1, 2)), dim=-1)\n",
        "        context = torch.bmm(attn_weights, encoder_outputs)\n",
        "        concat_output = torch.cat((output, context), dim=2)\n",
        "        output = F.log_softmax(self.out(concat_output).squeeze(1), dim=1)\n",
        "        return output, hidden, attn_weights.squeeze(1)\n",
        "\n",
        "class DecoderWithChecklistCopy(nn.Module):\n",
        "    def __init__(self, recipe_vocab_size, embedding_dim, hidden_size, n_layers, dropout_ratio,\n",
        "                 checklist_vocab_size, checklist_vocab, tokenizer_checklist,\n",
        "                 use_checklist=True, use_copy=False):\n",
        "        super().__init__()\n",
        "        self.recipe_vocab_size = recipe_vocab_size\n",
        "        self.embedding = nn.Embedding(recipe_vocab_size, embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_size, n_layers,\n",
        "                          dropout=dropout_ratio if n_layers > 1 else 0,\n",
        "                          batch_first=True)\n",
        "\n",
        "        self.out_proj = nn.Linear(hidden_size, recipe_vocab_size)\n",
        "        self.attn_proj = nn.Linear(hidden_size, hidden_size)  # for both checklist & encoder attention\n",
        "        self.ref_selector = nn.Linear(hidden_size, 3)\n",
        "\n",
        "        self.use_checklist = use_checklist\n",
        "        self.use_copy = use_copy\n",
        "\n",
        "        # Checklist\n",
        "        if self.use_checklist:\n",
        "            self.checklist_embedding = nn.Embedding(checklist_vocab_size, hidden_size)\n",
        "            self.checklist_max_len = checklist_vocab_size\n",
        "            self.checklist_vocab = checklist_vocab\n",
        "            self.tokenizer_checklist = tokenizer_checklist\n",
        "            self.a_prev_list = []\n",
        "\n",
        "        # Copy\n",
        "        if self.use_copy:\n",
        "            self.copy_attn = nn.Linear(hidden_size, hidden_size)\n",
        "            self.copy_score = nn.Linear(hidden_size, 1)  # to compute p_gen\n",
        "\n",
        "    def forward(self, input_token, hidden, encoder_outputs=None,\n",
        "                ingredient_texts=None, a_prev=None, checklist_embeds=None,\n",
        "                checklist_mask=None, encoder_input_ids=None, **kwargs):\n",
        "\n",
        "        input_token = input_token.unsqueeze(1)\n",
        "        embedded = self.dropout(self.embedding(input_token))\n",
        "        gru_out, hidden = self.gru(embedded, hidden)\n",
        "        h_t = gru_out.squeeze(1)  # [batch, hidden_dim]\n",
        "\n",
        "        # === Checklist ===\n",
        "        if self.use_checklist and ingredient_texts is not None and a_prev is not None:\n",
        "            batch_ids = []\n",
        "            for text in ingredient_texts:\n",
        "                tokens = self.tokenizer_checklist(text)\n",
        "                ids = [self.checklist_vocab[token] for token in tokens]\n",
        "                batch_ids.append(torch.tensor(ids, dtype=torch.long))\n",
        "            padded = pad_sequence(batch_ids, batch_first=True, padding_value=0)\n",
        "            checklist_embeds = self.checklist_embedding(padded.to(input_token.device))\n",
        "            checklist_mask = (padded != 0).float().to(input_token.device)\n",
        "\n",
        "            used_mask = a_prev.clamp(0, 1).unsqueeze(2)\n",
        "            unused_mask = (1 - a_prev).clamp(0, 1).unsqueeze(2)\n",
        "            E_new = checklist_embeds * unused_mask\n",
        "            E_used = checklist_embeds * used_mask\n",
        "\n",
        "            h_proj = self.attn_proj(h_t).unsqueeze(2)\n",
        "            scores_new = torch.bmm(E_new, h_proj).squeeze(2)\n",
        "            scores_used = torch.bmm(E_used, h_proj).squeeze(2)\n",
        "\n",
        "            scores_new = scores_new.masked_fill(checklist_mask == 0, -1e9)\n",
        "            scores_used = scores_used.masked_fill(checklist_mask == 0, -1e9)\n",
        "\n",
        "            alpha_new = F.softmax(scores_new, dim=1)\n",
        "            alpha_used = F.softmax(scores_used, dim=1)\n",
        "\n",
        "            c_new = torch.bmm(alpha_new.unsqueeze(1), checklist_embeds).squeeze(1)\n",
        "            c_used = torch.bmm(alpha_used.unsqueeze(1), checklist_embeds).squeeze(1)\n",
        "        else:\n",
        "            alpha_new = alpha_used = None\n",
        "            c_new = c_used = torch.zeros_like(h_t)\n",
        "\n",
        "        # === Copy ===\n",
        "        if self.use_copy and encoder_outputs is not None:\n",
        "            attn_scores = torch.bmm(encoder_outputs, h_t.unsqueeze(2)).squeeze(2)\n",
        "            attn_weights = F.softmax(attn_scores, dim=1)\n",
        "            context_vector = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
        "            # p_gen gate\n",
        "            p_gen_input = torch.cat([h_t, context_vector], dim=1)\n",
        "            p_gen = torch.sigmoid(self.copy_score(p_gen_input))\n",
        "        else:\n",
        "            context_vector = torch.zeros_like(h_t)\n",
        "            p_gen = torch.ones((h_t.size(0), 1), device=h_t.device)\n",
        "\n",
        "        # === Output ===\n",
        "        f_logits = self.ref_selector(h_t)\n",
        "        f = F.softmax(f_logits, dim=-1)\n",
        "        f_gru, f_new, f_used = f.chunk(3, dim=1)\n",
        "\n",
        "        o_t = f_gru * h_t + f_new * c_new + f_used * c_used + context_vector\n",
        "        output = self.out_proj(o_t)\n",
        "\n",
        "        # === Coverage update ===\n",
        "        if self.use_checklist and self.training:\n",
        "            if not hasattr(self, \"a_prev_list\"):\n",
        "                self.a_prev_list = []\n",
        "            self.a_prev_list.append((a_prev.detach(), alpha_new.detach()))\n",
        "\n",
        "        a_t = a_prev + f_new * alpha_new if self.use_checklist else None\n",
        "        return output, hidden, a_t, alpha_new, alpha_used\n",
        "\n",
        "\n",
        "class IngredientAutoEncoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder, vocab_size):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def forward(self, src):\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        input_token = src[:, 0]  # <sos>\n",
        "        outputs = torch.zeros(src.size(0), src.size(1) - 1, self.vocab_size).to(src.device)\n",
        "\n",
        "        for t in range(1, src.size(1)):\n",
        "            output, hidden, _ = self.decoder(input_token, hidden, encoder_outputs)\n",
        "            outputs[:, t-1, :] = output\n",
        "            input_token = src[:, t]  # teacher forcing\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device, use_attention, recipe_vocab):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.use_attention = use_attention\n",
        "        self.recipe_vocab = recipe_vocab\n",
        "\n",
        "    def forward(self, src, ingredient_texts=None, target=None,\n",
        "                teacher_forcing_ratio=0.5, max_len=50):\n",
        "        \n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        batch_size = src.size(0)\n",
        "        vocab_size = self.decoder.recipe_vocab_size\n",
        "\n",
        "        # ✅ Decoder 기능 판단\n",
        "        use_checklist = getattr(self.decoder, 'use_checklist', False)\n",
        "        use_copy = getattr(self.decoder, 'use_copy', False)\n",
        "\n",
        "        # ✅ Checklist 준비\n",
        "        checklist_embed, checklist_mask = None, None\n",
        "        if use_checklist:\n",
        "            checklist_embed, checklist_mask = get_checklist_tensor(\n",
        "                checklist_vocab=self.decoder.checklist_vocab,\n",
        "                checklist_tokenizer=self.decoder.tokenizer_checklist,\n",
        "                ingredient_texts=ingredient_texts,\n",
        "                device=self.device,\n",
        "                embedding_layer=self.decoder.checklist_embedding\n",
        "            )\n",
        "\n",
        "        a_prev = None\n",
        "        if checklist_mask is not None:\n",
        "            checklist_len = checklist_mask.size(1)\n",
        "            a_prev = torch.zeros(batch_size, checklist_len, device=self.device)\n",
        "\n",
        "        # ▶️ Inference Mode\n",
        "        if target is None:\n",
        "            outputs = []\n",
        "            input_token = torch.tensor([self.recipe_vocab['<sos>']] * batch_size).to(self.device).long()\n",
        "\n",
        "            for _ in range(max_len):\n",
        "                if use_checklist or use_copy:\n",
        "                    output, hidden, a_prev, *_ = self.decoder(\n",
        "                        input_token, hidden, encoder_outputs,\n",
        "                        ingredient_texts=ingredient_texts,\n",
        "                        a_prev=a_prev,\n",
        "                        checklist_embeds=checklist_embed,\n",
        "                        checklist_mask=checklist_mask,\n",
        "                        encoder_input_ids=src\n",
        "                    )\n",
        "                elif self.use_attention:\n",
        "                    result = self.decoder(input_token, hidden, encoder_outputs)\n",
        "                    output, hidden = result[:2] if isinstance(result, (tuple, list)) else result\n",
        "                else:\n",
        "                    output, hidden = self.decoder(input_token, hidden)\n",
        "\n",
        "                top1 = output.argmax(1)\n",
        "                outputs.append(top1.unsqueeze(1))\n",
        "                input_token = top1\n",
        "\n",
        "            return torch.cat(outputs, dim=1)\n",
        "\n",
        "        # 🟢 Training Mode\n",
        "        target = target.long()\n",
        "        target_len = target.shape[1]\n",
        "        outputs = torch.zeros(batch_size, target_len, vocab_size).to(self.device)\n",
        "        input_token = target[:, 0].long()\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            if use_checklist or use_copy:\n",
        "                output, hidden, a_prev, *_ = self.decoder(\n",
        "                    input_token, hidden, encoder_outputs,\n",
        "                    ingredient_texts=ingredient_texts,\n",
        "                    a_prev=a_prev,\n",
        "                    checklist_embeds=checklist_embed,\n",
        "                    checklist_mask=checklist_mask,\n",
        "                    encoder_input_ids=src\n",
        "                )\n",
        "            elif self.use_attention:\n",
        "                result = self.decoder(input_token, hidden, encoder_outputs)\n",
        "                output, hidden = result[:2] if isinstance(result, (tuple, list)) else result\n",
        "            else:\n",
        "                output, hidden = self.decoder(input_token, hidden)\n",
        "\n",
        "            outputs[:, t, :] = output\n",
        "            top1 = output.argmax(1)\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            input_token = target[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_encoder_decoder(model_type, config, input_dim, output_dim, embedding_weights=None, freeze_embedding=False, checklist_vocab=None, tokenizer_checklist=None):\n",
        "    use_attention = config[\"USE_ATTENTION\"]\n",
        "    is_extension = \"extension\" in model_type\n",
        "\n",
        "    if is_extension:\n",
        "        encoder = Encoder_GRU_extension(\n",
        "            ingredient_vocab_size=input_dim,\n",
        "            embedding_dim=config[\"EMBED_DIM\"],\n",
        "            hidden_dim=config[\"HIDDEN_DIM\"],\n",
        "            n_layers=config[\"N_LAYERS\"],\n",
        "            dropout_ratio=config[\"DROPOUT\"],\n",
        "            embedding_weights=embedding_weights,\n",
        "            freeze=freeze_embedding\n",
        "        )\n",
        "        if config.get(\"USE_CHECKLIST\", False):\n",
        "            decoder = DecoderWithChecklistCopy(\n",
        "                recipe_vocab_size=output_dim,\n",
        "                embedding_dim=config[\"EMBED_DIM\"],\n",
        "                hidden_size=config[\"HIDDEN_DIM\"],\n",
        "                n_layers=config[\"N_LAYERS\"],\n",
        "                dropout_ratio=config[\"DROPOUT\"],\n",
        "                checklist_vocab_size=len(checklist_vocab),\n",
        "                checklist_vocab=checklist_vocab,\n",
        "                tokenizer_checklist=tokenizer_checklist\n",
        "            )\n",
        "        else:\n",
        "            decoder = AttnDecoderRNN_extension(\n",
        "                recipe_vocab_size=output_dim,\n",
        "                embedding_dim=config[\"EMBED_DIM\"],\n",
        "                hidden_size=config[\"HIDDEN_DIM\"],\n",
        "                n_layers=config[\"N_LAYERS\"],\n",
        "                dropout_ratio=config[\"DROPOUT\"]\n",
        "            )\n",
        "    else:\n",
        "        encoder = Encoder_GRU(\n",
        "            ingredient_vocab_size=input_dim,\n",
        "            embedding_dim=config[\"EMBED_DIM\"],\n",
        "            hidden_dim=config[\"HIDDEN_DIM\"],\n",
        "            n_layers=config[\"N_LAYERS\"],\n",
        "            dropout_ratio=config[\"DROPOUT\"],\n",
        "            embedding_weights=embedding_weights,\n",
        "            freeze=freeze_embedding\n",
        "        )\n",
        "        if use_attention:\n",
        "            decoder = AttnDecoderRNN(\n",
        "                recipe_vocab_size=output_dim,\n",
        "                embedding_dim=config[\"EMBED_DIM\"],\n",
        "                hidden_size=config[\"HIDDEN_DIM\"],\n",
        "                n_layers=config[\"N_LAYERS\"],\n",
        "                dropout_ratio=config[\"DROPOUT\"]\n",
        "            )\n",
        "        else:\n",
        "            decoder = Decoder_GRU(\n",
        "                recipe_vocab_size=output_dim,\n",
        "                embedding_dim=config[\"EMBED_DIM\"],\n",
        "                hidden_dim=config[\"HIDDEN_DIM\"],\n",
        "                n_layers=config[\"N_LAYERS\"],\n",
        "                dropout_ratio=config[\"DROPOUT\"]\n",
        "            )\n",
        "\n",
        "    return encoder, decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_glove_embedding_matrix(glove_path, vocab, glove_dim=200):\n",
        "    print(\"🔎 Loading GloVe vectors...\")\n",
        "    glove_embeddings = {}\n",
        "\n",
        "    with open(glove_path, 'r', encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            vector = torch.tensor(list(map(float, values[1:])))\n",
        "            glove_embeddings[word] = vector\n",
        "\n",
        "    # Initialize matrix with random vectors\n",
        "    embedding_matrix = torch.randn(len(vocab), glove_dim)\n",
        "\n",
        "    for word, idx in vocab.get_stoi().items():\n",
        "        if word in glove_embeddings:\n",
        "            embedding_matrix[idx] = glove_embeddings[word]\n",
        "\n",
        "    print(\"✅ GloVe embedding matrix created.\")\n",
        "    return embedding_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_epoch(model, dataloader, criterion, optimizer=None,\n",
        "               teacher_forcing_ratio=0.5, max_len=50,\n",
        "               USE_PENALTY=False, penalty_lambda=0.2):\n",
        "    total_loss = 0\n",
        "    batch_losses = []\n",
        "\n",
        "    if hasattr(model.decoder, \"a_prev_list\"):\n",
        "        model.decoder.a_prev_list = []\n",
        "\n",
        "    for i, (src_batch, trg_batch, ingredient_texts) in enumerate(dataloader):\n",
        "        src_batch = src_batch.to(DEVICE)\n",
        "        trg_batch = trg_batch.to(DEVICE)\n",
        "\n",
        "        output = model(src_batch, ingredient_texts, trg_batch,\n",
        "                       teacher_forcing_ratio=teacher_forcing_ratio,\n",
        "                       max_len=max_len)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:, :].reshape(-1, output_dim)\n",
        "        trg = trg_batch[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        if USE_PENALTY and hasattr(model.decoder, 'a_prev_list'):\n",
        "            penalty = 0.0\n",
        "            for a_prev, alpha_new in model.decoder.a_prev_list:\n",
        "                penalty += torch.sum(torch.min(a_prev, alpha_new))\n",
        "            penalty = penalty / src_batch.size(0)\n",
        "            loss += penalty_lambda * penalty\n",
        "\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "            optimizer.step()\n",
        "\n",
        "        batch_loss = loss.item() * src_batch.shape[0]\n",
        "        batch_losses.append(batch_loss)\n",
        "        total_loss += batch_loss\n",
        "        print(f\"Batch {i+1}/{len(dataloader)} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = total_loss / len(dataloader.dataset)\n",
        "    return epoch_loss, batch_losses\n",
        "\n",
        "\n",
        "def Train(model, train_loader, val_loader, criterion, optimizer, recipe_vocab,\n",
        "          EPOCHS, BATCH_SIZE, TRAIN_RATIO,\n",
        "          save_model_path, save_history_path, TEACHER_FORCING_RATIO, MAX_LEN, **kwargs):\n",
        "\n",
        "    lr_step = kwargs.get(\"LR_STEP\")\n",
        "    lr_gamma = kwargs.get(\"LR_GAMMA\")\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_step, gamma=lr_gamma) if lr_step and lr_gamma else None\n",
        "\n",
        "    USE_PENALTY = kwargs.get(\"USE_PENALTY\", False)\n",
        "    PENALTY_LAMBDA = kwargs.get(\"PENALTY_LAMBDA\", 0.2)\n",
        "\n",
        "    loss_history = {\"train_epoch\": [], \"train_iter\": [], \"val_epoch\": []}\n",
        "    best_val_loss = float('inf')\n",
        "    train_start_time = time.time()\n",
        "    best_epoch = -1\n",
        "    best_train_loss = None\n",
        "\n",
        "    for ep in range(EPOCHS):\n",
        "        ep_start_time = time.time()\n",
        "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "        print(f\"[Epoch: {ep+1}/{EPOCHS}] current_LR = {current_lr}\")\n",
        "\n",
        "        model.train()\n",
        "        train_epoch_loss, train_batch_loss = loss_epoch(\n",
        "            model, train_loader, criterion, optimizer,\n",
        "            teacher_forcing_ratio=TEACHER_FORCING_RATIO,\n",
        "            max_len=MAX_LEN,\n",
        "            USE_PENALTY=USE_PENALTY,\n",
        "            penalty_lambda=PENALTY_LAMBDA\n",
        "        )\n",
        "        loss_history[\"train_epoch\"].append(train_epoch_loss)\n",
        "        loss_history[\"train_iter\"].append(train_batch_loss)\n",
        "        print(f\"{ep+1} Epoch Train Completed!\")\n",
        "\n",
        "        print(\"Validation Start!\")\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss, _ = loss_epoch(\n",
        "                model, val_loader, criterion, optimizer=None,\n",
        "                teacher_forcing_ratio=0.0,\n",
        "                max_len=MAX_LEN,\n",
        "                USE_PENALTY=USE_PENALTY,\n",
        "                penalty_lambda=PENALTY_LAMBDA\n",
        "            )\n",
        "            loss_history[\"val_epoch\"].append(val_loss)\n",
        "        print(\"Validation Completed!\")\n",
        "\n",
        "        ep_elapsed_time = time.time() - ep_start_time\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"[Epoch {ep+1}/{EPOCHS}] \")\n",
        "        print(f\"Train Loss: {train_epoch_loss:.4f} | Val Loss: {val_loss:.4f} | Time: {ep_elapsed_time:.2f}s\")\n",
        "        bleu, meteor, bert_f1 = compute_metrics(model, val_loader, recipe_vocab, max_len=MAX_LEN)\n",
        "        print(f\"Validation Metrics:\")\n",
        "        print(f\"📊 BLEU: {bleu:.4f} | METEOR: {meteor:.4f} | BERTScore-F1: {bert_f1:.4f}\")  \n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = ep+1\n",
        "            best_train_loss = train_epoch_loss\n",
        "            best_bleu, best_meteor, best_bert_f1 = bleu, meteor, bert_f1\n",
        "            os.makedirs(\"results\", exist_ok=True)\n",
        "            torch.save({\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"epoch\": ep+1,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"train_loss\": train_epoch_loss,\n",
        "            }, save_model_path)\n",
        "            print(\"Best model saved!\")\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "    final_model_path = save_model_path.replace(\".pt\", \"_final.pt\")\n",
        "    torch.save({\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"epoch\": EPOCHS,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"train_loss\": train_epoch_loss,\n",
        "    }, final_model_path)\n",
        "    print(\"Final model saved!\")\n",
        "\n",
        "    train_elapsed_time = time.time() - train_start_time\n",
        "    total_iterations = sum(len(batch_list) for batch_list in loss_history[\"train_iter\"])\n",
        "    torch.save({\n",
        "        \"loss_history\": loss_history,\n",
        "        \"EPOCHS\": EPOCHS,\n",
        "        \"BATCH_SIZE\": BATCH_SIZE,   \n",
        "        \"TRAIN_RATIO\": TRAIN_RATIO,\n",
        "        \"train_elapsed_time\": train_elapsed_time,\n",
        "        \"total_iterations\": total_iterations,\n",
        "        \"LR_STEP\": kwargs.get(\"LR_STEP\"),\n",
        "        \"LR_GAMMA\": kwargs.get(\"LR_GAMMA\")\n",
        "    }, save_history_path)\n",
        "    print(\"Training Completed!\")\n",
        "    print(f\"Total number of iteration : {total_iterations}\")\n",
        "    print(f\"Total Time for Training: {train_elapsed_time:.2f}s\")\n",
        "\n",
        "    print(\"\\n======= Training Summary =======\")\n",
        "    print(f\"Best Model:    Epoch {best_epoch}\")\n",
        "    if best_epoch == EPOCHS:\n",
        "        print(f\"(Best model is the final model)\")\n",
        "    print(f\"Train Loss:    {best_train_loss:.4f}\")\n",
        "    print(f\"Val Loss:      {best_val_loss:.4f}\")\n",
        "    print(f\"BLEU:          {best_bleu:.4f}\")\n",
        "    print(f\"METEOR:        {best_meteor:.4f}\")\n",
        "    print(f\"BERTScore-F1:  {best_bert_f1:.4f}\")\n",
        "    print(\"\\n\")\n",
        "    print(f\"Final Model:   Epoch {EPOCHS}\")\n",
        "    print(f\"Train Loss:    {train_epoch_loss:.4f}\")\n",
        "    print(f\"Val Loss:      {val_loss:.4f}\")\n",
        "    print(f\"BLEU:          {bleu:.4f}\")\n",
        "    print(f\"METEOR:        {meteor:.4f}\")\n",
        "    print(f\"BERTScore-F1:  {bert_f1:.4f}\")\n",
        "    print(f\"Total Time:     {train_elapsed_time:.2f}s\")\n",
        "    print(\"==================================\\n\")\n",
        "    return loss_history\n",
        "\n",
        "def train_autoencoder(model, dataloader, criterion, optimizer, epochs, device, label=\"AutoEncoder\"):\n",
        "    model.train()\n",
        "    for ep in range(epochs):\n",
        "        total_loss = 0\n",
        "        for src_batch, _, _ in tqdm(dataloader, desc=f\"Epoch {ep+1}/{epochs}\"):\n",
        "            src_batch = src_batch.to(device)\n",
        "            output = model(src_batch)\n",
        "            trg = src_batch[:, 1:].contiguous()\n",
        "\n",
        "            output = output.view(-1, output.size(-1))\n",
        "            trg = trg.view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"[Epoch {ep+1}/{epochs}] {label} Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "def Test(model, test_loader, criterion, recipe_vocab, MAX_LEN,\n",
        "         USE_PENALTY=False, penalty_lambda=0.2):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loss, _ = loss_epoch(\n",
        "            model,\n",
        "            test_loader,\n",
        "            criterion,\n",
        "            optimizer=None,\n",
        "            teacher_forcing_ratio=0.0,\n",
        "            max_len=MAX_LEN,\n",
        "            USE_PENALTY=USE_PENALTY,\n",
        "            penalty_lambda=penalty_lambda\n",
        "        )\n",
        "\n",
        "    # 평가 지표 계산\n",
        "    bleu_score, meteor_avg, bertscore_f1 = compute_metrics(model, test_loader, recipe_vocab)\n",
        "\n",
        "    print(f\"Test Loss      : {test_loss:.4f}\")\n",
        "    print(f\"BLEU-4 Score   : {bleu_score:.4f}\")\n",
        "    print(f\"METEOR Score   : {meteor_avg:.4f}\")\n",
        "    print(f\"BERTScore (F1) : {bertscore_f1:.4f}\")\n",
        "\n",
        "    return test_loss, bleu_score, meteor_avg, bertscore_f1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(model, dataloader, recipe_vocab, max_len=50):\n",
        "    \"\"\"\n",
        "    테스트셋 전체에서 BLEU, METEOR, BERTScore 계산\n",
        "\n",
        "    Returns:\n",
        "        bleu_score (float), meteor_avg (float), bertscore_f1 (float)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    smoothie = SmoothingFunction().method4\n",
        "\n",
        "    ref_list = []\n",
        "    hyp_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src_batch, trg_batch, ingredient_texts in tqdm(dataloader, desc=\"Evaluating Metrics\"):\n",
        "\n",
        "            src_batch = src_batch.to(DEVICE)\n",
        "            trg_batch = trg_batch.to(DEVICE)\n",
        "\n",
        "            generated = model(src_batch, target=None, teacher_forcing_ratio=0.0, max_len=max_len)\n",
        "\n",
        "            for i in range(src_batch.size(0)):\n",
        "                pred_tokens = generated[i].tolist()\n",
        "                trg_tokens = trg_batch[i].tolist()\n",
        "\n",
        "                # <eos> 기준으로 자르기\n",
        "                if recipe_vocab['<eos>'] in pred_tokens:\n",
        "                    pred_tokens = pred_tokens[:pred_tokens.index(recipe_vocab['<eos>'])]\n",
        "                if recipe_vocab['<eos>'] in trg_tokens:\n",
        "                    trg_tokens = trg_tokens[:trg_tokens.index(recipe_vocab['<eos>'])]\n",
        "\n",
        "                pred_words = [recipe_vocab.get_itos()[idx] for idx in pred_tokens]\n",
        "                trg_words = [recipe_vocab.get_itos()[idx] for idx in trg_tokens]\n",
        "\n",
        "                ref_list.append(trg_words)\n",
        "                hyp_list.append(pred_words)\n",
        "\n",
        "    # BLEU-4\n",
        "    bleu_score = corpus_bleu([[ref] for ref in ref_list], hyp_list, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie) \n",
        "\n",
        "    # METEOR\n",
        "    meteor_scores = [meteor_score([ref], hyp) for ref, hyp in zip(ref_list, hyp_list)]\n",
        "    meteor_avg = sum(meteor_scores) / len(meteor_scores)\n",
        "\n",
        "    # BERTScore\n",
        "    refs = [\" \".join(ref) for ref in ref_list]\n",
        "    hyps = [\" \".join(hyp) for hyp in hyp_list]\n",
        "    _, _, f1 = bert_score_fn(hyps, refs, lang='en', verbose=False)\n",
        "    bertscore_f1 = f1.mean().item()\n",
        "\n",
        "    return bleu_score, meteor_avg, bertscore_f1\n",
        "\n",
        "\n",
        "def plot_loss_epoch(name, loss_history):\n",
        "    plt.figure(figsize=(6, 3))\n",
        "\n",
        "    train_loss = loss_history[\"train_epoch\"]\n",
        "    val_loss = loss_history[\"val_epoch\"]\n",
        "\n",
        "    plt.plot(range(1, len(train_loss) + 1), train_loss, label=\"Train Loss\", color=\"blue\")\n",
        "    plt.plot(range(1, len(val_loss) + 1), val_loss, label=\"Validation Loss\", color=\"red\")\n",
        "\n",
        "    plt.xlabel(\"Epoch\", fontsize=10)\n",
        "    plt.ylabel(\"Loss\", fontsize=10)\n",
        "    plt.title(f\"Loss per Epoch: {name}\", fontsize=12)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main(model_type: str, config: dict,\n",
        "         train_df, dev_df, test_df,\n",
        "         glove_path: str = \"./glove.6B.200d.txt\"):\n",
        "\n",
        "    # 1. Tokenize & Vocab\n",
        "    ingredient_token_lists, recipe_token_lists, ingredient_vocab, recipe_vocab, tokenizer_ingredient, tokenizer_recipe, checklist_vocab, tokenizer_checklist = load_or_tokenize_data(model_type, train_df, config)\n",
        "\n",
        "    # 2. GloVe 임베딩 적용 여부\n",
        "    embedding_weights = None\n",
        "    freeze_embedding = config.get(\"freeze_embedding\", False)\n",
        "    if config.get(\"use_glove\", False):\n",
        "        print(f\"🔎 Using GloVe embeddings for {model_type}\")\n",
        "        ingredient_embedding_matrix = build_glove_embedding_matrix(\n",
        "            glove_path, ingredient_vocab, glove_dim=config[\"EMBED_DIM\"]\n",
        "        )\n",
        "        embedding_weights = ingredient_embedding_matrix\n",
        "\n",
        "    # 3. Dataset & DataLoader\n",
        "    BATCH_SIZE = config.get(\"BATCH_SIZE\", 64)\n",
        "    train_dataset = CustomDataset(train_df, ingredient_vocab, recipe_vocab, tokenizer_ingredient, tokenizer_recipe)\n",
        "    dev_dataset = CustomDataset(dev_df, ingredient_vocab, recipe_vocab, tokenizer_ingredient, tokenizer_recipe)\n",
        "    test_dataset = CustomDataset(test_df, ingredient_vocab, recipe_vocab, tokenizer_ingredient, tokenizer_recipe)\n",
        "    collate = partial(collate_fn, ingredient_vocab=ingredient_vocab, recipe_vocab=recipe_vocab, device=DEVICE)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)\n",
        "    dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
        "\n",
        "    # ✅ 4. AutoEncoder 사전학습 여부에 따라 사전학습 실행 (ingredient_vocab 사용)\n",
        "    if config.get(\"USE_AUTOENCODER\", False):\n",
        "        encoder_path = f\"results/{model_type}_encoder_pretrained.pt\"\n",
        "        if os.path.exists(encoder_path):\n",
        "            print(f\"AutoEncoder encoder weights already exist for {model_type}\")\n",
        "            pass\n",
        "        else:\n",
        "            print(f\"📦 AutoEncoder pretraining required for {model_type}\")\n",
        "            MakingAutoEncoder(\n",
        "                model_type=model_type,\n",
        "                config=config,\n",
        "                input_vocab=ingredient_vocab,\n",
        "                train_loader=train_loader_autoencoder,\n",
        "                embedding_weights=embedding_weights,\n",
        "                freeze_embedding=freeze_embedding,\n",
        "                device=DEVICE\n",
        "            )\n",
        "\n",
        "    # 5. Model 구성 (recipe_vocab 사용)\n",
        "    INPUT_DIM = len(ingredient_vocab)\n",
        "    OUTPUT_DIM = len(recipe_vocab)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=recipe_vocab['<pad>'])\n",
        "\n",
        "    encoder, decoder = get_encoder_decoder(\n",
        "        model_type=model_type,\n",
        "        config=config,\n",
        "        input_dim=INPUT_DIM,\n",
        "        output_dim=OUTPUT_DIM,\n",
        "        embedding_weights=embedding_weights,\n",
        "        freeze_embedding=freeze_embedding,\n",
        "        checklist_vocab=checklist_vocab,\n",
        "        tokenizer_checklist=tokenizer_checklist\n",
        "    )\n",
        "\n",
        "    # ✅ 6. 사전학습된 AutoEncoder 인코더 가중치만 로드\n",
        "    if config.get(\"USE_AUTOENCODER\", False):\n",
        "        encoder_path = f\"results/{model_type}_encoder_pretrained.pt\"\n",
        "        print(f\"🔁 Loading pretrained AutoEncoder encoder weights for {model_type}\")\n",
        "        encoder.load_state_dict(torch.load(encoder_path, map_location=DEVICE))\n",
        "\n",
        "    model = Seq2Seq(encoder, decoder, DEVICE, use_attention=config[\"USE_ATTENTION\"], recipe_vocab=recipe_vocab).to(DEVICE)\n",
        "\n",
        "    # 7. 학습 또는 로드\n",
        "    save_model_path = f\"results/{model_type}.pt\"\n",
        "    save_history_path = f\"results/{model_type}_history.pt\"\n",
        "\n",
        "    if config[\"new_model_train\"]:\n",
        "        print(f\"Training model: {model_type}\")\n",
        "        print(model)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=config[\"LR\"])\n",
        "        loss_history = Train(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=dev_loader,\n",
        "            criterion=criterion,\n",
        "            optimizer=optimizer,\n",
        "            recipe_vocab=recipe_vocab,\n",
        "            EPOCHS=config[\"EPOCHS\"],\n",
        "            BATCH_SIZE=BATCH_SIZE,\n",
        "            TRAIN_RATIO=1.0,\n",
        "            save_model_path=save_model_path,\n",
        "            save_history_path=save_history_path,\n",
        "            TEACHER_FORCING_RATIO=config[\"TEACHER_FORCING_RATIO\"],\n",
        "            MAX_LEN=config[\"MAX_LEN\"],\n",
        "            LR_STEP=config.get(\"LR_STEP\"),\n",
        "            LR_GAMMA=config.get(\"LR_GAMMA\"),\n",
        "            USE_PENALTY=config.get(\"USE_PENALTY\", False),\n",
        "            PENALTY_LAMBDA=config.get(\"PENALTY_LAMBDA\", 0.2)\n",
        "        )\n",
        "        return model, encoder, decoder, loss_history, save_model_path, save_history_path, test_loader, recipe_vocab, ingredient_vocab\n",
        "\n",
        "    else:\n",
        "        return model, encoder, decoder, None, save_model_path, save_history_path, test_loader, recipe_vocab, ingredient_vocab\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 하이퍼파라미터 저장\n",
        "experiment_configs = {\n",
        "    \"baseline1\": {\n",
        "        \"new_model_train\" : False,\n",
        "        \"EMBED_DIM\": 128,\n",
        "        \"HIDDEN_DIM\": 256,\n",
        "        \"DROPOUT\": 0.5,\n",
        "        \"N_LAYERS\": 1,\n",
        "        \"LR\": 0.001,\n",
        "        \"EPOCHS\": 5,\n",
        "        \"USE_ATTENTION\": False,\n",
        "        \"TEACHER_FORCING_RATIO\": 0.5,\n",
        "        \"MAX_LEN\": 50,\n",
        "        \"BATCH_SIZE\": 64\n",
        "    },\n",
        "    \"baseline2\": {\n",
        "        \"new_model_train\" : False,\n",
        "        \"EMBED_DIM\": 128,\n",
        "        \"HIDDEN_DIM\": 256,\n",
        "        \"DROPOUT\": 0.5,\n",
        "        \"N_LAYERS\": 1,\n",
        "        \"LR\": 0.001,\n",
        "        \"EPOCHS\": 5,\n",
        "        \"USE_ATTENTION\": True,\n",
        "        \"TEACHER_FORCING_RATIO\": 0.5,\n",
        "        \"MAX_LEN\": 50,\n",
        "        \"BATCH_SIZE\": 64\n",
        "    },\n",
        "    \"mild_extension1\": {\n",
        "        \"new_model_train\" : False,\n",
        "        \"EMBED_DIM\": 200,\n",
        "        \"HIDDEN_DIM\": 512,\n",
        "        \"DROPOUT\": 0.5,\n",
        "        \"N_LAYERS\": 2,\n",
        "        \"LR\": 0.001,\n",
        "        \"EPOCHS\": 5,\n",
        "        \"USE_ATTENTION\": True,\n",
        "        \"TEACHER_FORCING_RATIO\": 0.7,\n",
        "        \"use_glove\": True,\n",
        "        \"freeze_embedding\": False,\n",
        "        \"MAX_LEN\": 100,\n",
        "        \"LR_STEP\": 1,         \n",
        "        \"LR_GAMMA\": 0.7,\n",
        "        \"BATCH_SIZE\": 64,\n",
        "        \"USE_AUTOENCODER\": True,\n",
        "        \"AUTOENCODER_EPOCHS\": 5,\n",
        "    },\n",
        "    \"mild_extension2\": {\n",
        "        \"new_model_train\" : False,\n",
        "        \"EMBED_DIM\": 200,  # GloVe 6B 200D와 일치\n",
        "        \"HIDDEN_DIM\": 512,\n",
        "        \"DROPOUT\": 0.5,\n",
        "        \"N_LAYERS\": 2,\n",
        "        \"LR\": 0.001,\n",
        "        \"EPOCHS\": 5,\n",
        "        \"USE_ATTENTION\": True,\n",
        "        \"TEACHER_FORCING_RATIO\": 0.5,\n",
        "        \"MAX_LEN\": 100,\n",
        "        \"LR_STEP\": 1,         \n",
        "        \"LR_GAMMA\": 0.7,\n",
        "        \"use_glove\": True,   \n",
        "        \"freeze_embedding\": False,\n",
        "        \"BATCH_SIZE\": 64,\n",
        "        \"USE_AUTOENCODER\": True,\n",
        "        \"AUTOENCODER_EPOCHS\": 10,\n",
        "    },\n",
        "    \"spicy_extension1\": {\n",
        "    \"new_model_train\": False,\n",
        "    \"EMBED_DIM\": 256,\n",
        "    \"HIDDEN_DIM\": 512,\n",
        "    \"DROPOUT\": 0.5,\n",
        "    \"N_LAYERS\": 3,\n",
        "    \"LR\": 0.001,\n",
        "    \"EPOCHS\": 2,\n",
        "    \"USE_ATTENTION\": True,\n",
        "    \"TEACHER_FORCING_RATIO\": 0.5,\n",
        "    \"MAX_LEN\": 100,\n",
        "    \"LR_STEP\": 1,\n",
        "    \"LR_GAMMA\": 0.7,\n",
        "    \"BATCH_SIZE\": 64,\n",
        "    \"USE_CHECKLIST\": True,       # checklist 메커니즘\n",
        "    \"USE_PENALTY\": True,         # coverage loss 사용\n",
        "    \"PENALTY_LAMBDA\": 0.2        # coverage loss 가중치\n",
        "},\n",
        "\"spicy_extension2\": {\n",
        "    \"new_model_train\": False,\n",
        "    \"EMBED_DIM\": 256,\n",
        "    \"HIDDEN_DIM\": 512,\n",
        "    \"DROPOUT\": 0.5,\n",
        "    \"N_LAYERS\": 2,\n",
        "    \"LR\": 0.001,\n",
        "    \"EPOCHS\": 2,\n",
        "    \"USE_ATTENTION\": True,\n",
        "    \"TEACHER_FORCING_RATIO\": 0.5,\n",
        "    \"MAX_LEN\": 100,\n",
        "    \"LR_STEP\": 1,\n",
        "    \"LR_GAMMA\": 0.7,\n",
        "    \"BATCH_SIZE\": 64,\n",
        "    \"USE_CHECKLIST\": True,   # checklist attention\n",
        "    \"USE_COPY\": True         # copy mechanism\n",
        "}\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def MakingAutoEncoder(model_type, config, input_vocab, train_loader,\n",
        "                      embedding_weights=None, freeze_embedding=False,device=DEVICE):\n",
        "\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "    INPUT_DIM = len(input_vocab)\n",
        "\n",
        "    # 1. Encoder / Decoder 준비 (입력 = 출력)\n",
        "    encoder, decoder = get_encoder_decoder(\n",
        "        model_type=model_type,\n",
        "        config=config,\n",
        "        input_dim=INPUT_DIM,\n",
        "        output_dim=INPUT_DIM,\n",
        "        embedding_weights=embedding_weights,\n",
        "        freeze_embedding=freeze_embedding\n",
        "    )\n",
        "\n",
        "    # 2. AutoEncoder 구성\n",
        "    autoencoder = IngredientAutoEncoder(encoder, decoder, vocab_size=INPUT_DIM).to(device)\n",
        "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=config.get(\"LR\", 0.001))\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=input_vocab['<pad>'])\n",
        "\n",
        "    # 3. 학습\n",
        "    print(f\"🚀 Pretraining AutoEncoder for {model_type}\")\n",
        "    train_autoencoder(autoencoder, train_loader, criterion, optimizer,\n",
        "                      epochs=config.get(\"AUTOENCODER_EPOCHS\", 5), device=device)\n",
        "\n",
        "    # 4. 저장\n",
        "    torch.save(encoder.state_dict(), f\"results/{model_type}_encoder_pretrained.pt\")\n",
        "    torch.save(decoder.state_dict(), f\"results/{model_type}_decoder_pretrained.pt\")\n",
        "    print(f\"✅ Saved pretrained encoder/decoder for {model_type}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_trained_model(model_type: str, config: dict, device=DEVICE):\n",
        "    \"\"\"\n",
        "    best model과 final model을 모두 불러옵니다.\n",
        "\n",
        "    Args:\n",
        "        model_type (str): 저장 파일 이름 결정에 사용되는 모델 이름\n",
        "        config (dict): 설정 정보 (input_dim, output_dim, attention 등 포함)\n",
        "        device (torch.device): 모델을 로드할 디바이스\n",
        "\n",
        "    Returns:\n",
        "        best_model (nn.Module)\n",
        "        final_model (nn.Module)\n",
        "        loss_history (dict)\n",
        "        best_checkpoint (dict)\n",
        "        final_checkpoint (dict)\n",
        "    \"\"\"\n",
        "    # 경로 구성\n",
        "    best_model_path = f\"results/{model_type}.pt\"\n",
        "    final_model_path = best_model_path.replace(\".pt\", \"_final.pt\")\n",
        "    save_history_path = f\"results/{model_type}_history.pt\"\n",
        "\n",
        "    INPUT_DIM = config[\"input_dim\"]\n",
        "    OUTPUT_DIM = config[\"output_dim\"]\n",
        "    embedding_weights = config.get(\"embedding_weights\", None)\n",
        "    freeze_embedding = config.get(\"freeze_embedding\", False)\n",
        "    checklist_vocab = config.get(\"checklist_vocab\", None)\n",
        "    tokenizer_checklist = config.get(\"tokenizer_checklist\", None)\n",
        "    recipe_vocab = config[\"recipe_vocab\"]\n",
        "\n",
        "    def build_model():\n",
        "        encoder, decoder = get_encoder_decoder(\n",
        "            model_type=model_type,\n",
        "            config=config,\n",
        "            input_dim=INPUT_DIM,\n",
        "            output_dim=OUTPUT_DIM,\n",
        "            embedding_weights=embedding_weights,\n",
        "            freeze_embedding=freeze_embedding,\n",
        "            checklist_vocab=checklist_vocab,\n",
        "            tokenizer_checklist=tokenizer_checklist\n",
        "        )\n",
        "        return Seq2Seq(encoder, decoder, device, use_attention=config[\"USE_ATTENTION\"], recipe_vocab=recipe_vocab).to(device)\n",
        "\n",
        "    # ✅ Best model\n",
        "    best_model = build_model()\n",
        "    best_checkpoint = torch.load(best_model_path, map_location=device)\n",
        "    best_model.load_state_dict(best_checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    # ✅ Final model\n",
        "    final_model = build_model()\n",
        "    final_checkpoint = torch.load(final_model_path, map_location=device)\n",
        "    final_model.load_state_dict(final_checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    # ✅ 학습 이력\n",
        "    history = torch.load(save_history_path, map_location=device)\n",
        "    loss_history = history[\"loss_history\"]\n",
        "\n",
        "    print(f\"📥 Loaded BEST model (Epoch {best_checkpoint['epoch']})\")\n",
        "    print(f\"📥 Loaded FINAL model (Epoch {final_checkpoint['epoch']})\")\n",
        "\n",
        "    return best_model, final_model, loss_history, best_checkpoint, final_checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_and_test_model(model_type: str,\n",
        "                             config: dict,\n",
        "                             ingredient_vocab,\n",
        "                             recipe_vocab,\n",
        "                             test_loader,\n",
        "                             criterion,\n",
        "                             tokenizer_checklist=None,\n",
        "                             checklist_vocab=None,\n",
        "                             embedding_weights=None,\n",
        "                             device=DEVICE):\n",
        "    \"\"\"\n",
        "    저장된 best/final 모델을 불러와 summary, 그래프, 테스트 수행\n",
        "    \"\"\"\n",
        "\n",
        "    # config에 input/output dim 등 필요한 정보 추가 (원본 손상 X)\n",
        "    config = config.copy()\n",
        "    config[\"input_dim\"] = len(ingredient_vocab)\n",
        "    config[\"output_dim\"] = len(recipe_vocab)\n",
        "    config[\"recipe_vocab\"] = recipe_vocab\n",
        "    config[\"embedding_weights\"] = embedding_weights\n",
        "    config[\"checklist_vocab\"] = checklist_vocab\n",
        "    config[\"tokenizer_checklist\"] = tokenizer_checklist\n",
        "\n",
        "    # ✅ 모델 두 개 모두 불러오기\n",
        "    best_model, final_model, loss_history, best_ckpt, final_ckpt = load_trained_model(\n",
        "        model_type, config, device\n",
        "    )\n",
        "\n",
        "    # ✅ 손실 곡선 시각화\n",
        "    plot_loss_epoch(model_type, loss_history)\n",
        "\n",
        "    # ✅ BEST 모델 평가\n",
        "    print(f\"\\n========== BEST MODEL SUMMARY ({model_type}) ==========\")\n",
        "    print(f\"Epoch:         {best_ckpt['epoch']}\")\n",
        "    print(f\"Train Loss:    {best_ckpt['train_loss']:.4f}\")\n",
        "    print(f\"Val Loss:      {best_ckpt['val_loss']:.4f}\")\n",
        "    print(f\"Parameters:    {sum(p.numel() for p in best_model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    print(\"\\nRunning test on BEST model...\")\n",
        "    best_test_loss, best_bleu, best_meteor, best_bert = Test(\n",
        "        best_model, test_loader, criterion, recipe_vocab, MAX_LEN=config[\"MAX_LEN\"]\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- BEST MODEL TEST RESULTS ---\")\n",
        "    print(f\"Test Loss    : {best_test_loss:.4f}\")\n",
        "    print(f\"BLEU-4 Score : {best_bleu:.4f}\")\n",
        "    print(f\"METEOR Score : {best_meteor:.4f}\")\n",
        "    print(f\"BERTScore-F1 : {best_bert:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # ✅ FINAL 모델 평가\n",
        "    print(f\"\\n========== FINAL MODEL SUMMARY ({model_type}) ==========\")\n",
        "    print(f\"Epoch:         {final_ckpt['epoch']}\")\n",
        "    print(f\"Train Loss:    {final_ckpt['train_loss']:.4f}\")\n",
        "    print(f\"Val Loss:      {final_ckpt['val_loss']:.4f}\")\n",
        "    print(f\"Parameters:    {sum(p.numel() for p in final_model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    print(\"\\nRunning test on FINAL model...\")\n",
        "    final_test_loss, final_bleu, final_meteor, final_bert = Test(\n",
        "        final_model, test_loader, criterion, recipe_vocab, MAX_LEN=config[\"MAX_LEN\"]\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- FINAL MODEL TEST RESULTS ---\")\n",
        "    print(f\"Test Loss    : {final_test_loss:.4f}\")\n",
        "    print(f\"BLEU-4 Score : {final_bleu:.4f}\")\n",
        "    print(f\"METEOR Score : {final_meteor:.4f}\")\n",
        "    print(f\"BERTScore-F1 : {final_bert:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    return {\n",
        "        \"best\": {\n",
        "            \"test_loss\": best_test_loss,\n",
        "            \"BLEU\": best_bleu,\n",
        "            \"METEOR\": best_meteor,\n",
        "            \"BERTScore\": best_bert,\n",
        "        },\n",
        "        \"final\": {\n",
        "            \"test_loss\": final_test_loss,\n",
        "            \"BLEU\": final_bleu,\n",
        "            \"METEOR\": final_meteor,\n",
        "            \"BERTScore\": final_bert,\n",
        "        }\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================== 🔧 Training & Evaluating: baseline1 ==================\n",
            "✅ Loaded ingredient token cache.\n",
            "✅ Loaded recipe token cache.\n",
            "⚠️ Checkpoint for baseline1 not found. Skipping evaluation.\n",
            "\n",
            "\n",
            "================== 🔧 Training & Evaluating: baseline2 ==================\n",
            "✅ Loaded ingredient token cache.\n",
            "✅ Loaded recipe token cache.\n",
            "⚠️ Checkpoint for baseline2 not found. Skipping evaluation.\n",
            "\n",
            "\n",
            "================== 🔧 Training & Evaluating: mild_extension1 ==================\n",
            "✅ Loaded ingredient token cache.\n",
            "✅ Loaded recipe token cache.\n",
            "⚠️ Checkpoint for mild_extension1 not found. Skipping evaluation.\n",
            "\n",
            "\n",
            "================== 🔧 Training & Evaluating: mild_extension2 ==================\n",
            "✅ Loaded ingredient token cache.\n",
            "✅ Loaded recipe token cache.\n",
            "🔎 Using GloVe embeddings for mild_extension2\n",
            "🔎 Loading GloVe vectors...\n",
            "✅ GloVe embedding matrix created.\n",
            "📦 AutoEncoder pretraining required for mild_extension2\n",
            "🚀 Pretraining AutoEncoder for mild_extension2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10:   0%|          | 0/2546 [00:00<?, ?it/s]/Users/jeeeunkim/Desktop/FIT5217_NLP/NLP/.venv/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "Epoch 1/10:  15%|█▍        | 374/2546 [16:15<1:51:35,  3.08s/it]"
          ]
        }
      ],
      "source": [
        "for model_type, base_config in experiment_configs.items():\n",
        "    print(f\"\\n================== 🔧 Training & Evaluating: {model_type} ==================\")\n",
        "\n",
        "    # ✅ config 복사 및 설정\n",
        "    config = base_config.copy()\n",
        "\n",
        "    # main 실행\n",
        "    model, encoder, decoder, _, save_model_path, save_history_path, test_loader, recipe_vocab, ingredient_vocab = main(\n",
        "        model_type, config, train_df, dev_df, test_df\n",
        "    )\n",
        "\n",
        "    # ✅ config에 필요한 정보 추가\n",
        "    config[\"input_dim\"] = len(ingredient_vocab)\n",
        "    config[\"output_dim\"] = len(recipe_vocab)\n",
        "    config[\"recipe_vocab\"] = recipe_vocab\n",
        "\n",
        "    # optional: GloVe embedding이 사용되었다면 전달\n",
        "    config[\"embedding_weights\"] = config.get(\"embedding_weights\", None)\n",
        "\n",
        "    # optional: checklist/copy 관련 키 추가\n",
        "    config[\"checklist_vocab\"] = config.get(\"checklist_vocab\", None)\n",
        "    config[\"tokenizer_checklist\"] = config.get(\"tokenizer_checklist\", None)\n",
        "\n",
        "    # 🔸 파일 존재 여부 확인\n",
        "    if not os.path.exists(save_model_path) or not os.path.exists(save_history_path):\n",
        "        print(f\"⚠️ Checkpoint for {model_type} not found. Skipping evaluation.\\n\")\n",
        "        continue\n",
        "\n",
        "    # 손실 함수 정의\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=recipe_vocab['<pad>'])\n",
        "\n",
        "    # 평가 실행\n",
        "    summarize_and_test_model(\n",
        "        model_type=model_type,\n",
        "        config=config,\n",
        "        ingredient_vocab=ingredient_vocab,\n",
        "        recipe_vocab=recipe_vocab,\n",
        "        test_loader=test_loader,\n",
        "        criterion=criterion,\n",
        "        checklist_vocab=config.get(\"checklist_vocab\"),\n",
        "        tokenizer_checklist=config.get(\"tokenizer_checklist\"),\n",
        "        embedding_weights=config.get(\"embedding_weights\")\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plot for all model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def plot_loss_iter_multi(model_list, experiment_configs, device=DEVICE):\n",
        "#     plt.figure(figsize=(12, 6))\n",
        "    \n",
        "#     for model_type in model_list:\n",
        "#         config = experiment_configs[model_type].copy()\n",
        "#         ingredient_token_lists, recipe_token_lists, ingredient_vocab, recipe_vocab, tokenizer_ing, tokenizer_rec, checklist_vocab, tokenizer_checklist = \\\n",
        "#             load_or_tokenize_data(model_type, pd.DataFrame(), config)\n",
        "    \n",
        "#         config[\"input_dim\"] = len(ingredient_vocab)\n",
        "#         config[\"output_dim\"] = len(recipe_vocab)\n",
        "#         config[\"recipe_vocab\"] = recipe_vocab\n",
        "\n",
        "#         # 📥 모델, 기록 불러오기\n",
        "#         _, _, loss_history, _, _ = load_trained_model(model_type, config, device)\n",
        "#         train_iter_loss = loss_history[\"train_iter\"]  # 리스트 of 리스트\n",
        "\n",
        "#         # 리스트 평탄화 (iteration 기준)\n",
        "#         iter_losses = [loss for epoch_losses in train_iter_loss for loss in epoch_losses]\n",
        "#         plt.plot(iter_losses, label=model_type)\n",
        "\n",
        "#     plt.title(\"Training Loss per Iteration\")\n",
        "#     plt.xlabel(\"Iteration\")\n",
        "#     plt.ylabel(\"Loss\")\n",
        "#     plt.legend()\n",
        "#     plt.grid(True)\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def smooth(values, window=100):\n",
        "    return np.convolve(values, np.ones(window)/window, mode='valid')\n",
        "\n",
        "def plot_loss_iter_multi(model_list, experiment_configs, device=DEVICE):\n",
        "    plt.style.use(\"seaborn-v0_8\")\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    for model_type in model_list:\n",
        "        config = experiment_configs[model_type].copy()\n",
        "        ingredient_token_lists, recipe_token_lists, ingredient_vocab, recipe_vocab, tokenizer_ing, tokenizer_rec, checklist_vocab, tokenizer_checklist = \\\n",
        "            load_or_tokenize_data(model_type, pd.DataFrame(), config)\n",
        "\n",
        "        config[\"input_dim\"] = len(ingredient_vocab)\n",
        "        config[\"output_dim\"] = len(recipe_vocab)\n",
        "        config[\"recipe_vocab\"] = recipe_vocab\n",
        "\n",
        "        _, _, loss_history, _, _ = load_trained_model(model_type, config, device)\n",
        "        train_iter_loss = loss_history[\"train_iter\"]\n",
        "\n",
        "        iter_losses = [loss for epoch in train_iter_loss for loss in epoch]\n",
        "        smoothed = smooth(iter_losses, window=100)\n",
        "\n",
        "        plt.plot(smoothed, label=model_type, linewidth=2)\n",
        "\n",
        "    plt.title(\"Smoothed Training Loss per Iteration\", fontsize=14)\n",
        "    plt.xlabel(\"Iteration\", fontsize=12)\n",
        "    plt.ylabel(\"Loss\", fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded ingredient token cache.\n",
            "✅ Loaded recipe token cache.\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'results/baseline1.pt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_loss_iter_multi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbaseline1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbaseline2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_configs\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[21], line 48\u001b[0m, in \u001b[0;36mplot_loss_iter_multi\u001b[0;34m(model_list, experiment_configs, device)\u001b[0m\n\u001b[1;32m     45\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(recipe_vocab)\n\u001b[1;32m     46\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecipe_vocab\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m recipe_vocab\n\u001b[0;32m---> 48\u001b[0m _, _, loss_history, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mload_trained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m train_iter_loss \u001b[38;5;241m=\u001b[39m loss_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_iter\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     51\u001b[0m iter_losses \u001b[38;5;241m=\u001b[39m [loss \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m train_iter_loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m epoch]\n",
            "Cell \u001b[0;32mIn[18], line 45\u001b[0m, in \u001b[0;36mload_trained_model\u001b[0;34m(model_type, config, device)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# ✅ Best model\u001b[39;00m\n\u001b[1;32m     44\u001b[0m best_model \u001b[38;5;241m=\u001b[39m build_model()\n\u001b[0;32m---> 45\u001b[0m best_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m best_model\u001b[38;5;241m.\u001b[39mload_state_dict(best_checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# ✅ Final model\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/FIT5217_NLP/NLP/.venv/lib/python3.10/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
            "File \u001b[0;32m~/Desktop/FIT5217_NLP/NLP/.venv/lib/python3.10/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
            "File \u001b[0;32m~/Desktop/FIT5217_NLP/NLP/.venv/lib/python3.10/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/baseline1.pt'"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_loss_iter_multi(\n",
        "    model_list=[\"baseline1\", \"baseline2\"],\n",
        "    experiment_configs=experiment_configs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_recipes(sample_raw, model_names, experiment_configs):\n",
        "    \"\"\"\n",
        "    여러 모델을 불러와 주어진 input에 대해 recipe를 생성하는 함수\n",
        "    \"\"\"\n",
        "    # 🧾 입력을 리스트로 정리\n",
        "    if isinstance(sample_raw, str):\n",
        "        ingredient_list = [i.strip() for i in sample_raw.split(',')]\n",
        "    elif isinstance(sample_raw, list):\n",
        "        ingredient_list = sample_raw\n",
        "    else:\n",
        "        raise ValueError(\"Input must be a comma-separated string or a list.\")\n",
        "\n",
        "    print(f\"🧾 Ingredients: {ingredient_list}\")\n",
        "\n",
        "    results = {}\n",
        "    for model_type in model_names:\n",
        "        print(f\"\\n🔍 Generating with model: {model_type}\")\n",
        "        config = experiment_configs[model_type].copy()\n",
        "\n",
        "        # ✅ vocab, tokenizer 불러오기\n",
        "        ingredient_token_lists, recipe_token_lists, ingredient_vocab, recipe_vocab, tokenizer_ing, tokenizer_rec, checklist_vocab, tokenizer_checklist = \\\n",
        "            load_or_tokenize_data(model_type, pd.DataFrame(), config)\n",
        "\n",
        "        config[\"input_dim\"] = len(ingredient_vocab)\n",
        "        config[\"output_dim\"] = len(recipe_vocab)\n",
        "        config[\"recipe_vocab\"] = recipe_vocab\n",
        "\n",
        "        # ✅ 모델 불러오기\n",
        "        _, final_model, _, _, _ = load_trained_model(model_type, config, device=DEVICE)\n",
        "\n",
        "        # ✅ ingredient_list → 문자열로 변환 후 토크나이징\n",
        "        ingredient_str = str(ingredient_list)\n",
        "        tokenized = tokenizer_ing(ingredient_str)\n",
        "        input_ids = torch.tensor([ingredient_vocab[token] for token in tokenized], dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        # ✅ 예측\n",
        "        final_model.eval()\n",
        "        with torch.no_grad():\n",
        "            prediction_ids = final_model(input_ids, ingredient_texts=[ingredient_list], target=None)\n",
        "\n",
        "        # ✅ 디코딩\n",
        "        pred_tokens = [recipe_vocab.lookup_token(tok.item()) for tok in prediction_ids[0]]\n",
        "        recipe = ' '.join([\n",
        "            t for t in pred_tokens \n",
        "            if t not in [ '', ',', '[', ']', '\"'] # '<pad>', '<sos>', '<eos>',\n",
        "        ]).strip()\n",
        "        results[model_type] = recipe\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧾 Ingredients: ['8 oz philadelphia cream cheese', '14 oz can sweetened condensed milk', '1 ts vanilla', '1/3 c  lemon juice', '48 oz canned cherries', '8 inch graham cracker', 'pie crusts']\n",
            "\n",
            "🔍 Generating with model: baseline1\n",
            "✅ Loaded ingredient token cache.\n",
            "✅ Loaded recipe token cache.\n",
            "📥 Loaded BEST model (Epoch 4)\n",
            "📥 Loaded FINAL model (Epoch 5)\n",
            "\n",
            "🔍 Generating with model: baseline2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jeeeunkim/Desktop/FIT5217_NLP/NLP/.venv/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded ingredient token cache.\n",
            "✅ Loaded recipe token cache.\n",
            "📥 Loaded BEST model (Epoch 3)\n",
            "📥 Loaded FINAL model (Epoch 5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jeeeunkim/Desktop/FIT5217_NLP/NLP/.venv/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Model: baseline1\n",
            "🍽️ Recipe: combine first 5 ingredients in a large saucepan . stir in milk\n",
            "--------------------------------------------------\n",
            "\n",
            "🧠 Model: baseline2\n",
            "🍽️ Recipe: in a large bowl combine cream cheese milk add milk and milk . beat until smooth . add in milk and beat until smooth . fold in whipped cream .\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "sample1_raw = \"sugar, lemon juice,  water,  orange juice, strawberries, icecream\"\n",
        "sample2_raw = \"8 oz philadelphia cream cheese, 14 oz can sweetened condensed milk, 1 ts vanilla, 1/3 c  lemon juice, 48 oz canned cherries, 8 inch graham cracker,  pie crusts\"\n",
        "\n",
        "# 예시 config\n",
        "model_names = [\"baseline1\", \"baseline2\"]\n",
        "results = generate_recipes(sample2_raw, model_names, experiment_configs)\n",
        "\n",
        "for model, recipe in results.items():\n",
        "    print(f\"\\n🧠 Model: {model}\\n🍽️ Recipe: {recipe}\")\n",
        "    print(\"-\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fill in the XLSX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_recipes_from_excel(xlsx_path, output_path, model_names, experiment_configs):\n",
        "    df = pd.read_excel(xlsx_path)\n",
        "\n",
        "    # 결과 컬럼 초기화\n",
        "    for model in model_names:\n",
        "        col_name = f\"Recipe - {model}\"\n",
        "        if col_name not in df.columns:\n",
        "            df[col_name] = \"\"\n",
        "\n",
        "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating Recipes\"):\n",
        "        try:\n",
        "            ingredient_raw = row[\"Ingredients\"]\n",
        "            ingredient_list = ast.literal_eval(ingredient_raw) if isinstance(ingredient_raw, str) else ingredient_raw\n",
        "            results = generate_recipes(\n",
        "                sample_raw=ingredient_list,\n",
        "                model_names=model_names,\n",
        "                tokenizer_ingredient=None,  # 내부에서 처리\n",
        "                experiment_configs=experiment_configs\n",
        "            )\n",
        "            for model in model_names:\n",
        "                df.at[i, f\"Recipe - {model}\"] = results.get(model, \"\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error at row {i}: {e}\")\n",
        "\n",
        "    df.to_excel(output_path, index=False)\n",
        "    print(f\"✅ Saved to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xlsx_path = \"recipes_input.xlsx\"\n",
        "output_path = \"recipes_output.xlsx\"\n",
        "model_names = [\"Baseline 1\", \"Mild Ext 1\", \"Spicy Ext 1\"]\n",
        "generate_recipes_from_excel(xlsx_path, output_path, model_names, experiment_configs)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
