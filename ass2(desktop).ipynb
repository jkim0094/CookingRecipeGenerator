{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {
        "id": "pnWO37v6oLFg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from bert_score import score as bert_score_fn\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import re\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "import gdown\n",
        "from functools import partial\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import spacy\n",
        "import ast\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mps\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    DEVICE = 'mps'  # Apple GPU 사용\n",
        "elif torch.cuda.is_available():\n",
        "    DEVICE = 'cuda'  # NVIDIA GPU 사용\n",
        "else:\n",
        "    DEVICE = 'cpu'   # CPU fallback\n",
        "\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {
        "id": "iuol8CxrHQZC"
      },
      "outputs": [],
      "source": [
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzpEEDthSnhi",
        "outputId": "3677fb2b-d313-4960-fa2a-f5932daa0af8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data size: 200\n",
            "Dev data size: 1065\n",
            "Test data size: 1081\n",
            "\n",
            "Train data sample:\n",
            "                            Title  \\\n",
            "105214  Frozen Pudding On A Stick   \n",
            "135446            Grasshopper Pie   \n",
            "44368                   Pea Salad   \n",
            "131851                   Clam Dip   \n",
            "68316                Apple Butter   \n",
            "\n",
            "                                              Ingredients  \\\n",
            "105214  [\"1 (14 oz.) can condensed milk\", \"1 1/2 c. co...   \n",
            "135446  [\"1 (7 oz.) jar marshmallow cream\", \"1 pt. whi...   \n",
            "44368   [\"1 pkg. frozen green peas (do not cook)\", \"1/...   \n",
            "131851  [\"1 (8 oz.) carton cottage cheese\", \"1 Tbsp. g...   \n",
            "68316   [\"1 qt. sweet apple cider or apple juice\", \"8 ...   \n",
            "\n",
            "                                                   Recipe  \n",
            "105214  [\"Combine condensed milk and water; mix well.\"...  \n",
            "135446  [\"Crush wafers and add melted butter.\", \"Press...  \n",
            "44368   [\"Mix all\", \"together with salad dressing.\\tBe...  \n",
            "131851  [\"Combine all ingredients; blend well with han...  \n",
            "68316   [\"Wash, core and quarter apples.\", \"Cover and ...  \n"
          ]
        }
      ],
      "source": [
        "# data 다운\n",
        "train_path = 'Cooking_Dataset/train.csv'\n",
        "dev_path = 'Cooking_Dataset/dev.csv'\n",
        "test_path = 'Cooking_Dataset/test.csv'\n",
        "\n",
        "\n",
        "if not os.path.exists('Cooking_Dataset'):\n",
        "    os.makedirs('Cooking_Dataset')\n",
        "    print(\"Downloading Dataset\") \n",
        "    gdown.download(\"https://drive.google.com/uc?id=1uZdYjvllt0dSdKKtrCgKHUk-APKdmeNU\", train_path, quiet=False)\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1SAMbkdtjGBYgojqobiwe7ZmnEq7SiGsF\", dev_path, quiet=False)\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1v6Rr2et_4WA5mRwwlRxtLhn38pbmr9Yr\", test_path, quiet=False)\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "train_df = train_df.sample(n=200,random_state=42)\n",
        "dev_df = pd.read_csv(dev_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"Train data size: {len(train_df)}\")\n",
        "print(f\"Dev data size: {len(dev_df)}\")\n",
        "print(f\"Test data size: {len(test_df)}\")\n",
        "print(\"\\nTrain data sample:\")\n",
        "print(train_df.head())\n",
        "#No-Bake Nut Cookies\n",
        "# [\"1 c. firmly packed brown sugar\", \"1/2 c. evaporated milk\", \"1/2 tsp. vanilla\", \"1/2 c. broken nuts (pecans)\", \"2 Tbsp. butter or margarine\", \"3 1/2 c. bite size shredded rice biscuits\"]\n",
        "# [\"In a heavy 2-quart saucepan, mix brown sugar, nuts, evaporated milk and butter or margarine.\", \"Stir over medium heat until mixture bubbles all over top.\", \"Boil and stir 5 minutes more. Take off heat.\", \"Stir in vanilla and cereal; mix well.\", \"Using 2 teaspoons, drop and shape into 30 clusters on wax paper.\", \"Let stand until firm, about 30 minutes.\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtQwUvtETjCF",
        "outputId": "86b57a8d-0dd5-4fa7-abdf-0d96cb367749"
      },
      "outputs": [],
      "source": [
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenizer_ingredient_baseline(text):\n",
        "    \"\"\"\n",
        "    Baseline 전처리: 소문자화 + lemmatization + 간단한 필터링\n",
        "    - stopword 제거 있음\n",
        "    - 숫자, 구두점 제거 있음\n",
        "    - 불필요한 복잡 전처리 없음\n",
        "    \"\"\"\n",
        "    import ast\n",
        "    text_list = ast.literal_eval(text)\n",
        "    tokens = []\n",
        "\n",
        "    with spacy_en.select_pipes(disable=[\"parser\", \"ner\", \"tagger\"]):\n",
        "        for item in text_list:\n",
        "            doc = spacy_en(item.lower())\n",
        "            for token in doc:\n",
        "                if token.is_punct or token.like_num or token.is_stop:\n",
        "                    continue\n",
        "                tokens.append(token.lemma_.strip())  # 기본 lemmatization만 유지\n",
        "    return tokens\n",
        "\n",
        "\n",
        "\n",
        "def tokenizer_recipe_baseline(text):\n",
        "    \"\"\"\n",
        "    Recipe 전처리: 소문자화 + lemmatization만 수행 (구두점, stopword, 숫자는 유지)\n",
        "    - 조리 순서, 동사 등 자연스러운 문장 흐름을 보존해야 하므로 간단한 전처리만 적용\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    with spacy_en.select_pipes(disable=[\"parser\", \"ner\", \"tagger\"]):\n",
        "        doc = spacy_en(text.lower())\n",
        "        for token in doc:\n",
        "            # 너무 공격적인 필터링은 하지 않음\n",
        "            if token.is_space:\n",
        "                continue\n",
        "            tokens.append(token.lemma_.strip())\n",
        "    return tokens\n",
        "\n",
        "\n",
        "\n",
        "def tokenizer_ingredient_checklist(text, remove_stopwords=True, lemmatize=True):\n",
        "    text_list = ast.literal_eval(text)  # 문자열 → 리스트로 변환\n",
        "    unit_keywords = {\n",
        "        'c', 'cup', 'cups', 'tbsp', 'tablespoon', 'tsp', 'teaspoon',\n",
        "        'oz', 'ounce', 'lb', 'pound', 'g', 'kg', 'mg',\n",
        "        'pt', 'qt', 'gal', 'ml', 'l','carton','container',\n",
        "        'package', 'pkg', 'envelope', 'box', 'bag', 'jar', 'can', 'cans', 'bottle',\n",
        "        'dash', 'pinch', 'slice', 'slices', 'head', 'inch', 'inches',\n",
        "        'stick', 'sticks', 'small', 'medium', 'large', 'size','graham'\n",
        "    }\n",
        "\n",
        "    tokens = []\n",
        "\n",
        "    with spacy_en.select_pipes(disable=[\"parser\", \"ner\"]):\n",
        "        for item in text_list:\n",
        "            doc = spacy_en(item.lower())\n",
        "            for token in doc:\n",
        "                if token.is_punct or token.is_space:\n",
        "                    continue\n",
        "                if token.like_num:\n",
        "                    continue\n",
        "                if token.text.strip(\".\") in unit_keywords:\n",
        "                    continue\n",
        "                if remove_stopwords and token.is_stop:\n",
        "                    continue\n",
        "                if token.pos_ in {\"ADJ\", \"VERB\", \"ADV\", \"PRON\"}:\n",
        "                    continue\n",
        "\n",
        "                tokens.append(token.lemma_.strip() if lemmatize else token.text.strip())\n",
        "    \n",
        "\n",
        "    return tokens\n",
        "\n",
        "def tokenizer_recipe_extension(text):  # 디폴트: lemmatize 안 함\n",
        "    text_list = ast.literal_eval(text)\n",
        "    tokens = []\n",
        "\n",
        "    # tagger는 유지해서 lemma 써도 warning 없음\n",
        "    with spacy_en.select_pipes(disable=[\"parser\", \"ner\", \"tagger\"]):\n",
        "        for item in text_list:\n",
        "            doc = spacy_en(item.lower())  # 소문자화\n",
        "            for token in doc:\n",
        "                if token.is_punct or token.is_space:\n",
        "                    continue  # 마침표, 쉼표 제거\n",
        "                if token.like_num:  # 숫자 유지\n",
        "                    tokens.append(token.text.strip())\n",
        "                    continue\n",
        "                \n",
        "                tokens.append(token.text.strip())\n",
        "                \n",
        "    return tokens\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"Crush wafers and add melted butter.\", \"Press into an 8-inch pan and save some for top decoration.\"]\n",
            "['crush', 'wafers', 'and', 'add', 'melted', 'butter', 'press', 'into', 'an', '8', 'inch', 'pan', 'and', 'save', 'some', 'for', 'top', 'decoration']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jeeeunkim/Desktop/FIT5217_NLP/NLP/.venv/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ],
      "source": [
        "ing = train_df.iloc[1,2]\n",
        "print(ing)\n",
        "print(tokenizer_recipe_extension(ing))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {
        "id": "O5IrjwMXuv3h"
      },
      "outputs": [],
      "source": [
        "def build_vocab(token_lists, min_freq=2):\n",
        "    # vocab 생성: 자주 등장하는 단어만 포함 + 특수 토큰 정의\n",
        "    vocab = build_vocab_from_iterator(\n",
        "        token_lists,  # 토큰 리스트들을 직접 반복\n",
        "        min_freq=min_freq,  # 최소 등장 빈도\n",
        "        specials=['<pad>', '<sos>', '<eos>', '<unk>']  # 특수 토큰 추가\n",
        "    )\n",
        "    vocab.set_default_index(vocab['<unk>'])  # 없는 단어는 <unk>로 처리\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_or_tokenize_data(model_type, train_df, config=None):\n",
        "    use_checklist = config.get(\"USE_CHECKLIST\", False)\n",
        "\n",
        "    tokenizer_ingredient = tokenizer_ingredient_baseline\n",
        "    tokenizer_recipe = tokenizer_recipe_baseline\n",
        "    tokenizer_checklist = tokenizer_ingredient_checklist if use_checklist else None\n",
        "\n",
        "    ingredient_cache_path = \"tokens/ingredient_tokens.pkl\"\n",
        "    recipe_cache_path = \"tokens/recipe_tokens.pkl\"\n",
        "    checklist_cache_path = \"tokens/checklist_tokens.pkl\"\n",
        "\n",
        "    ingredient_cache_download = \"https://drive.google.com/uc?id=1Wxvq-qy4ifbzPKmcm_VuNRRXrEOSqisJ\"\n",
        "    recipe_cache_download = \"https://drive.google.com/uc?id=1IyBlDfL9sE8_Ip3muDyciCK9Hiv_VZF8\"\n",
        "    checklist_cache_download = \"https://drive.google.com/uc?id=1-WVksjq8Cgj6UiJ7wtemU2CfP4hhPrci\"\n",
        "\n",
        "    if not os.path.exists('tokens'):\n",
        "        os.makedirs('tokens')\n",
        "\n",
        "    cache_targets = [\n",
        "        (ingredient_cache_path, ingredient_cache_download),\n",
        "        (recipe_cache_path, recipe_cache_download)\n",
        "    ]\n",
        "    if use_checklist:\n",
        "        cache_targets.append((checklist_cache_path, checklist_cache_download))\n",
        "\n",
        "    for path, url in cache_targets:\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"⬇️  Downloading: {path}\")\n",
        "            try:\n",
        "                r = requests.get(url, allow_redirects=True)\n",
        "                if r.status_code == 200:\n",
        "                    with open(path, 'wb') as f:\n",
        "                        f.write(r.content)\n",
        "                    print(f\"✅ Downloaded: {path}\")\n",
        "                else:\n",
        "                    print(f\"❌ Download failed for {path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Download error: {e}\")\n",
        "\n",
        "    if os.path.exists(ingredient_cache_path):\n",
        "        with open(ingredient_cache_path, \"rb\") as f:\n",
        "            ingredient_token_lists = pickle.load(f)\n",
        "        print(\"✅ Loaded ingredient token cache.\")\n",
        "    else:\n",
        "        print(\"⚠️ No ingredient cache → Tokenizing...\")\n",
        "        ingredient_token_lists = [tokenizer_ingredient(text) for text in tqdm(train_df['Ingredients'], desc=\"Tokenizing ingredients\")]\n",
        "        with open(ingredient_cache_path, \"wb\") as f:\n",
        "            pickle.dump(ingredient_token_lists, f)\n",
        "\n",
        "    if os.path.exists(recipe_cache_path):\n",
        "        with open(recipe_cache_path, \"rb\") as f:\n",
        "            recipe_token_lists = pickle.load(f)\n",
        "        print(\"✅ Loaded recipe token cache.\")\n",
        "    else:\n",
        "        print(\"⚠️ No recipe cache → Tokenizing...\")\n",
        "        recipe_token_lists = [tokenizer_recipe(text) for text in tqdm(train_df['Recipe'], desc=\"Tokenizing recipes\")]\n",
        "        with open(recipe_cache_path, \"wb\") as f:\n",
        "            pickle.dump(recipe_token_lists, f)\n",
        "\n",
        "    ingredient_vocab = build_vocab(ingredient_token_lists, min_freq=1)\n",
        "    recipe_vocab = build_vocab(recipe_token_lists, min_freq=2)\n",
        "\n",
        "    checklist_vocab = None\n",
        "    if use_checklist:\n",
        "        if os.path.exists(checklist_cache_path):\n",
        "            with open(checklist_cache_path, \"rb\") as f:\n",
        "                checklist_token_lists = pickle.load(f)\n",
        "            print(\"✅ Loaded checklist token cache.\")\n",
        "        else:\n",
        "            print(\"⚠️ No checklist cache → Tokenizing...\")\n",
        "            checklist_token_lists = [tokenizer_checklist(text) for text in tqdm(train_df['Ingredients'], desc=\"Checklist tokenize\")]\n",
        "            with open(checklist_cache_path, \"wb\") as f:\n",
        "                pickle.dump(checklist_token_lists, f)\n",
        "\n",
        "        checklist_vocab = build_vocab(checklist_token_lists, min_freq=1)\n",
        "\n",
        "    return ingredient_token_lists, recipe_token_lists, ingredient_vocab, recipe_vocab, tokenizer_ingredient, tokenizer_recipe, checklist_vocab, tokenizer_checklist\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_checklist_tensor(checklist_vocab, checklist_tokenizer, ingredient_texts, device, embedding_layer):\n",
        "    batch_ids = []\n",
        "\n",
        "\n",
        "\n",
        "    for text in ingredient_texts:\n",
        "        tokens = checklist_tokenizer(text)\n",
        "        ids = [checklist_vocab[token] for token in tokens]\n",
        "        batch_ids.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    checklist_padded = pad_sequence(batch_ids, batch_first=True, padding_value=checklist_vocab['<pad>']).to(device)\n",
        "    checklist_mask = (checklist_padded != checklist_vocab['<pad>']).float().to(device)\n",
        "    checklist_embeds = embedding_layer(checklist_padded)\n",
        "\n",
        "\n",
        "\n",
        "    return checklist_embeds, checklist_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "metadata": {
        "id": "u3aRuH1BcoQQ"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, ingredient_vocab, recipe_vocab, tokenizer_ingredient, tokenizer_recipe):\n",
        "        self.df = df\n",
        "        self.ingredient_vocab = ingredient_vocab\n",
        "        self.recipe_vocab = recipe_vocab\n",
        "        self.tokenizer_ingredient = tokenizer_ingredient\n",
        "        self.tokenizer_recipe = tokenizer_recipe\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ingredient_text = self.df.iloc[idx]['Ingredients']\n",
        "        recipe_text = self.df.iloc[idx]['Recipe']\n",
        "\n",
        "        ingredient_tokens = self.tokenizer_ingredient(ingredient_text)\n",
        "        recipe_tokens = self.tokenizer_recipe(recipe_text)\n",
        "\n",
        "        ingredient_ids = [self.ingredient_vocab['<sos>']] + [self.ingredient_vocab[token] for token in ingredient_tokens] + [self.ingredient_vocab['<eos>']]\n",
        "        recipe_ids = [self.recipe_vocab['<sos>']] + [self.recipe_vocab[token] for token in recipe_tokens] + [self.recipe_vocab['<eos>']]\n",
        "\n",
        "        return torch.tensor(ingredient_ids), torch.tensor(recipe_ids), ingredient_text  # ✅ 3개 반환\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {
        "id": "zvtpEXhJyzRN"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch, ingredient_vocab, recipe_vocab, device):\n",
        "    ingredients, recipes, ingredient_texts = zip(*batch)  # ingredient_texts: str\n",
        "    ingredients_padded = pad_sequence(ingredients, batch_first=True, padding_value=ingredient_vocab['<pad>'])\n",
        "    recipes_padded = pad_sequence(recipes, batch_first=True, padding_value=recipe_vocab['<pad>'])\n",
        "    return ingredients_padded.to(device), recipes_padded.to(device), list(ingredient_texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {
        "id": "ZHuvOqpGoe5N"
      },
      "outputs": [],
      "source": [
        "class Encoder_GRU(nn.Module):\n",
        "    def __init__(self, ingredient_vocab_size, embedding_dim, hidden_dim, n_layers, dropout_ratio,\n",
        "                 embedding_weights=None, freeze=False):\n",
        "        super().__init__()\n",
        "        # 임베딩\n",
        "        if embedding_weights is not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=freeze)\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(ingredient_vocab_size, embedding_dim)\n",
        "        \n",
        "\n",
        "        # GRU 레이어\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.gru = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          n_layers,\n",
        "                          dropout=dropout_ratio if n_layers>1 else 0,\n",
        "                          batch_first=True)\n",
        "\n",
        "        # 드롭아웃\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src : [batch_size, src_len]\n",
        "        src = src.long() \n",
        "        # 임베딩\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded : [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        # gru 통과\n",
        "        outputs, hidden = self.gru(embedded) # h0를 따로 주지 않으면, 디폴트로 h0가 0로 초기화되서 들어감\n",
        "        # outputs: [batch_size, src_len, hidden_size]\n",
        "        # hidden: [n_layers, batch_size, hidden_size]\n",
        "\n",
        "        return outputs,hidden\n",
        "\n",
        "class Encoder_GRU_extension(nn.Module):\n",
        "    def __init__(self, ingredient_vocab_size, embedding_dim, hidden_dim, n_layers, dropout_ratio,\n",
        "                 embedding_weights=None, freeze=False):\n",
        "        super().__init__()\n",
        "        if embedding_weights is not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=freeze)\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(ingredient_vocab_size, embedding_dim)\n",
        "        self.ingredient_vocab_size = ingredient_vocab_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers,\n",
        "                          dropout=dropout_ratio if n_layers > 1 else 0,\n",
        "                          batch_first=True)\n",
        "\n",
        "        self.skip_proj = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        self._init_weights()  # ✅ 초기화 적용\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # embedding 초기화 (from_pretrained 아닌 경우만)\n",
        "        if not hasattr(self.embedding, 'weight') or self.embedding.weight.requires_grad:\n",
        "            nn.init.kaiming_normal_(self.embedding.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
        "\n",
        "        # GRU 내부 weight 초기화\n",
        "        for name, param in self.gru.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.kaiming_normal_(param, mode='fan_in', nonlinearity='leaky_relu')\n",
        "            elif 'bias' in name:\n",
        "                nn.init.constant_(param, 0)\n",
        "\n",
        "        # skip connection projection 초기화\n",
        "        nn.init.kaiming_normal_(self.skip_proj.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
        "        nn.init.constant_(self.skip_proj.bias, 0)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = src.long()\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, hidden = self.gru(embedded)\n",
        "        skip = self.skip_proj(embedded)\n",
        "        outputs = self.leaky_relu(outputs + skip)\n",
        "        outputs = self.layer_norm(outputs)\n",
        "        return outputs, hidden\n",
        "\n",
        "\n",
        "\n",
        "class Decoder_GRU(nn.Module):\n",
        "    def __init__(self, recipe_vocab_size, embedding_dim, hidden_dim, n_layers, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        self.recipe_vocab_size = recipe_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "\n",
        "        # 임베딩\n",
        "        self.embedding = nn.Embedding(recipe_vocab_size, embedding_dim)\n",
        "\n",
        "        # GRU 레이어\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.gru = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          n_layers,\n",
        "                          dropout=dropout_ratio if n_layers>1 else 0,\n",
        "                          batch_first=True)\n",
        "\n",
        "        # fc 레이어\n",
        "        self.fc_out = nn.Linear(hidden_dim, recipe_vocab_size)\n",
        "\n",
        "        # 드롭아웃\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input = input.long()\n",
        "        # input : [batch_size]\n",
        "        input = input.unsqueeze(1)\n",
        "        # input : [batch_size, 단어의 개수=1]\n",
        "\n",
        "        # 임베딩\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded : [batch_size, 단어의 개수=1, hidden_dim]\n",
        "\n",
        "        # GRU 통과\n",
        "        outputs, hidden = self.gru(embedded,hidden)\n",
        "        # outputs: [batch_size, 단어의 개수=1, hidden_size]\n",
        "        # hidden: [n_layers, batch_size, hidden_size]\n",
        "\n",
        "        # fc 통과\n",
        "        prediction = self.fc_out(outputs.squeeze(1))\n",
        "        # prediction: [batch_size, vocab_size]\n",
        "\n",
        "        return prediction, hidden\n",
        "\n",
        "# Define a decoder with attention mechanism using PyTorch's nn.Module\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, recipe_vocab_size,embedding_dim, hidden_size, n_layers, dropout_ratio):\n",
        "        # Initialize the base nn.Module class\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "\n",
        "        # Save parameters\n",
        "        self.recipe_vocab_size = recipe_vocab_size              # Size of the output vocabulary\n",
        "        self.embedding_dim = embedding_dim                  # Dropout probability\n",
        "        self.hidden_size = hidden_size              # Size of the hidden state in GRU\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "                  \n",
        "\n",
        "        # Define layers\n",
        "        self.embedding = nn.Embedding(self.recipe_vocab_size, self.hidden_size)  # Converts word indices to dense vectors\n",
        "        self.dropout = nn.Dropout(self.dropout_ratio)                          # Applies dropout for regularization\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, self.n_layers, dropout=self.dropout_ratio if self.n_layers>1 else 0, batch_first=True)\n",
        "             # GRU to process the embedded inputs\n",
        "        self.out = nn.Linear(self.hidden_size * 2, self.recipe_vocab_size)       # Linear layer for generating final output\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        input = input.long() \n",
        "        # input : [batch_size]\n",
        "        input = input.unsqueeze(1)\n",
        "        # input : [batch_size, 단어의 개수=1]\n",
        "\n",
        "        # 임베딩\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded : [batch_size, 단어의 개수=1, hidden_dim]\n",
        "\n",
        "        # Pass through GRU\n",
        "        output, hidden = self.gru(embedded, hidden)  # output: [batch, 1, hidden_size]\n",
        "\n",
        "        # Compute attention weights using dot-product attention:\n",
        "        # hidden[-1]: [batch, hidden_size]\n",
        "        # encoder_outputs: [batch, src_len, hidden_size]\n",
        "    \n",
        "        attn_weights = F.softmax(\n",
        "            torch.bmm(output, encoder_outputs.transpose(1, 2)), # [batch, 1, hidden_size] x [batch, hidden_size, src_len]\n",
        "            dim=-1\n",
        "        )  # [batch, 1, src_len]\n",
        "\n",
        "        # Apply attention weights to encoder outputs to get context vector\n",
        "        # attn_weights: (1, 1, max_length)\n",
        "        # encoder_outputs.unsqueeze(0): (1, max_length, hidden_size)\n",
        "        attn_output = torch.bmm(attn_weights, encoder_outputs)  # [batch, 1, hidden_size]\n",
        "\n",
        "        # Concatenate attention output and decoder hidden state\n",
        "        concat_output = torch.cat((output, attn_output), dim=2)  # [batch, 1, hidden*2]\n",
        "\n",
        "        # Pass through linear layer and softmax to get output word probabilities\n",
        "        output = F.log_softmax(self.out(concat_output).squeeze(1), dim=1)  # [batch, vocab_size]\n",
        "\n",
        "        # Return output word distribution, updated hidden state, and attention weights\n",
        "        return output, hidden, attn_weights.squeeze(1)\n",
        "\n",
        "class AttnDecoderRNN_extension(nn.Module):\n",
        "    def __init__(self, recipe_vocab_size, embedding_dim, hidden_size, n_layers, dropout_ratio):\n",
        "        super().__init__()\n",
        "        self.recipe_vocab_size = recipe_vocab_size\n",
        "        self.embedding = nn.Embedding(recipe_vocab_size, embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_size, n_layers,\n",
        "                          dropout=dropout_ratio if n_layers > 1 else 0,\n",
        "                          batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size * 2, recipe_vocab_size)\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "        self._init_weights()  # ✅ He 초기화 호출\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # Embedding\n",
        "        nn.init.kaiming_normal_(self.embedding.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
        "\n",
        "        # GRU weights & biases\n",
        "        for name, param in self.gru.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.kaiming_normal_(param, mode='fan_in', nonlinearity='leaky_relu')\n",
        "            elif 'bias' in name:\n",
        "                nn.init.constant_(param, 0)\n",
        "\n",
        "        # Linear layer\n",
        "        nn.init.kaiming_normal_(self.out.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
        "        nn.init.constant_(self.out.bias, 0)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        input = input.long().unsqueeze(1)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        output = self.leaky_relu(output)\n",
        "        output = self.layer_norm(output)\n",
        "\n",
        "        attn_weights = F.softmax(torch.bmm(output, encoder_outputs.transpose(1, 2)), dim=-1)\n",
        "        context = torch.bmm(attn_weights, encoder_outputs)\n",
        "        concat_output = torch.cat((output, context), dim=2)\n",
        "        output = F.log_softmax(self.out(concat_output).squeeze(1), dim=1)\n",
        "        return output, hidden, attn_weights.squeeze(1)\n",
        "\n",
        "class DecoderWithChecklist(nn.Module):\n",
        "    def __init__(self, recipe_vocab_size, embedding_dim, hidden_size, n_layers, dropout_ratio, checklist_vocab_size, checklist_vocab, tokenizer_checklist):\n",
        "        super().__init__()\n",
        "        self.recipe_vocab_size = recipe_vocab_size\n",
        "        self.embedding = nn.Embedding(recipe_vocab_size, embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_size, n_layers,\n",
        "                          dropout=dropout_ratio if n_layers > 1 else 0,\n",
        "                          batch_first=True)\n",
        "\n",
        "        self.checklist_embedding = nn.Embedding(checklist_vocab_size, hidden_size)\n",
        "        self.attn_proj = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out_proj = nn.Linear(hidden_size, recipe_vocab_size)\n",
        "\n",
        "        self.ref_selector = nn.Linear(hidden_size, 3)\n",
        "        self.checklist_max_len = checklist_vocab_size\n",
        "        self.checklist_vocab = checklist_vocab\n",
        "        self.tokenizer_checklist = tokenizer_checklist\n",
        "        self.a_prev_list = []\n",
        "\n",
        "    def forward(self, input_token, hidden, encoder_outputs=None, ingredient_texts=None, a_prev=None, **kwargs):\n",
        "        input_token = input_token.unsqueeze(1)\n",
        "        embedded = self.dropout(self.embedding(input_token))\n",
        "        gru_out, hidden = self.gru(embedded, hidden)\n",
        "        h_t = gru_out.squeeze(1)\n",
        "\n",
        "        if ingredient_texts is None or a_prev is None:\n",
        "            output = self.out_proj(h_t)\n",
        "            return output, hidden\n",
        "\n",
        "        batch_ids = []\n",
        "        for text in ingredient_texts:\n",
        "            tokens = self.tokenizer_checklist(text)\n",
        "            ids = [self.checklist_vocab[token] for token in tokens]\n",
        "            batch_ids.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "        padded = pad_sequence(batch_ids, batch_first=True, padding_value=0)\n",
        "        checklist_embeds = self.checklist_embedding(padded.to(input_token.device))\n",
        "        checklist_mask = (padded != 0).float().to(input_token.device)\n",
        "\n",
        "        used_mask = a_prev.clamp(0, 1).unsqueeze(2)\n",
        "        unused_mask = (1 - a_prev).clamp(0, 1).unsqueeze(2)\n",
        "\n",
        "        E_new = checklist_embeds * unused_mask\n",
        "        E_used = checklist_embeds * used_mask\n",
        "\n",
        "        h_proj = self.attn_proj(h_t).unsqueeze(2)\n",
        "        scores_new = torch.bmm(E_new, h_proj).squeeze(2)\n",
        "        scores_used = torch.bmm(E_used, h_proj).squeeze(2)\n",
        "\n",
        "        scores_new = scores_new.masked_fill(checklist_mask == 0, -1e9)\n",
        "        scores_used = scores_used.masked_fill(checklist_mask == 0, -1e9)\n",
        "\n",
        "        alpha_new = F.softmax(scores_new, dim=1)\n",
        "        alpha_used = F.softmax(scores_used, dim=1)\n",
        "\n",
        "        c_new = torch.bmm(alpha_new.unsqueeze(1), checklist_embeds).squeeze(1)\n",
        "        c_used = torch.bmm(alpha_used.unsqueeze(1), checklist_embeds).squeeze(1)\n",
        "\n",
        "        f_logits = self.ref_selector(h_t)\n",
        "        f = F.softmax(f_logits, dim=-1)\n",
        "        f_gru, f_new, f_used = f.chunk(3, dim=1)\n",
        "\n",
        "        o_t = f_gru * h_t + f_new * c_new + f_used * c_used\n",
        "        output = self.out_proj(o_t)\n",
        "\n",
        "        a_t = a_prev + f_new * alpha_new\n",
        "\n",
        "        # ✅ Coverage Loss 계산을 위한 저장\n",
        "        if self.training:\n",
        "            if not hasattr(self, \"a_prev_list\"):\n",
        "                self.a_prev_list = []\n",
        "            self.a_prev_list.append((a_prev.detach(), alpha_new.detach()))\n",
        "\n",
        "        return output, hidden, a_t, alpha_new, alpha_used\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device, use_attention, recipe_vocab):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.use_attention = use_attention\n",
        "        self.recipe_vocab = recipe_vocab  # 🔹 추가: vocab 저장\n",
        "\n",
        "    def forward(self, src, ingredient_texts=None, target=None, teacher_forcing_ratio=0.5, max_len=50):\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        batch_size = src.size(0)\n",
        "        vocab_size = self.decoder.recipe_vocab_size\n",
        "\n",
        "        checklist_embed, checklist_mask = None, None\n",
        "        is_checklist_decoder = hasattr(self.decoder, 'checklist_vocab')\n",
        "\n",
        "        if is_checklist_decoder:\n",
        "            checklist_embed, checklist_mask = get_checklist_tensor(\n",
        "                checklist_vocab=self.decoder.checklist_vocab,\n",
        "                checklist_tokenizer=self.decoder.tokenizer_checklist,\n",
        "                ingredient_texts=ingredient_texts,\n",
        "                device=self.device,\n",
        "                embedding_layer=self.decoder.checklist_embedding\n",
        "            )\n",
        "\n",
        "        a_prev = None\n",
        "        if checklist_mask is not None:\n",
        "            checklist_len = checklist_mask.size(1)\n",
        "            a_prev = torch.zeros(batch_size, checklist_len, device=self.device)\n",
        "\n",
        "        # ▶️ Inference mode\n",
        "        if target is None:\n",
        "            outputs = []\n",
        "            input_token = torch.tensor([self.recipe_vocab['<sos>']] * batch_size).to(self.device).long()  # 🔹 수정\n",
        "\n",
        "            for _ in range(max_len):\n",
        "                if is_checklist_decoder:\n",
        "                    output, hidden, a_prev, *_ = self.decoder(\n",
        "                        input_token, hidden, encoder_outputs,\n",
        "                        ingredient_texts=ingredient_texts, a_prev=a_prev,\n",
        "                        checklist_embeds=checklist_embed,\n",
        "                        checklist_mask=checklist_mask\n",
        "                    )\n",
        "                elif self.use_attention:\n",
        "                    result = self.decoder(input_token, hidden, encoder_outputs)\n",
        "                    output, hidden = result[:2] if isinstance(result, (tuple, list)) else result\n",
        "                else:\n",
        "                    output, hidden = self.decoder(input_token, hidden)\n",
        "\n",
        "                top1 = output.argmax(1)\n",
        "                outputs.append(top1.unsqueeze(1))\n",
        "                input_token = top1\n",
        "\n",
        "            return torch.cat(outputs, dim=1)\n",
        "\n",
        "        # 🟢 Training mode\n",
        "        target = target.long()\n",
        "        target_len = target.shape[1]\n",
        "        outputs = torch.zeros(batch_size, target_len, vocab_size).to(self.device)\n",
        "        input_token = target[:, 0].long()\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            if is_checklist_decoder:\n",
        "                output, hidden, a_prev, *_ = self.decoder(\n",
        "                    input_token, hidden, encoder_outputs,\n",
        "                    ingredient_texts=ingredient_texts, a_prev=a_prev,\n",
        "                    checklist_embeds=checklist_embed,\n",
        "                    checklist_mask=checklist_mask\n",
        "                )\n",
        "            elif self.use_attention:\n",
        "                result = self.decoder(input_token, hidden, encoder_outputs)\n",
        "                output, hidden = result[:2] if isinstance(result, (tuple, list)) else result\n",
        "            else:\n",
        "                output, hidden = self.decoder(input_token, hidden)\n",
        "\n",
        "            outputs[:, t, :] = output\n",
        "            top1 = output.argmax(1)\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            input_token = target[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class IngredientAutoEncoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder, vocab_size):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def forward(self, src):\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        input_token = src[:, 0]  # <sos>\n",
        "        outputs = torch.zeros(src.size(0), src.size(1) - 1, self.vocab_size).to(src.device)\n",
        "\n",
        "        for t in range(1, src.size(1)):\n",
        "            output, hidden, _ = self.decoder(input_token, hidden, encoder_outputs)\n",
        "            outputs[:, t-1, :] = output\n",
        "            input_token = src[:, t]  # teacher forcing\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_glove_embedding_matrix(glove_path, vocab, glove_dim=200):\n",
        "    print(\"🔎 Loading GloVe vectors...\")\n",
        "    glove_embeddings = {}\n",
        "\n",
        "    with open(glove_path, 'r', encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            vector = torch.tensor(list(map(float, values[1:])))\n",
        "            glove_embeddings[word] = vector\n",
        "\n",
        "    # Initialize matrix with random vectors\n",
        "    embedding_matrix = torch.randn(len(vocab), glove_dim)\n",
        "\n",
        "    for word, idx in vocab.get_stoi().items():\n",
        "        if word in glove_embeddings:\n",
        "            embedding_matrix[idx] = glove_embeddings[word]\n",
        "\n",
        "    print(\"✅ GloVe embedding matrix created.\")\n",
        "    return embedding_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_epoch(model, dataloader, criterion, optimizer=None,\n",
        "               teacher_forcing_ratio=0.5, max_len=50,\n",
        "               USE_PENALTY=False, penalty_lambda=0.2):\n",
        "    # model.train() if optimizer else model.eval()\n",
        "    total_loss = 0\n",
        "    batch_losses = []\n",
        "    # ✅ attention checklist decoder의 coverage 정보 초기화\n",
        "    if hasattr(model.decoder, \"a_prev_list\"):\n",
        "        model.decoder.a_prev_list = []\n",
        "\n",
        "    for i, (src_batch, trg_batch, ingredient_texts) in enumerate(dataloader):\n",
        "        src_batch = src_batch.to(DEVICE)\n",
        "        trg_batch = trg_batch.to(DEVICE)\n",
        "\n",
        "        output = model(src_batch, ingredient_texts, trg_batch,\n",
        "                       teacher_forcing_ratio=teacher_forcing_ratio,\n",
        "                       max_len=max_len)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:, :].reshape(-1, output_dim)\n",
        "        trg = trg_batch[:, 1:].reshape(-1)\n",
        "\n",
        "        # ✅ 기본 cross entropy loss\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        # ✅ Penalty Loss 추가 (조건부)\n",
        "        if USE_PENALTY and hasattr(model.decoder, 'a_prev_list'):\n",
        "            penalty = 0.0\n",
        "            for a_prev, alpha_new in model.decoder.a_prev_list:\n",
        "                penalty += torch.sum(torch.min(a_prev, alpha_new))\n",
        "            penalty = penalty / src_batch.size(0)  # 평균화\n",
        "            loss += penalty_lambda * penalty\n",
        "\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "            optimizer.step()\n",
        "\n",
        "        batch_loss = loss.item() * src_batch.shape[0]\n",
        "        batch_losses.append(batch_loss)\n",
        "        total_loss += batch_loss\n",
        "        print(f\"Batch {i+1}/{len(dataloader)} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = total_loss / len(dataloader.dataset)\n",
        "    return epoch_loss, batch_losses\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Train(model, train_loader, val_loader, criterion, optimizer,recipe_vocab,\n",
        "          EPOCHS, BATCH_SIZE, TRAIN_RATIO,\n",
        "          save_model_path, save_history_path, TEACHER_FORCING_RATIO, MAX_LEN, **kwargs\n",
        "          ):\n",
        "    \"\"\"\n",
        "    이어서 학습할 수 있게 start_epoch와 best_val_loss를 인자로 받음\n",
        "    \"\"\"\n",
        "    lr_step = kwargs.get(\"LR_STEP\")\n",
        "    lr_gamma = kwargs.get(\"LR_GAMMA\")\n",
        "\n",
        "    if isinstance(lr_step, int) and isinstance(lr_gamma, float):\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_step, gamma=lr_gamma)\n",
        "    else:\n",
        "        scheduler = None\n",
        "    # if \"LR_STEP\" in kwargs:\n",
        "    #     scheduler = StepLR(optimizer, step_size = kwargs[\"LR_STEP\"], gamma = kwargs[\"LR_GAMMA\"])\n",
        "    # else:\n",
        "    #     scheduler = None\n",
        "\n",
        "    \n",
        "    loss_history = {\"train_epoch\": [], \"train_iter\": [], \"val_epoch\": []}\n",
        "    best_val_loss = float('inf')\n",
        "    train_start_time = time.time()\n",
        "    best_epoch = -1\n",
        "    best_train_loss = None\n",
        "    \n",
        "    #for ep in tqdm(range(EPOCHS), desc=\"Epochs\",file=sys.stdout):\n",
        "    for ep in range(EPOCHS):\n",
        "\n",
        "        ep_start_time = time.time()\n",
        "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "        print(f\"[Epoch: {ep+1}/{EPOCHS}] current_LR = {current_lr}\")\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_epoch_loss, train_batch_loss = loss_epoch(\n",
        "            model, train_loader, criterion, optimizer,\n",
        "            teacher_forcing_ratio=TEACHER_FORCING_RATIO,\n",
        "            max_len=MAX_LEN,\n",
        "            USE_PENALTY=config.get(\"USE_PENALTY\", False),\n",
        "            penalty_lambda=config.get(\"PENALTY_LAMBDA\", 0.2)\n",
        "        )\n",
        "        loss_history[\"train_epoch\"].append(train_epoch_loss)\n",
        "        loss_history[\"train_iter\"].append(train_batch_loss)\n",
        "        print(f\"{ep+1} Epoch Train Completed!\")\n",
        "    \n",
        "        \n",
        "\n",
        "        # Validation\n",
        "        print(\"Validation Start!\")\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss, _ = loss_epoch(\n",
        "                model, val_loader, criterion, optimizer=None,\n",
        "                teacher_forcing_ratio=0.0,\n",
        "                max_len=MAX_LEN,\n",
        "                USE_PENALTY=config.get(\"USE_PENALTY\", False),\n",
        "                penalty_lambda=config.get(\"PENALTY_LAMBDA\", 0.2)\n",
        "            )\n",
        "            loss_history[\"val_epoch\"].append(val_loss)\n",
        "        print(\"Validation Completed!\")\n",
        "\n",
        "        ep_elapsed_time = time.time() - ep_start_time\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"[Epoch {ep+1}/{EPOCHS}] \")\n",
        "        print(f\"Train Loss: {train_epoch_loss:.4f} | Val Loss: {val_loss:.4f} | Time: {ep_elapsed_time:.2f}s\")\n",
        "        bleu, meteor, bert_f1 = compute_metrics(model, val_loader, recipe_vocab, max_len=MAX_LEN)\n",
        "        print(f\"Validation Metrics:\")\n",
        "        print(f\"📊 BLEU: {bleu:.4f} | METEOR: {meteor:.4f} | BERTScore-F1: {bert_f1:.4f}\")  \n",
        "        print(\"-\" * 50) \n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = ep+1\n",
        "            best_train_loss = train_epoch_loss\n",
        "            best_bleu, best_meteor, best_bert_f1 = bleu, meteor, bert_f1\n",
        "            # 디렉토리 없으면 생성\n",
        "            os.makedirs(\"results\", exist_ok=True)\n",
        "            torch.save({\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"epoch\": ep+1,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"train_loss\": train_epoch_loss,  # train_loss 저장\n",
        "            }, save_model_path)\n",
        "            print(\"Best model saved!\")\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "    # 🔸 Save final model after training\n",
        "    final_model_path = save_model_path.replace(\".pt\", \"_final.pt\")\n",
        "    torch.save({\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"epoch\": EPOCHS,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"train_loss\": train_epoch_loss,\n",
        "    }, final_model_path)\n",
        "    print(\"Final model saved!\")\n",
        "    \n",
        "    \n",
        "    # Save training history\n",
        "    train_elapsed_time = time.time() - train_start_time\n",
        "    total_iterations = sum(len(batch_list) for batch_list in loss_history[\"train_iter\"])\n",
        "    torch.save({\n",
        "        \"loss_history\": loss_history,\n",
        "        \"EPOCHS\": EPOCHS,\n",
        "        \"BATCH_SIZE\": BATCH_SIZE,   \n",
        "        \"TRAIN_RATIO\": TRAIN_RATIO,\n",
        "        \"train_elapsed_time\": train_elapsed_time,\n",
        "        \"total_iterations\": total_iterations,\n",
        "        \"LR_STEP\": kwargs[\"LR_STEP\"] if \"LR_STEP\" in kwargs else None,\n",
        "        \"LR_GAMMA\": kwargs[\"LR_GAMMA\"] if \"LR_GAMMA\" in kwargs else None\n",
        "    }, save_history_path)\n",
        "    print(\"Training Completed!\")\n",
        "    print(f\"Total number of iteration : {total_iterations}\")\n",
        "    print(f\"Total Time for Training: {train_elapsed_time:.2f}s\")\n",
        "    # Summary print\n",
        "    print(\"\\n======= Training Summary =======\")\n",
        "    print(f\"Best Model:    Epoch {best_epoch}\")\n",
        "    if best_epoch == EPOCHS:\n",
        "        print(f\"(Best model is the final model)\")\n",
        "    print(f\"Train Loss:    {best_train_loss:.4f}\")\n",
        "    print(f\"Val Loss:      {best_val_loss:.4f}\")\n",
        "    print(f\"BLEU:          {best_bleu:.4f}\")\n",
        "    print(f\"METEOR:        {best_meteor:.4f}\")\n",
        "    print(f\"BERTScore-F1:  {best_bert_f1:.4f}\")\n",
        "    print(\"\\n\")\n",
        "    print(f\"Final Model:   Epoch {EPOCHS}\")\n",
        "    print(f\"Train Loss:    {train_epoch_loss:.4f}\")\n",
        "    print(f\"Val Loss:      {val_loss:.4f}\")\n",
        "    print(f\"BLEU:          {bleu:.4f}\")\n",
        "    print(f\"METEOR:        {meteor:.4f}\")\n",
        "    print(f\"BERTScore-F1:  {bert_f1:.4f}\")\n",
        "    print(f\"Total Time:     {train_elapsed_time:.2f}s\")\n",
        "    print(\"==================================\\n\")\n",
        "    return loss_history\n",
        "\n",
        "def train_autoencoder(model, dataloader, criterion, optimizer, epochs, device, label=\"AutoEncoder\"):\n",
        "    model.train()\n",
        "    for ep in range(epochs):\n",
        "        total_loss = 0\n",
        "        for src_batch, _ in dataloader:\n",
        "            src_batch = src_batch.to(device)\n",
        "            output = model(src_batch)\n",
        "            trg = src_batch[:, 1:].contiguous()\n",
        "\n",
        "            output = output.view(-1, output.size(-1))\n",
        "            trg = trg.view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"[Epoch {ep+1}] {label} Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "def Test(model, test_loader, criterion, recipe_vocab, MAX_LEN):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loss, _ = loss_epoch(model, test_loader, criterion, optimizer=None, teacher_forcing_ratio=0.0, max_len=MAX_LEN)\n",
        "\n",
        "    # 평가 지표 계산\n",
        "    bleu_score, meteor_avg, bertscore_f1 = compute_metrics(model, test_loader, recipe_vocab)\n",
        "\n",
        "    print(f\"Test Loss      : {test_loss:.4f}\")\n",
        "    print(f\"BLEU-4 Score   : {bleu_score:.4f}\")\n",
        "    print(f\"METEOR Score   : {meteor_avg:.4f}\")\n",
        "    print(f\"BERTScore (F1) : {bertscore_f1:.4f}\")\n",
        "\n",
        "    return test_loss, bleu_score, meteor_avg, bertscore_f1\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(model, dataloader, recipe_vocab, max_len=50):\n",
        "    \"\"\"\n",
        "    테스트셋 전체에서 BLEU, METEOR, BERTScore 계산\n",
        "\n",
        "    Returns:\n",
        "        bleu_score (float), meteor_avg (float), bertscore_f1 (float)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    smoothie = SmoothingFunction().method4\n",
        "\n",
        "    ref_list = []\n",
        "    hyp_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src_batch, trg_batch, ingredient_texts in tqdm(dataloader, desc=\"Evaluating Metrics\"):\n",
        "\n",
        "            src_batch = src_batch.to(DEVICE)\n",
        "            trg_batch = trg_batch.to(DEVICE)\n",
        "\n",
        "            generated = model(src_batch, target=None, teacher_forcing_ratio=0.0, max_len=max_len)\n",
        "\n",
        "            for i in range(src_batch.size(0)):\n",
        "                pred_tokens = generated[i].tolist()\n",
        "                trg_tokens = trg_batch[i].tolist()\n",
        "\n",
        "                # <eos> 기준으로 자르기\n",
        "                if recipe_vocab['<eos>'] in pred_tokens:\n",
        "                    pred_tokens = pred_tokens[:pred_tokens.index(recipe_vocab['<eos>'])]\n",
        "                if recipe_vocab['<eos>'] in trg_tokens:\n",
        "                    trg_tokens = trg_tokens[:trg_tokens.index(recipe_vocab['<eos>'])]\n",
        "\n",
        "                pred_words = [recipe_vocab.get_itos()[idx] for idx in pred_tokens]\n",
        "                trg_words = [recipe_vocab.get_itos()[idx] for idx in trg_tokens]\n",
        "\n",
        "                ref_list.append(trg_words)\n",
        "                hyp_list.append(pred_words)\n",
        "\n",
        "    # BLEU-4\n",
        "    bleu_score = corpus_bleu([[ref] for ref in ref_list], hyp_list, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie) \n",
        "\n",
        "    # METEOR\n",
        "    meteor_scores = [meteor_score([ref], hyp) for ref, hyp in zip(ref_list, hyp_list)]\n",
        "    meteor_avg = sum(meteor_scores) / len(meteor_scores)\n",
        "\n",
        "    # BERTScore\n",
        "    refs = [\" \".join(ref) for ref in ref_list]\n",
        "    hyps = [\" \".join(hyp) for hyp in hyp_list]\n",
        "    _, _, f1 = bert_score_fn(hyps, refs, lang='en', verbose=False)\n",
        "    bertscore_f1 = f1.mean().item()\n",
        "\n",
        "    return bleu_score, meteor_avg, bertscore_f1\n",
        "\n",
        "\n",
        "def plot_loss_epoch(name, loss_history):\n",
        "    plt.figure(figsize=(6, 3))\n",
        "\n",
        "    train_loss = loss_history[\"train_epoch\"]\n",
        "    val_loss = loss_history[\"val_epoch\"]\n",
        "\n",
        "    plt.plot(range(1, len(train_loss) + 1), train_loss, label=\"Train Loss\", color=\"blue\")\n",
        "    plt.plot(range(1, len(val_loss) + 1), val_loss, label=\"Validation Loss\", color=\"red\")\n",
        "\n",
        "    plt.xlabel(\"Epoch\", fontsize=10)\n",
        "    plt.ylabel(\"Loss\", fontsize=10)\n",
        "    plt.title(f\"Loss per Epoch: {name}\", fontsize=12)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_loss_iter(**models_histories):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for model_name, history in models_histories.items():\n",
        "        train_iter_losses = history.get(\"train_iter\", [])\n",
        "        if train_iter_losses:\n",
        "            flat_iter_losses = [loss for epoch_losses in train_iter_losses for loss in epoch_losses]\n",
        "            plt.plot(flat_iter_losses, label=model_name)\n",
        "        else:\n",
        "            print(f\"[경고] {model_name}에 train_iter 데이터가 없습니다.\")\n",
        "\n",
        "    plt.title(\"Training Iteration Loss\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_encoder_decoder(model_type, config, input_dim, output_dim, embedding_weights=None, freeze_embedding=False, checklist_vocab=None, tokenizer_checklist=None):\n",
        "    use_attention = config[\"USE_ATTENTION\"]\n",
        "    is_extension = \"extension\" in model_type\n",
        "\n",
        "    if is_extension:\n",
        "        encoder = Encoder_GRU_extension(\n",
        "            ingredient_vocab_size=input_dim,\n",
        "            embedding_dim=config[\"EMBED_DIM\"],\n",
        "            hidden_dim=config[\"HIDDEN_DIM\"],\n",
        "            n_layers=config[\"N_LAYERS\"],\n",
        "            dropout_ratio=config[\"DROPOUT\"],\n",
        "            embedding_weights=embedding_weights,\n",
        "            freeze=freeze_embedding\n",
        "        )\n",
        "        if config.get(\"USE_CHECKLIST\", False):\n",
        "            decoder = DecoderWithChecklist(\n",
        "                recipe_vocab_size=output_dim,\n",
        "                embedding_dim=config[\"EMBED_DIM\"],\n",
        "                hidden_size=config[\"HIDDEN_DIM\"],\n",
        "                n_layers=config[\"N_LAYERS\"],\n",
        "                dropout_ratio=config[\"DROPOUT\"],\n",
        "                checklist_vocab_size=len(checklist_vocab),\n",
        "                checklist_vocab=checklist_vocab,\n",
        "                tokenizer_checklist=tokenizer_checklist\n",
        "            )\n",
        "        else:\n",
        "            decoder = AttnDecoderRNN_extension(\n",
        "                recipe_vocab_size=output_dim,\n",
        "                embedding_dim=config[\"EMBED_DIM\"],\n",
        "                hidden_size=config[\"HIDDEN_DIM\"],\n",
        "                n_layers=config[\"N_LAYERS\"],\n",
        "                dropout_ratio=config[\"DROPOUT\"]\n",
        "            )\n",
        "    else:\n",
        "        encoder = Encoder_GRU(\n",
        "            ingredient_vocab_size=input_dim,\n",
        "            embedding_dim=config[\"EMBED_DIM\"],\n",
        "            hidden_dim=config[\"HIDDEN_DIM\"],\n",
        "            n_layers=config[\"N_LAYERS\"],\n",
        "            dropout_ratio=config[\"DROPOUT\"],\n",
        "            embedding_weights=embedding_weights,\n",
        "            freeze=freeze_embedding\n",
        "        )\n",
        "        if use_attention:\n",
        "            decoder = AttnDecoderRNN(\n",
        "                recipe_vocab_size=output_dim,\n",
        "                embedding_dim=config[\"EMBED_DIM\"],\n",
        "                hidden_size=config[\"HIDDEN_DIM\"],\n",
        "                n_layers=config[\"N_LAYERS\"],\n",
        "                dropout_ratio=config[\"DROPOUT\"]\n",
        "            )\n",
        "        else:\n",
        "            decoder = Decoder_GRU(\n",
        "                recipe_vocab_size=output_dim,\n",
        "                embedding_dim=config[\"EMBED_DIM\"],\n",
        "                hidden_dim=config[\"HIDDEN_DIM\"],\n",
        "                n_layers=config[\"N_LAYERS\"],\n",
        "                dropout_ratio=config[\"DROPOUT\"]\n",
        "            )\n",
        "\n",
        "    return encoder, decoder\n",
        "\n",
        "\n",
        "def MakingAutoEncoder(model_type, config, ingredient_vocab, train_loader, embedding_weights=None, freeze_embedding=False):\n",
        "\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "    INPUT_DIM = len(ingredient_vocab)\n",
        "\n",
        "    # 3. Encoder/Decoder 구성 (output_dim = input_dim)\n",
        "    encoder, decoder = get_encoder_decoder(\n",
        "        model_type=model_type,\n",
        "        config=config,\n",
        "        input_dim=INPUT_DIM,\n",
        "        output_dim=INPUT_DIM,\n",
        "        embedding_weights=embedding_weights,\n",
        "        freeze_embedding=freeze_embedding\n",
        "    )\n",
        "\n",
        "    # 4. AutoEncoder 모델 생성\n",
        "    autoencoder = IngredientAutoEncoder(encoder, decoder, vocab_size=INPUT_DIM).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=config.get(\"LR\", 0.001))\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=ingredient_vocab['<pad>'])\n",
        "\n",
        "    # 5. 학습\n",
        "    print(f\"🚀 Pretraining AutoEncoder for {model_type}\")\n",
        "    train_autoencoder(\n",
        "        model=autoencoder,\n",
        "        dataloader=train_loader,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        epochs=config.get(\"AUTOENCODER_EPOCHS\", 10),\n",
        "        device=DEVICE\n",
        "    )\n",
        "\n",
        "    # 6. 가중치 저장\n",
        "    encoder_path = f\"results/{model_type}_encoder_pretrained.pt\"\n",
        "    decoder_path = f\"results/{model_type}_decoder_pretrained.pt\"\n",
        "    torch.save(encoder.state_dict(), encoder_path)\n",
        "    torch.save(decoder.state_dict(), decoder_path)\n",
        "\n",
        "    print(f\"✅ Saved pretrained encoder to: {encoder_path}\")\n",
        "    print(f\"✅ Saved pretrained decoder to: {decoder_path}\")\n",
        "\n",
        "def main(model_type: str, config: dict,\n",
        "         train_df, dev_df, test_df,\n",
        "         glove_path: str = \"./glove.6B.200d.txt\"):\n",
        "\n",
        "    # 1. Tokenize & Vocab\n",
        "    ingredient_token_lists, recipe_token_lists, ingredient_vocab, recipe_vocab, tokenizer_ingredient, tokenizer_recipe, checklist_vocab, tokenizer_checklist = load_or_tokenize_data(model_type, train_df, config)\n",
        "\n",
        "    # 2. GloVe 임베딩 적용 여부\n",
        "    embedding_weights = None\n",
        "    freeze_embedding = config.get(\"freeze_embedding\", False)\n",
        "    if config.get(\"use_glove\", False):\n",
        "        print(f\"🔎 Using GloVe embeddings for {model_type}\")\n",
        "        ingredient_embedding_matrix = build_glove_embedding_matrix(\n",
        "            glove_path, ingredient_vocab, glove_dim=config[\"EMBED_DIM\"]\n",
        "        )\n",
        "        embedding_weights = ingredient_embedding_matrix\n",
        "\n",
        "    # 3. Dataset & DataLoader\n",
        "    BATCH_SIZE = config.get(\"BATCH_SIZE\", 64)\n",
        "    train_dataset = CustomDataset(train_df, ingredient_vocab, recipe_vocab, tokenizer_ingredient, tokenizer_recipe)\n",
        "    dev_dataset = CustomDataset(dev_df, ingredient_vocab, recipe_vocab, tokenizer_ingredient, tokenizer_recipe)\n",
        "    test_dataset = CustomDataset(test_df, ingredient_vocab, recipe_vocab, tokenizer_ingredient, tokenizer_recipe)\n",
        "    collate = partial(collate_fn, ingredient_vocab=ingredient_vocab, recipe_vocab=recipe_vocab, device=DEVICE)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)\n",
        "    dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
        "\n",
        "    # ✅ 4. AutoEncoder 사전학습 여부에 따라 사전학습 실행 (ingredient_vocab 사용)\n",
        "    if config.get(\"USE_AUTOENCODER\", False):\n",
        "        print(f\"📦 AutoEncoder pretraining required for {model_type}\")\n",
        "        MakingAutoEncoder(\n",
        "            model_type=model_type,\n",
        "            config=config,\n",
        "            input_vocab=ingredient_vocab,\n",
        "            train_loader=train_loader,\n",
        "            embedding_weights=embedding_weights,\n",
        "            freeze_embedding=freeze_embedding,\n",
        "            device=DEVICE\n",
        "        )\n",
        "\n",
        "    # 5. Model 구성 (recipe_vocab 사용)\n",
        "    INPUT_DIM = len(ingredient_vocab)\n",
        "    OUTPUT_DIM = len(recipe_vocab)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=recipe_vocab['<pad>'])\n",
        "\n",
        "    encoder, decoder = get_encoder_decoder(\n",
        "        model_type=model_type,\n",
        "        config=config,\n",
        "        input_dim=INPUT_DIM,\n",
        "        output_dim=OUTPUT_DIM,\n",
        "        embedding_weights=embedding_weights,\n",
        "        freeze_embedding=freeze_embedding,\n",
        "        checklist_vocab=checklist_vocab,\n",
        "        tokenizer_checklist=tokenizer_checklist\n",
        "    )\n",
        "\n",
        "    # ✅ 6. 사전학습된 AutoEncoder 인코더 가중치만 로드\n",
        "    if config.get(\"USE_AUTOENCODER\", False):\n",
        "        encoder_path = f\"results/{model_type}_encoder_pretrained.pt\"\n",
        "        print(f\"🔁 Loading pretrained AutoEncoder encoder weights for {model_type}\")\n",
        "        encoder.load_state_dict(torch.load(encoder_path, map_location=DEVICE))\n",
        "\n",
        "    model = Seq2Seq(encoder, decoder, DEVICE, use_attention=config[\"USE_ATTENTION\"],recipe_vocab=recipe_vocab).to(DEVICE)\n",
        "\n",
        "    # 7. 학습 또는 로드\n",
        "    save_model_path = f\"results/{model_type}.pt\"\n",
        "    save_history_path = f\"results/{model_type}_history.pt\"\n",
        "\n",
        "    if config[\"new_model_train\"]:\n",
        "        print(f\"Training model: {model_type}\")\n",
        "        print(model)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=config[\"LR\"])\n",
        "        loss_history = Train(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=dev_loader,\n",
        "            criterion=criterion,\n",
        "            optimizer=optimizer,\n",
        "            recipe_vocab=recipe_vocab,\n",
        "            EPOCHS=config[\"EPOCHS\"],\n",
        "            BATCH_SIZE=BATCH_SIZE,\n",
        "            TRAIN_RATIO=1.0,\n",
        "            save_model_path=save_model_path,\n",
        "            save_history_path=save_history_path,\n",
        "            TEACHER_FORCING_RATIO=config[\"TEACHER_FORCING_RATIO\"],\n",
        "            MAX_LEN=config[\"MAX_LEN\"],\n",
        "            LR_STEP=config.get(\"LR_STEP\"),\n",
        "            LR_GAMMA=config.get(\"LR_GAMMA\")\n",
        "        )\n",
        "        return model, encoder, decoder, loss_history, save_model_path, save_history_path, test_loader, recipe_vocab, ingredient_vocab\n",
        "\n",
        "    else:\n",
        "        return model, encoder, decoder, None, save_model_path, save_history_path, test_loader, recipe_vocab, ingredient_vocab\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 하이퍼파라미터 저장\n",
        "experiment_configs = {\n",
        "    \"baseline1\": {\n",
        "        \"new_model_train\" : True,\n",
        "        \"EMBED_DIM\": 256,\n",
        "        \"HIDDEN_DIM\": 512,\n",
        "        \"DROPOUT\": 0.5,\n",
        "        \"N_LAYERS\": 1,\n",
        "        \"LR\": 0.001,\n",
        "        \"EPOCHS\": 2,\n",
        "        \"USE_ATTENTION\": False,\n",
        "        \"TEACHER_FORCING_RATIO\": 0.5,\n",
        "        \"MAX_LEN\": 50,\n",
        "        \"BATCH_SIZE\": 64\n",
        "    },\n",
        "    \"baseline2\": {\n",
        "        \"new_model_train\" : True,\n",
        "        \"EMBED_DIM\": 256,\n",
        "        \"HIDDEN_DIM\": 512,\n",
        "        \"DROPOUT\": 0.5,\n",
        "        \"N_LAYERS\": 1,\n",
        "        \"LR\": 0.001,\n",
        "        \"EPOCHS\": 2,\n",
        "        \"USE_ATTENTION\": True,\n",
        "        \"TEACHER_FORCING_RATIO\": 0.5,\n",
        "        \"MAX_LEN\": 50,\n",
        "        \"BATCH_SIZE\": 64\n",
        "    }#,\n",
        "    # \"mild_extension1\": {\n",
        "    #     \"new_model_train\" : True,\n",
        "    #     \"EMBED_DIM\": 256,\n",
        "    #     \"HIDDEN_DIM\": 512,\n",
        "    #     \"DROPOUT\": 0.5,\n",
        "    #     \"N_LAYERS\": 3,\n",
        "    #     \"LR\": 0.001,\n",
        "    #     \"EPOCHS\": 5,\n",
        "    #     \"USE_ATTENTION\": True,\n",
        "    #     \"TEACHER_FORCING_RATIO\": 0.5,\n",
        "    #     \"MAX_LEN\": 100,\n",
        "    #     \"LR_STEP\": 1,         \n",
        "    #     \"LR_GAMMA\": 0.7,\n",
        "    #     \"BATCH_SIZE\": 64\n",
        "    # },\n",
        "    # \"mild_extension2\": {\n",
        "    #     \"new_model_train\" : True,\n",
        "    #     \"EMBED_DIM\": 200,  # GloVe 6B 200D와 일치\n",
        "    #     \"HIDDEN_DIM\": 512,\n",
        "    #     \"DROPOUT\": 0.5,\n",
        "    #     \"N_LAYERS\": 2,\n",
        "    #     \"LR\": 0.001,\n",
        "    #     \"EPOCHS\": 5,\n",
        "    #     \"USE_ATTENTION\": True,\n",
        "    #     \"TEACHER_FORCING_RATIO\": 0.5,\n",
        "    #     \"MAX_LEN\": 100,\n",
        "    #     \"LR_STEP\": 1,         \n",
        "    #     \"LR_GAMMA\": 0.7,\n",
        "    #     \"use_glove\": True,   \n",
        "    #     \"freeze_embedding\": False,\n",
        "    #     \"BATCH_SIZE\": 64,\n",
        "    #     \"USE_AUTOENCODER\": True,\n",
        "    #     \"AUTOENCODER_EPOCHS\": 10,\n",
        "    # },\n",
        "    # \"spicy_extension1\": {\n",
        "    #     \"new_model_train\": True,\n",
        "    #     \"EMBED_DIM\": 256,\n",
        "    #     \"HIDDEN_DIM\": 512,\n",
        "    #     \"DROPOUT\": 0.5,\n",
        "    #     \"N_LAYERS\": 2,\n",
        "    #     \"LR\": 0.001,\n",
        "    #     \"EPOCHS\": 10,\n",
        "    #     \"USE_ATTENTION\": True,\n",
        "    #     \"TEACHER_FORCING_RATIO\": 0.5,\n",
        "    #     \"MAX_LEN\": 100,\n",
        "    #     \"LR_STEP\": 1,         \n",
        "    #     \"LR_GAMMA\": 0.7,\n",
        "    #     \"BATCH_SIZE\": 64,\n",
        "    #     \"USE_CHECKLIST\": True\n",
        "    #}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {},
      "outputs": [],
      "source": [
        "def MakingAutoEncoder(model_type, config, input_vocab, train_loader,\n",
        "                      embedding_weights=None, freeze_embedding=False,device=DEVICE):\n",
        "\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "    INPUT_DIM = len(input_vocab)\n",
        "\n",
        "    # 1. Encoder / Decoder 준비 (입력 = 출력)\n",
        "    encoder, decoder = get_encoder_decoder(\n",
        "        model_type=model_type,\n",
        "        config=config,\n",
        "        input_dim=INPUT_DIM,\n",
        "        output_dim=INPUT_DIM,\n",
        "        embedding_weights=embedding_weights,\n",
        "        freeze_embedding=freeze_embedding\n",
        "    )\n",
        "\n",
        "    # 2. AutoEncoder 구성\n",
        "    autoencoder = IngredientAutoEncoder(encoder, decoder, vocab_size=INPUT_DIM).to(device)\n",
        "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=config.get(\"LR\", 0.001))\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=input_vocab['<pad>'])\n",
        "\n",
        "    # 3. 학습\n",
        "    print(f\"🚀 Pretraining AutoEncoder for {model_type}\")\n",
        "    train_autoencoder(autoencoder, train_loader, criterion, optimizer,\n",
        "                      epochs=config.get(\"EPOCHS\", 5), device=device)\n",
        "\n",
        "    # 4. 저장\n",
        "    torch.save(encoder.state_dict(), f\"results/{model_type}_encoder_pretrained.pt\")\n",
        "    torch.save(decoder.state_dict(), f\"results/{model_type}_decoder_pretrained.pt\")\n",
        "    print(f\"✅ Saved pretrained encoder/decoder for {model_type}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_trained_model(model_type: str, config: dict, device=DEVICE):\n",
        "    \"\"\"\n",
        "    best model과 final model을 모두 불러옵니다.\n",
        "\n",
        "    Args:\n",
        "        model_type (str): 저장 파일 이름 결정에 사용되는 모델 이름\n",
        "        config (dict): 설정 정보 (input_dim, output_dim, attention 등 포함)\n",
        "        device (torch.device): 모델을 로드할 디바이스\n",
        "\n",
        "    Returns:\n",
        "        best_model (nn.Module)\n",
        "        final_model (nn.Module)\n",
        "        loss_history (dict)\n",
        "        best_checkpoint (dict)\n",
        "        final_checkpoint (dict)\n",
        "    \"\"\"\n",
        "    # 경로 구성\n",
        "    best_model_path = f\"results/{model_type}.pt\"\n",
        "    final_model_path = best_model_path.replace(\".pt\", \"_final.pt\")\n",
        "    save_history_path = f\"results/{model_type}_history.pt\"\n",
        "\n",
        "    # 모델 구조 재생성 (encoder/decoder 두 번 생성)\n",
        "    INPUT_DIM = config[\"input_dim\"]\n",
        "    OUTPUT_DIM = config[\"output_dim\"]\n",
        "\n",
        "    def build_model():\n",
        "        encoder, decoder = get_encoder_decoder(\n",
        "            model_type=model_type,\n",
        "            config=config,\n",
        "            input_dim=INPUT_DIM,\n",
        "            output_dim=OUTPUT_DIM,\n",
        "            embedding_weights=None,\n",
        "            freeze_embedding=config.get(\"freeze_embedding\", False),\n",
        "            checklist_vocab=None,\n",
        "            tokenizer_checklist=None\n",
        "        )\n",
        "        return Seq2Seq(encoder, decoder, device, use_attention=config[\"USE_ATTENTION\"], recipe_vocab=config[\"recipe_vocab\"]).to(device)\n",
        "\n",
        "    # ✅ Best model\n",
        "    best_model = build_model()\n",
        "    best_checkpoint = torch.load(best_model_path, map_location=device)\n",
        "    best_model.load_state_dict(best_checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    # ✅ Final model\n",
        "    final_model = build_model()\n",
        "    final_checkpoint = torch.load(final_model_path, map_location=device)\n",
        "    final_model.load_state_dict(final_checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    # ✅ 학습 이력\n",
        "    history = torch.load(save_history_path, map_location=device)\n",
        "    loss_history = history[\"loss_history\"]\n",
        "\n",
        "    print(f\"📥 Loaded BEST model (Epoch {best_checkpoint['epoch']})\")\n",
        "    print(f\"📥 Loaded FINAL model (Epoch {final_checkpoint['epoch']})\")\n",
        "\n",
        "    return best_model, final_model, loss_history, best_checkpoint, final_checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_and_test_model(model_type: str,\n",
        "                             config: dict,\n",
        "                             ingredient_vocab,\n",
        "                             recipe_vocab,\n",
        "                             test_loader,\n",
        "                             criterion,\n",
        "                             device=DEVICE):\n",
        "    \"\"\"\n",
        "    저장된 best/final 모델을 불러와 summary, 그래프, 테스트 수행\n",
        "    \"\"\"\n",
        "\n",
        "    # config에 input/output dim 추가 (원본 손상 X)\n",
        "    config = config.copy()\n",
        "    config[\"input_dim\"] = len(ingredient_vocab)\n",
        "    config[\"output_dim\"] = len(recipe_vocab)\n",
        "\n",
        "    # 모델 두 개 모두 불러오기\n",
        "    best_model, final_model, loss_history, best_ckpt, final_ckpt = load_trained_model(model_type, config, device)\n",
        "\n",
        "    # ✅ 손실 곡선 시각화\n",
        "    plot_loss_epoch(model_type, loss_history)\n",
        "\n",
        "    # ✅ BEST 모델 평가\n",
        "    print(f\"\\n========== BEST MODEL SUMMARY ({model_type}) ==========\")\n",
        "    print(f\"Epoch:         {best_ckpt['epoch']}\")\n",
        "    print(f\"Train Loss:    {best_ckpt['train_loss']:.4f}\")\n",
        "    print(f\"Val Loss:      {best_ckpt['val_loss']:.4f}\")\n",
        "    print(f\"Parameters:    {sum(p.numel() for p in best_model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    print(\"\\nRunning test on BEST model...\")\n",
        "    best_test_loss, best_bleu, best_meteor, best_bert = Test(\n",
        "        best_model, test_loader, criterion, recipe_vocab, MAX_LEN=config[\"MAX_LEN\"]\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- BEST MODEL TEST RESULTS ---\")\n",
        "    print(f\"Test Loss    : {best_test_loss:.4f}\")\n",
        "    print(f\"BLEU-4 Score : {best_bleu:.4f}\")\n",
        "    print(f\"METEOR Score : {best_meteor:.4f}\")\n",
        "    print(f\"BERTScore-F1 : {best_bert:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # ✅ FINAL 모델 평가\n",
        "    print(f\"\\n========== FINAL MODEL SUMMARY ({model_type}) ==========\")\n",
        "    print(f\"Epoch:         {final_ckpt['epoch']}\")\n",
        "    print(f\"Train Loss:    {final_ckpt['train_loss']:.4f}\")\n",
        "    print(f\"Val Loss:      {final_ckpt['val_loss']:.4f}\")\n",
        "    print(f\"Parameters:    {sum(p.numel() for p in final_model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    print(\"\\nRunning test on FINAL model...\")\n",
        "    final_test_loss, final_bleu, final_meteor, final_bert = Test(\n",
        "        final_model, test_loader, criterion, recipe_vocab, MAX_LEN=config[\"MAX_LEN\"]\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- FINAL MODEL TEST RESULTS ---\")\n",
        "    print(f\"Test Loss    : {final_test_loss:.4f}\")\n",
        "    print(f\"BLEU-4 Score : {final_bleu:.4f}\")\n",
        "    print(f\"METEOR Score : {final_meteor:.4f}\")\n",
        "    print(f\"BERTScore-F1 : {final_bert:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "\n",
        "    # ✅ 결과 리턴 (딕셔너리로)\n",
        "    return {\n",
        "        \"best\": {\n",
        "            \"test_loss\": best_test_loss,\n",
        "            \"BLEU\": best_bleu,\n",
        "            \"METEOR\": best_meteor,\n",
        "            \"BERTScore\": best_bert,\n",
        "        },\n",
        "        \"final\": {\n",
        "            \"test_loss\": final_test_loss,\n",
        "            \"BLEU\": final_bleu,\n",
        "            \"METEOR\": final_meteor,\n",
        "            \"BERTScore\": final_bert,\n",
        "        }\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================== 🔧 Training & Evaluating: baseline1 ==================\n",
            "✅ Loaded ingredient token cache.\n",
            "✅ Loaded recipe token cache.\n",
            "Training model: baseline1\n",
            "Seq2Seq(\n",
            "  (encoder): Encoder_GRU(\n",
            "    (embedding): Embedding(9032, 256)\n",
            "    (gru): GRU(256, 512, batch_first=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): Decoder_GRU(\n",
            "    (embedding): Embedding(9081, 256)\n",
            "    (gru): GRU(256, 512, batch_first=True)\n",
            "    (fc_out): Linear(in_features=512, out_features=9081, bias=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (recipe_vocab): Vocab()\n",
            ")\n",
            "[Epoch: 1/2] current_LR = 0.001\n",
            "Batch 1/4 | Loss: 9.1162\n",
            "Batch 2/4 | Loss: 8.8942\n",
            "Batch 3/4 | Loss: 8.4278\n",
            "Batch 4/4 | Loss: 7.9603\n",
            "1 Epoch Train Completed!\n",
            "Validation Start!\n",
            "Batch 1/17 | Loss: 6.4560\n",
            "Batch 2/17 | Loss: 6.5261\n",
            "Batch 3/17 | Loss: 6.5066\n",
            "Batch 4/17 | Loss: 6.4365\n",
            "Batch 5/17 | Loss: 6.4807\n",
            "Batch 6/17 | Loss: 6.5422\n",
            "Batch 7/17 | Loss: 6.5259\n",
            "Batch 8/17 | Loss: 6.5132\n",
            "Batch 9/17 | Loss: 6.5460\n",
            "Batch 10/17 | Loss: 6.5075\n",
            "Batch 11/17 | Loss: 6.4679\n",
            "Batch 12/17 | Loss: 6.5755\n",
            "Batch 13/17 | Loss: 6.5633\n",
            "Batch 14/17 | Loss: 6.4266\n",
            "Batch 15/17 | Loss: 6.5307\n",
            "Batch 16/17 | Loss: 6.4180\n",
            "Batch 17/17 | Loss: 6.4172\n",
            "Validation Completed!\n",
            "--------------------------------------------------\n",
            "[Epoch 1/2] \n",
            "Train Loss: 8.7786 | Val Loss: 6.4982 | Time: 273.83s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Metrics: 100%|██████████| 17/17 [00:39<00:00,  2.34s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Metrics:\n",
            "📊 BLEU: 0.0017 | METEOR: 0.0836 | BERTScore-F1: 0.7191\n",
            "--------------------------------------------------\n",
            "Best model saved!\n",
            "[Epoch: 2/2] current_LR = 0.001\n",
            "Batch 1/4 | Loss: 6.9590\n",
            "Batch 2/4 | Loss: 6.1700\n",
            "Batch 3/4 | Loss: 5.5974\n",
            "Batch 4/4 | Loss: 5.4168\n",
            "2 Epoch Train Completed!\n",
            "Validation Start!\n",
            "Batch 1/17 | Loss: 5.1964\n",
            "Batch 2/17 | Loss: 5.2966\n",
            "Batch 3/17 | Loss: 5.2675\n",
            "Batch 4/17 | Loss: 5.1593\n",
            "Batch 5/17 | Loss: 5.2032\n",
            "Batch 6/17 | Loss: 5.3197\n",
            "Batch 7/17 | Loss: 5.2626\n",
            "Batch 8/17 | Loss: 5.2932\n",
            "Batch 9/17 | Loss: 5.3557\n",
            "Batch 10/17 | Loss: 5.2545\n",
            "Batch 11/17 | Loss: 5.2278\n",
            "Batch 12/17 | Loss: 5.3233\n",
            "Batch 13/17 | Loss: 5.3447\n",
            "Batch 14/17 | Loss: 5.1951\n",
            "Batch 15/17 | Loss: 5.3070\n",
            "Batch 16/17 | Loss: 5.1923\n",
            "Batch 17/17 | Loss: 5.1187\n",
            "Validation Completed!\n",
            "--------------------------------------------------\n",
            "[Epoch 2/2] \n",
            "Train Loss: 6.2091 | Val Loss: 5.2569 | Time: 192.73s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Metrics: 100%|██████████| 17/17 [00:40<00:00,  2.35s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Metrics:\n",
            "📊 BLEU: 0.0012 | METEOR: 0.0835 | BERTScore-F1: 0.7187\n",
            "--------------------------------------------------\n",
            "Best model saved!\n",
            "Final model saved!\n",
            "Training Completed!\n",
            "Total number of iteration : 8\n",
            "Total Time for Training: 720.22s\n",
            "\n",
            "======= Training Summary =======\n",
            "Best Model:    Epoch 2\n",
            "Train Loss:    6.2091\n",
            "Val Loss:      5.2569\n",
            "BLEU:          0.0012\n",
            "METEOR:        0.0835\n",
            "BERTScore-F1:  0.7187\n",
            "\n",
            "\n",
            "Final Model:   Epoch 2\n",
            "Train Loss:    6.2091\n",
            "Val Loss:      5.2569\n",
            "BLEU:          0.0012\n",
            "METEOR:        0.0835\n",
            "BERTScore-F1:  0.7187\n",
            "Total Time:     720.22s\n",
            "==================================\n",
            "\n",
            "📥 Loaded BEST model (Epoch 2)\n",
            "📥 Loaded FINAL model (Epoch 2)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========== BEST MODEL SUMMARY (baseline1) ==========\n",
            "Epoch:         2\n",
            "Train Loss:    6.2091\n",
            "Val Loss:      5.2569\n",
            "Parameters:    11,660,921\n",
            "\n",
            "Running test on BEST model...\n",
            "Batch 1/17 | Loss: 5.2805\n",
            "Batch 2/17 | Loss: 5.1867\n",
            "Batch 3/17 | Loss: 5.1725\n",
            "Batch 4/17 | Loss: 5.2764\n",
            "Batch 5/17 | Loss: 5.1909\n",
            "Batch 6/17 | Loss: 5.1689\n",
            "Batch 7/17 | Loss: 5.1481\n",
            "Batch 8/17 | Loss: 5.2630\n",
            "Batch 9/17 | Loss: 5.1942\n",
            "Batch 10/17 | Loss: 5.2138\n",
            "Batch 11/17 | Loss: 5.3113\n",
            "Batch 12/17 | Loss: 5.3241\n",
            "Batch 13/17 | Loss: 5.3236\n",
            "Batch 14/17 | Loss: 5.1696\n",
            "Batch 15/17 | Loss: 5.3465\n",
            "Batch 16/17 | Loss: 5.3303\n",
            "Batch 17/17 | Loss: 5.3530\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Metrics: 100%|██████████| 17/17 [00:31<00:00,  1.88s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss      : 5.2495\n",
            "BLEU-4 Score   : 0.0013\n",
            "METEOR Score   : 0.0845\n",
            "BERTScore (F1) : 0.7185\n",
            "\n",
            "--- BEST MODEL TEST RESULTS ---\n",
            "Test Loss    : 5.2495\n",
            "BLEU-4 Score : 0.0013\n",
            "METEOR Score : 0.0845\n",
            "BERTScore-F1 : 0.7185\n",
            "--------------------------------------------------\n",
            "\n",
            "========== FINAL MODEL SUMMARY (baseline1) ==========\n",
            "Epoch:         2\n",
            "Train Loss:    6.2091\n",
            "Val Loss:      5.2569\n",
            "Parameters:    11,660,921\n",
            "\n",
            "Running test on FINAL model...\n",
            "Batch 1/17 | Loss: 5.2805\n",
            "Batch 2/17 | Loss: 5.1867\n",
            "Batch 3/17 | Loss: 5.1725\n",
            "Batch 4/17 | Loss: 5.2764\n",
            "Batch 5/17 | Loss: 5.1909\n",
            "Batch 6/17 | Loss: 5.1689\n",
            "Batch 7/17 | Loss: 5.1481\n",
            "Batch 8/17 | Loss: 5.2630\n",
            "Batch 9/17 | Loss: 5.1942\n",
            "Batch 10/17 | Loss: 5.2138\n",
            "Batch 11/17 | Loss: 5.3113\n",
            "Batch 12/17 | Loss: 5.3241\n",
            "Batch 13/17 | Loss: 5.3236\n",
            "Batch 14/17 | Loss: 5.1696\n",
            "Batch 15/17 | Loss: 5.3465\n",
            "Batch 16/17 | Loss: 5.3303\n",
            "Batch 17/17 | Loss: 5.3530\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Metrics: 100%|██████████| 17/17 [00:34<00:00,  2.00s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss      : 5.2495\n",
            "BLEU-4 Score   : 0.0013\n",
            "METEOR Score   : 0.0845\n",
            "BERTScore (F1) : 0.7185\n",
            "\n",
            "--- FINAL MODEL TEST RESULTS ---\n",
            "Test Loss    : 5.2495\n",
            "BLEU-4 Score : 0.0013\n",
            "METEOR Score : 0.0845\n",
            "BERTScore-F1 : 0.7185\n",
            "--------------------------------------------------\n",
            "\n",
            "================== 🔧 Training & Evaluating: baseline2 ==================\n",
            "✅ Loaded ingredient token cache.\n",
            "✅ Loaded recipe token cache.\n",
            "Training model: baseline2\n",
            "Seq2Seq(\n",
            "  (encoder): Encoder_GRU(\n",
            "    (embedding): Embedding(9032, 256)\n",
            "    (gru): GRU(256, 512, batch_first=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): AttnDecoderRNN(\n",
            "    (embedding): Embedding(9081, 512)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "    (gru): GRU(512, 512, batch_first=True)\n",
            "    (out): Linear(in_features=1024, out_features=9081, bias=True)\n",
            "  )\n",
            "  (recipe_vocab): Vocab()\n",
            ")\n",
            "[Epoch: 1/2] current_LR = 0.001\n",
            "Batch 1/4 | Loss: 9.1355\n",
            "Batch 2/4 | Loss: 8.7158\n",
            "Batch 3/4 | Loss: 7.7992\n",
            "Batch 4/4 | Loss: 7.1568\n",
            "1 Epoch Train Completed!\n",
            "Validation Start!\n",
            "Batch 1/17 | Loss: 5.7105\n",
            "Batch 2/17 | Loss: 5.8149\n",
            "Batch 3/17 | Loss: 5.7873\n",
            "Batch 4/17 | Loss: 5.7110\n",
            "Batch 5/17 | Loss: 5.7439\n",
            "Batch 6/17 | Loss: 5.8449\n",
            "Batch 7/17 | Loss: 5.8131\n",
            "Batch 8/17 | Loss: 5.8146\n",
            "Batch 9/17 | Loss: 5.8376\n",
            "Batch 10/17 | Loss: 5.7760\n",
            "Batch 11/17 | Loss: 5.7486\n",
            "Batch 12/17 | Loss: 5.8708\n",
            "Batch 13/17 | Loss: 5.8472\n",
            "Batch 14/17 | Loss: 5.7057\n",
            "Batch 15/17 | Loss: 5.8286\n",
            "Batch 16/17 | Loss: 5.6868\n",
            "Batch 17/17 | Loss: 5.7394\n",
            "Validation Completed!\n",
            "--------------------------------------------------\n",
            "[Epoch 1/2] \n",
            "Train Loss: 8.4944 | Val Loss: 5.7821 | Time: 228.16s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Metrics: 100%|██████████| 17/17 [00:42<00:00,  2.52s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Metrics:\n",
            "📊 BLEU: 0.0015 | METEOR: 0.0738 | BERTScore-F1: 0.7132\n",
            "--------------------------------------------------\n",
            "Best model saved!\n",
            "[Epoch: 2/2] current_LR = 0.001\n",
            "Batch 1/4 | Loss: 5.9161\n",
            "Batch 2/4 | Loss: 5.4118\n",
            "Batch 3/4 | Loss: 5.1452\n",
            "Batch 4/4 | Loss: 4.7286\n",
            "2 Epoch Train Completed!\n",
            "Validation Start!\n",
            "Batch 1/17 | Loss: 5.1636\n",
            "Batch 2/17 | Loss: 5.2995\n",
            "Batch 3/17 | Loss: 5.2837\n"
          ]
        }
      ],
      "source": [
        "for model_type, base_config in experiment_configs.items():\n",
        "    print(f\"\\n================== 🔧 Training & Evaluating: {model_type} ==================\")\n",
        "\n",
        "    # ✅ config 복사 및 설정\n",
        "    config = base_config.copy()\n",
        "\n",
        "    # main 실행\n",
        "    model, encoder, decoder, _, save_model_path, save_history_path, test_loader, recipe_vocab, ingredient_vocab = main(\n",
        "        model_type, config, train_df, dev_df, test_df\n",
        "    )\n",
        "\n",
        "    # ✅ config에 필요한 정보 추가\n",
        "    config[\"input_dim\"] = len(ingredient_vocab)\n",
        "    config[\"output_dim\"] = len(recipe_vocab)\n",
        "    config[\"recipe_vocab\"] = recipe_vocab\n",
        "\n",
        "    if not os.path.exists(save_model_path) or not os.path.exists(save_history_path):\n",
        "        print(f\"⚠️ Checkpoint for {model_type} not found. Skipping evaluation.\\n\")\n",
        "        continue\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=recipe_vocab['<pad>'])\n",
        "\n",
        "    summarize_and_test_model(\n",
        "        model_type=model_type,\n",
        "        config=config,\n",
        "        ingredient_vocab=ingredient_vocab,\n",
        "        recipe_vocab=recipe_vocab,\n",
        "        test_loader=test_loader,\n",
        "        criterion=criterion\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plot for all model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 여러 모델들의 히스토리를 불러온 후\n",
        "# baseline1_history = torch.load(\"results/baseline1_history.pt\")[\"loss_history\"]\n",
        "# baseline2_history = torch.load(\"results/baseline2_history.pt\")[\"loss_history\"]\n",
        "# mild1_history = torch.load(\"results/mild_extension1_history.pt\")[\"loss_history\"]\n",
        "\n",
        "# plot_loss_iter(\n",
        "#     baseline1=baseline1_history,\n",
        "#     baseline2=baseline2_history,\n",
        "#     mild_extension1=mild1_history\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## old version ##\n",
        "def generate_recipes(ingredient_list, ingredient_vocab, recipe_vocab, max_len=30, is_raw_string=False, **models):\n",
        "\n",
        "    # Tokenizer 적용 여부\n",
        "    if is_raw_string:\n",
        "        tokens = tokenizer_ingredient(str(ingredient_list))\n",
        "    else:\n",
        "        tokens = ingredient_list\n",
        "\n",
        "    # index로 변환\n",
        "    tokens_ids = [ ingredient_vocab[token] if token in ingredient_vocab else ingredient_vocab['<unk>'] for token in tokens]\n",
        "    src_tensor = torch.tensor(tokens_ids, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
        "\n",
        "    print(\"Ingredient :\",', '.join(tokens))\n",
        "    for name, model in models.items():\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            generated = model(src_tensor, target=None, teacher_forcing_ratio=0.0, max_len=max_len)\n",
        "            pred_ids = generated[0].tolist()\n",
        "\n",
        "            # <eos> 기준으로 자르기\n",
        "            if recipe_vocab['<eos>'] in pred_ids:\n",
        "                pred_ids = pred_ids[:pred_ids.index(recipe_vocab['<eos>'])]\n",
        "\n",
        "            pred_tokens = [recipe_vocab.get_itos()[idx] for idx in pred_ids]\n",
        "            print(f\"{name}: {' '.join(pred_tokens[:30])}\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Seq2Seq.__init__() missing 1 required positional argument: 'recipe_vocab'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[260], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## old version ##\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2Seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUSE_ATTENTION\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      3\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(save_model_path, map_location\u001b[38;5;241m=\u001b[39mDEVICE)\n\u001b[1;32m      4\u001b[0m checkpoint2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(save_history_path, map_location\u001b[38;5;241m=\u001b[39mDEVICE)\n",
            "\u001b[0;31mTypeError\u001b[0m: Seq2Seq.__init__() missing 1 required positional argument: 'recipe_vocab'"
          ]
        }
      ],
      "source": [
        "## old version ##\n",
        "model = Seq2Seq(encoder, decoder, DEVICE, use_attention=config[\"USE_ATTENTION\"]).to(DEVICE)\n",
        "checkpoint = torch.load(save_model_path, map_location=DEVICE)\n",
        "checkpoint2 = torch.load(save_history_path, map_location=DEVICE)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "models = {\n",
        "    \"baseline1\": load_baseline1,\n",
        "    # \"baseline2\": load_baseline2\n",
        "}\n",
        "\n",
        "# # Sample 1: 전처리된 리스트\n",
        "sample1_raw = \"sugar, lemon juice,  water,  orange juice, strawberries, icecream\"\n",
        "sample1 = sample1_raw.split(\", \")\n",
        "generate_recipes(sample1, ingredient_vocab, recipe_vocab, is_raw_string=False, **models)\n",
        "\n",
        "# Sample2 : \n",
        "sample2_raw =\"8 oz philadelphia cream cheese, 14 oz can sweetened condensed milk, 1 ts vanilla, 1/3 c  lemon juice, 48 oz canned cherries, 8 inch graham cracker,  pie crusts\"\n",
        "sample2 = sample2_raw.split(\", \")\n",
        "generate_recipes(sample2, ingredient_vocab, recipe_vocab, is_raw_string=True, **models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_trained_model_with_vocab(model_type: str, config: dict,\n",
        "                                  train_df, dev_df, test_df):\n",
        "    \"\"\"\n",
        "    모델 + vocab + tokenizer 세트 로드\n",
        "    \"\"\"\n",
        "    model, encoder, decoder, _, save_model_path, _, test_loader, recipe_vocab = main(\n",
        "        model_type, config, train_df, dev_df, test_df\n",
        "    )\n",
        "    checkpoint = torch.load(save_model_path, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    \n",
        "    # tokenizer, vocab도 함께 반환\n",
        "    _, _, ingredient_vocab, recipe_vocab, tokenizer_ingredient, _ = load_or_tokenize_data(model_type, train_df)\n",
        "\n",
        "    return {\n",
        "        \"model\": model,\n",
        "        \"ingredient_vocab\": ingredient_vocab,\n",
        "        \"recipe_vocab\": recipe_vocab,\n",
        "        \"tokenizer_ingredient\": tokenizer_ingredient\n",
        "    }\n",
        "\n",
        "def generate_recipes(ingredient_list, max_len=30, is_raw_string=False, **model_bundle_dict):\n",
        "    for name, bundle in model_bundle_dict.items():\n",
        "        model = bundle[\"model\"]\n",
        "        ingredient_vocab = bundle[\"ingredient_vocab\"]\n",
        "        recipe_vocab = bundle[\"recipe_vocab\"]\n",
        "        tokenizer_ingredient = bundle[\"tokenizer_ingredient\"]\n",
        "\n",
        "        # Tokenize\n",
        "        if is_raw_string:\n",
        "            tokens = tokenizer_ingredient(str(ingredient_list))\n",
        "        else:\n",
        "            tokens = ingredient_list\n",
        "\n",
        "        # Index 변환\n",
        "        token_ids = [ingredient_vocab[token] if token in ingredient_vocab else ingredient_vocab['<unk>'] for token in tokens]\n",
        "        src_tensor = torch.tensor(token_ids, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
        "\n",
        "        print(f\"Ingredient ({name}):\", ', '.join(tokens))\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            generated = model(src_tensor, target=None, teacher_forcing_ratio=0.0, max_len=max_len)\n",
        "            pred_ids = generated[0].tolist()\n",
        "            if recipe_vocab['<eos>'] in pred_ids:\n",
        "                pred_ids = pred_ids[:pred_ids.index(recipe_vocab['<eos>'])]\n",
        "            pred_tokens = [recipe_vocab.get_itos()[idx] for idx in pred_ids]\n",
        "            print(f\"{name}: {' '.join(pred_tokens[:30])}\")\n",
        "    print(\"-\" * 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\"baseline1\"}\n",
        "for model_type, config in experiment_configs.items():\n",
        "    if not config[\"new_model_train\"]:\n",
        "        continue\n",
        "    models[model_type] = load_trained_model_with_vocab(model_type, config, train_df, dev_df, test_df)\n",
        "\n",
        "sample1_raw = \"sugar, lemon juice,  water,  orange juice, strawberries, icecream\"\n",
        "sample2_raw =\"8 oz philadelphia cream cheese, 14 oz can sweetened condensed milk, 1 ts vanilla, 1/3 c  lemon juice, 48 oz canned cherries, 8 inch graham cracker,  pie crusts\"\n",
        "generate_recipes(sample1_raw, is_raw_string=True, **models)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
